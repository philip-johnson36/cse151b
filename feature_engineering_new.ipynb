{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "#df_tr = pd.read_csv(\"archive/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr.head()\n",
    "# avg = df_tr['LEN'].mean()\n",
    "# print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "def parse_time(x):\n",
    "  dt = datetime.fromtimestamp(x['TIMESTAMP'])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday(), dt.timetuple().tm_yday\n",
    "\n",
    "# year, month, day, hour, weekday, dayOfYear = parse_time(1408048611)\n",
    "# Because we are assigning multiple values at a , wetime need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "def parse_midnight_minutes(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return (dt.hour * 60 + dt.minute) / 1440\n",
    "\n",
    "metadata = pd.read_csv('archive/metaData_taxistandsID_name_GPSlocation.csv')\n",
    "longlatTable = []\n",
    "for i in range(0, 63):\n",
    "    longlatTable.append([metadata[i:i+1].Latitude.values[0], metadata[i:i+1].Longitude.values[0]])\n",
    "\n",
    "maxLat = np.max(np.array(longlatTable)[:, 0])\n",
    "minLat = np.min(np.array(longlatTable)[:, 0])\n",
    "maxLong = np.max(np.array(longlatTable)[:, 1])\n",
    "minLong = np.min(np.array(longlatTable)[:, 1])\n",
    "\n",
    "# print(minLong, maxLong, minLat, maxLat)\n",
    "# print(longlatTable)\n",
    "def parse_origin_cal_lat_long(x):\n",
    "    if math.isnan(x['ORIGIN_STAND']):\n",
    "        return 0, 0\n",
    "    lat, long = longlatTable[ round(x['ORIGIN_STAND']) - 1][0], longlatTable[round(x['ORIGIN_STAND']) - 1][1]\n",
    "    \n",
    "    return (lat - minLat) / (maxLat - minLat), (long - minLong) / (maxLong - minLong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    x=x.values[0]\n",
    "    if x==\"A\":\n",
    "        return 1, 0, 0\n",
    "    elif x==\"B\":\n",
    "        return 0, 1, 0\n",
    "    elif x==\"C\":\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "# df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "# df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"HR\"].value_counts()\n",
    "# print(vals)\n",
    "# def reduce(x):\n",
    "#     return x - 2013\n",
    "# hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "# print(hr_oh)\n",
    "# print(hr_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "\n",
    "# # df_tr[\"TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(reduce)\n",
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "# oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"TAXI_ID\"].values))\n",
    "# print(oh)\n",
    "# print(oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr[[\"CALL_A\", \"LEN\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273 279 226 305 355]\n"
     ]
    }
   ],
   "source": [
    "# Feature Trimming\n",
    "# mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "# median = df_tr[\"LEN\"].median()\n",
    "# outlier_threshold = 3\n",
    "# df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "# print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "# print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "# df_tr = df_trimmed\n",
    "\n",
    "testcsvv = pd.read_csv('archive/test_public.csv')\n",
    "testcsvv[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR']] = testcsvv[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "unique_test_dates = np.array(testcsvv['DAYOFYEAR'].value_counts().keys())\n",
    "unique_taxi_ids = np.array(testcsvv['TAXI_ID'].value_counts().keys())\n",
    "unique_OC = np.array(testcsvv['ORIGIN_CALL'].value_counts().keys())\n",
    "unique_OS = np.array(testcsvv['ORIGIN_STAND'].value_counts().keys())\n",
    "\n",
    "print(unique_test_dates)\n",
    "def parse_booleans(x):\n",
    "  weekend = x.WK == 5 or x.WK == 6\n",
    "  isuniqueday = x.DAYOFYEAR in unique_test_dates\n",
    "  isuniquetaxi = x.TAXI_ID in unique_taxi_ids\n",
    "  isOC = x.ORIGIN_CALL in unique_OC\n",
    "  isOS = x.ORIGIN_STAND in unique_OS\n",
    "  return int(weekend), int(isuniqueday), int(isuniquetaxi), int(isOS), int(isOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 855335\n",
      "After Trimming: 824883\n",
      "1\n",
      "2002.0     28356\n",
      "63882.0     3155\n",
      "2001.0      1202\n",
      "14123.0      489\n",
      "2024.0       395\n",
      "           ...  \n",
      "34007.0        1\n",
      "44696.0        1\n",
      "85698.0        1\n",
      "31780.0        1\n",
      "76232.0        1\n",
      "Name: ORIGIN_CALL, Length: 62, dtype: int64\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "loaded_train = pd.read_csv(\"archive/train.csv\").sample(frac=.5)\n",
    "testcsvv = pd.read_csv(\"archive/test_public.csv\")\n",
    "testOC = testcsvv['ORIGIN_CALL'].values\n",
    "df_tr = loaded_train.copy(deep=True)\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "outlier_threshold = 3\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "df_trimmed = df_trimmed[df_trimmed['LEN'] > 25]\n",
    "print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "df_tr = df_trimmed\n",
    "\n",
    "\n",
    "\n",
    "trlen = len(df_tr)\n",
    "\n",
    "df_ts = pd.read_csv(\"archive/test_public.csv\")\n",
    "df_ts[\"POLYLINE\"]=\"trololololo\"\n",
    "\n",
    "\n",
    "df_both = pd.concat([df_tr, df_ts])\n",
    "df_both[\"LEN\"] = df_both[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "ocvc = df_both[\"ORIGIN_CALL\"].value_counts()\n",
    "def filterOC(x):\n",
    "#     print(x)\n",
    "    val = x.values[0]\n",
    "    if pd.isnull(val):\n",
    "        return float('nan')\n",
    "    if(ocvc[val] > 1000 or val in testOC):\n",
    "        return val\n",
    "    return float('nan')\n",
    "\n",
    "df_both[\"ORIGIN_CALL\"] = df_both[[\"ORIGIN_CALL\"]].apply(filterOC, axis = 1, result_type=\"expand\")\n",
    "print('1')\n",
    "print(df_both[\"ORIGIN_CALL\"].value_counts())\n",
    "df_both[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR']] = df_both[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "print('1')\n",
    "df_both[\"MIDMINS\"] = df_both[[\"TIMESTAMP\"]].apply(parse_midnight_minutes, axis=1, result_type=\"expand\")\n",
    "print('1')\n",
    "df_both[[\"LAT_ORIGIN\", \"LONG_ORIGIN\"]] = df_both[[\"ORIGIN_STAND\"]].apply(parse_origin_cal_lat_long, axis=1, result_type=\"expand\")\n",
    "print('1')\n",
    "df_both[['IS_WEEKEND', 'IS_UNIQUE_DAY', 'TAXI_ID_IN_TEST_SET', 'ORIGIN_STAND_IN_TEST_SET', 'ORIGIN_CALL_IN_TEST_SET']] = df_both[[\"WK\", 'DAYOFYEAR', 'TAXI_ID', 'ORIGIN_STAND', 'ORIGIN_CALL']].apply(parse_booleans, axis=1, result_type=\"expand\")\n",
    "\n",
    "#df_tr[\"MIDMINS\"]=(df_tr[\"MIDMINS\"]-df_tr[\"MIDMINS\"].min())/(df_tr[\"MIDMINS\"].max()-df_tr[\"MIDMINS\"].min())\n",
    "df_both = pd.get_dummies(data=df_both, columns=['CALL_TYPE', \"ORIGIN_STAND\", \"ORIGIN_CALL\"])\n",
    "\n",
    "df_tr = df_both.iloc[:trlen]\n",
    "df_ts = df_both.iloc[trlen:]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(df_tr.columns)\n",
    "# import pandas as pd\n",
    "df_tr.to_csv(\"df_tr.csv\", index=None)\n",
    "df_ts.to_csv(\"df_ts.csv\", index=None)\n",
    "# df_tr = pd.read_csv(\"df_tr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ts = pd.read_csv(\"df_ts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824883, 149)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped count:  1\n"
     ]
    }
   ],
   "source": [
    "acceptable_taxi_ids = []\n",
    "dropped = 0\n",
    "for val, cnt in df_tr['TAXI_ID'].value_counts().items():\n",
    "#     print(val, cnt, np.mean(df_tr[df_tr['TAXI_ID'] == val]['LEN']))\n",
    "    if np.mean(df_tr[df_tr['TAXI_ID'] == val]['LEN']) < 400:\n",
    "        dropped += 1\n",
    "    else:\n",
    "        acceptable_taxi_ids.append(val)\n",
    "        \n",
    "print('Dropped count:  ' + str(dropped))\n",
    "x_train_unfiltered_dropped_ids = df_tr[df_tr['TAXI_ID'].isin(acceptable_taxi_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDMINS\n",
      "YR\n",
      "MON\n",
      "DAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22119/2833014413.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tr[feature] = (df_tr[feature] - minVal) / (maxVal - minVal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "WK\n",
      "DAYOFYEAR\n",
      "LAT_ORIGIN\n",
      "LONG_ORIGIN\n",
      "MIDMINS\n",
      "YR\n",
      "MON\n",
      "DAY\n",
      "HR\n",
      "WK\n",
      "DAYOFYEAR\n",
      "LAT_ORIGIN\n",
      "LONG_ORIGIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22119/2833014413.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tr[feature] = 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def normalize(df_tr, features):\n",
    "    for feature in features:\n",
    "        print (feature)\n",
    "        maxVal = np.max(df_tr[feature])\n",
    "        minVal = np.min(df_tr[feature])\n",
    "        if (maxVal == minVal):\n",
    "            df_tr[feature] = 1.0\n",
    "        else:\n",
    "            df_tr[feature] = (df_tr[feature] - minVal) / (maxVal - minVal)\n",
    "            \n",
    "normalize(df_tr, [\"MIDMINS\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR', 'LAT_ORIGIN', 'LONG_ORIGIN'])\n",
    "normalize(df_ts, [\"MIDMINS\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR', 'LAT_ORIGIN', 'LONG_ORIGIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parse_booleans({'TIMESTAMP': 10990421})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    794964\n",
       "1     29919\n",
       "Name: ORIGIN_CALL_IN_TEST_SET, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_tr['IS_WEEKEND'] = df_tr['IS_WEEKEND'].astype('float64')\n",
    "# df_tr['IS_UNIQUE_DAY'] = df_tr['IS_UNIQUE_DAY'].astype('float64')\n",
    "# df_tr['TAXI_ID_IN_TEST_SET'] = df_tr['TAXI_ID_IN_TEST_SET'].astype('float64')\n",
    "# df_tr['ORIGIN_STAND_IN_TEST_SET'] = df_tr['ORIGIN_STAND_IN_TEST_SET'].astype('float64')\n",
    "# df_tr['ORIGIN_CALL_IN_TEST_SET'] = df_tr['ORIGIN_CALL_IN_TEST_SET'].astype('float64')\n",
    "\n",
    "# df_both[['IS_WEEKEND', 'IS_UNIQUE_DAY', 'TAXI_ID_IN_TEST_SET', 'ORIGIN_STAND_IN_TEST_SET', 'ORIGIN_CALL_IN_TEST_SET']] = df_both[[\"WK\", 'DAYOFYEAR', 'TAXI_ID', 'ORIGIN_STAND', 'ORIGIN_CALL']].apply(parse_booleans, axis=1, result_type=\"expand\")\n",
    "df_tr['ORIGIN_CALL_IN_TEST_SET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, df):\n",
    "\n",
    "    boolcols = list(df.columns)\n",
    "    badcols = [\"TRIP_ID\", 'MIDMINS',  'TIMESTAMP', 'MISSING_DATA', 'POLYLINE', 'LEN', \"TAXI_ID\", \"DAY_TYPE\",\n",
    "              \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR', 'LAT_ORIGIN', 'LONG_ORIGIN',\n",
    "              ] \n",
    "    \n",
    "    for b in badcols:\n",
    "#       print(b)\n",
    "      boolcols.remove(b)\n",
    "    # print(boolcols)\n",
    "    \n",
    "#     print(boolcols)\n",
    "    \n",
    "    boolz=df[boolcols].values\n",
    "    intz = df[[\"MIDMINS\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR', 'LAT_ORIGIN', 'LONG_ORIGIN']].values\n",
    "    # print(boolz)\n",
    "    y=df[\"LEN\"].values\n",
    "    \n",
    "    print(df[[\"MIDMINS\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'DAYOFYEAR', 'LAT_ORIGIN', 'LONG_ORIGIN',\n",
    "              'IS_WEEKEND', 'IS_UNIQUE_DAY', 'TAXI_ID_IN_TEST_SET', 'ORIGIN_STAND_IN_TEST_SET', 'ORIGIN_CALL_IN_TEST_SET']].dtypes)\n",
    "    \n",
    "    booltens = torch.tensor(boolz,dtype=torch.float32)\n",
    "    inttens = torch.tensor(intz,dtype=torch.float32)\n",
    "    # print(booltens.shape)\n",
    "    # print(inttens.shape)\n",
    " \n",
    "    self.x_train=torch.cat([booltens, inttens], dim=1)\n",
    "    # print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDMINS                     float64\n",
      "YR                          float64\n",
      "MON                         float64\n",
      "DAY                         float64\n",
      "HR                          float64\n",
      "WK                          float64\n",
      "DAYOFYEAR                   float64\n",
      "LAT_ORIGIN                  float64\n",
      "LONG_ORIGIN                 float64\n",
      "IS_WEEKEND                    int64\n",
      "IS_UNIQUE_DAY                 int64\n",
      "TAXI_ID_IN_TEST_SET           int64\n",
      "ORIGIN_STAND_IN_TEST_SET      int64\n",
      "ORIGIN_CALL_IN_TEST_SET       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "trds=MyDataset(df_tr)\n",
    "\n",
    "# trds, valds = random_split(trds, [0.9, 0.1])\n",
    "# print(len(trds), len(valds))\n",
    "# val_loader=DataLoader(valds)\n",
    "# trds.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(trds,batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        neuron = 64\n",
    "        self.dro = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(142, neuron)\n",
    "        self.fc2 = nn.Linear(neuron, neuron)\n",
    "        self.fc3 = nn.Linear(neuron, neuron)\n",
    "        self.fc4 = nn.Linear(neuron, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dro(x)\n",
    "        x = self.fc4(x)\n",
    "        # x = self.dro(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1793, 1.0000,\n",
       "        0.2727, 0.6333, 0.1739, 1.0000, 0.2995, 0.2045, 0.7346])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "net = Net()\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "criterion = RMSELoss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=.0001)\n",
    "\n",
    "trds.x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 620.688\n",
      "[1,  3223] loss: 447.959\n",
      "Epoch: 1, steps:3223\n",
      "[2,  2000] loss: 439.417\n",
      "[2,  3223] loss: 437.053\n",
      "Epoch: 2, steps:6446\n",
      "[3,  2000] loss: 433.167\n",
      "[3,  3223] loss: 433.685\n",
      "Epoch: 3, steps:9669\n",
      "[4,  2000] loss: 431.621\n",
      "[4,  3223] loss: 432.057\n",
      "Epoch: 4, steps:12892\n",
      "[5,  2000] loss: 430.069\n",
      "[5,  3223] loss: 430.442\n",
      "Epoch: 5, steps:16115\n",
      "[6,  2000] loss: 428.643\n",
      "[6,  3223] loss: 428.669\n",
      "Epoch: 6, steps:19338\n",
      "[7,  2000] loss: 427.232\n",
      "[7,  3223] loss: 428.297\n",
      "Epoch: 7, steps:22561\n",
      "[8,  2000] loss: 426.429\n",
      "[8,  3223] loss: 427.119\n",
      "Epoch: 8, steps:25784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22119/415869120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# loss = criterion(outputs, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "total_loss = 0.0\n",
    "iter = 0\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        losses.append(total_loss / iter)\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            #print(outputs[0], labels[0])\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / (i % 2000):.3f}')\n",
    "    print(\"Epoch: \" + str(epoch + 1) + \", steps:\" + str(iter))\n",
    "    # epoch_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "print(len(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDMINS                     float64\n",
      "YR                          float64\n",
      "MON                         float64\n",
      "DAY                         float64\n",
      "HR                          float64\n",
      "WK                          float64\n",
      "DAYOFYEAR                   float64\n",
      "LAT_ORIGIN                  float64\n",
      "LONG_ORIGIN                 float64\n",
      "IS_WEEKEND                    int64\n",
      "IS_UNIQUE_DAY                 int64\n",
      "TAXI_ID_IN_TEST_SET           int64\n",
      "ORIGIN_STAND_IN_TEST_SET      int64\n",
      "ORIGIN_CALL_IN_TEST_SET       int64\n",
      "dtype: object\n",
      "    TRIP_ID  TRAVEL_TIME\n",
      "0        T1   748.783630\n",
      "1        T2   652.466553\n",
      "2        T3   748.511780\n",
      "3        T4   618.670837\n",
      "4        T5   696.232117\n",
      "..      ...          ...\n",
      "315    T323   790.033997\n",
      "316    T324   663.543762\n",
      "317    T325   762.361572\n",
      "318    T326   790.305908\n",
      "319    T327   816.552551\n",
      "\n",
      "[320 rows x 2 columns]\n",
      "mean 708.8574314117432\n",
      "std 65.0791465571354\n",
      "median 714.3421936035156\n",
      "min 537.7814331054688\n",
      "max 899.4182739257812\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fe67b124970>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "testset = MyDataset(df_ts)\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(output.item())\n",
    "\n",
    "# preds = [630] * 320\n",
    "# print(preds)\n",
    "ids = testset.df[\"TRIP_ID\"]\n",
    "# print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"normalizedFeaturesDidntOneHotEncodeEverything30EpochesAgain.csv\", index=None)\n",
    "labels = preds\n",
    "# print(labels)\n",
    "print('mean', np.mean(labels))\n",
    "print('std', np.std(labels))\n",
    "print('median', np.median(labels))\n",
    "print('min', np.min(labels))\n",
    "print('max', np.max(labels))\n",
    "print('----------')\n",
    "sns.displot(labels).set(title='Travel Times Generated by Neural Network')\n",
    "#Try to include all the stats in your kaggle submissions and save the image / send to discord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xRange = range(1, len(losses) + 1)\n",
    "\n",
    "plt.plot(xRange, losses)\n",
    "\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.title(\"RMSE Loss as Neural Network is Trained\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "net.eval()\n",
    "# Create a list to store the losses\n",
    "data_point_losses = []\n",
    "\n",
    "# Iterate over the dataset and compute the losses\n",
    "with torch.no_grad():\n",
    "    print(trds.df.shape)\n",
    "    for i in range(trds.df.shape[0]):\n",
    "        if i % 100000 == 0:\n",
    "            print(\"i: \", i)\n",
    "        # print(trds.df[i:i+1])\n",
    "        input = trds.x_train[i]\n",
    "        output = net(input)\n",
    "        label = trds.y_train[i]\n",
    "        # print(input)\n",
    "        # print('output: ', output.item())\n",
    "        # print('label: ', label.item())\n",
    "        # print('loss: ', criterion(output.item(), label).item())\n",
    "        loss = criterion(output, label)\n",
    "        data_point_losses.append(loss.item())  # Append the loss value to the list\n",
    "\n",
    "# Convert the losses to a tensor\n",
    "loss_tensor = torch.tensor(data_point_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices and values of the top 10 highest losses\n",
    "top_loss_values, top_loss_indices = torch.topk(loss_tensor, k=10, largest=True)\n",
    "\n",
    "# Print the indices and corresponding losses\n",
    "for value, index in zip(top_loss_values, top_loss_indices):\n",
    "    # print(\"Index:\", index.item(), \"Loss:\", value.item())\n",
    "    polyline = trds.df[index.item():index.item()+1]['POLYLINE'].values[0]\n",
    "    polyline_array = ast.literal_eval(polyline)\n",
    "    longitude_values = [row[0] for row in polyline_array]  # First column elements\n",
    "    latitude_values = [row[1] for row in polyline_array]  # Second column elements\n",
    "    print(polyline_array)\n",
    "    plt.plot(longitude_values, latitude_values, label = str(index.item()), alpha=0.6)\n",
    "\n",
    "    # print(x_vals)\n",
    "\n",
    "# plt.plot()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "# plt.legend()\n",
    "plt.title(\"Map of 10 Taxi Trips with Highest Training Error\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     total_loss = 0.0\n",
    "#     for i, data in enumerate(val_loader, 0):\n",
    "\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         labels=labels\n",
    "#         outputs = net(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "#         # print statistics\n",
    "#         total_loss += loss.item()\n",
    "#         # if i%1000==0:\n",
    "#         #     print(round(loss.item()), outputs, labels)\n",
    "\n",
    "# print(f'Total Loss: {total_loss / len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample submission file that is given on kaggle\n",
    "# df_sample = pd.read_csv(\"archive/sampleSubmission.csv\")\n",
    "\n",
    "# df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# # mean(716.43) -> 792.73593\n",
    "# # median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosted Decision Trees**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCSV(csvname):\n",
    "  xg_train = pd.read_csv(csvname)\n",
    "  df_tr = xg_train\n",
    "  if \"POLYLINE\" not in df_tr: #test dataset\n",
    "    df_tr[\"POLYLINE\"]=\"[[[\"\n",
    "    df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "  else: \n",
    "    df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "    mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "    median = df_tr[\"LEN\"].median()\n",
    "    outlier_threshold = 3\n",
    "    print(mean, std)\n",
    "    df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "    df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "    print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "    print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "    df_tr = df_trimmed\n",
    "    # first trim the dataset\n",
    "\n",
    "  df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"DAYOFYEAR\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "  df_tr[\"MIDMINS\"] = df_tr[[\"TIMESTAMP\"]].apply(parse_midnight_minutes, axis=1, result_type=\"expand\")\n",
    "  \n",
    "  LetterToIndex = {'A': 0, 'B': 1, 'C': 2}\n",
    "  df_trimmed_copy = df_tr\n",
    "  # print('there was ' + str(len(df_trimmed_copy['LEN'] == 0)) + \" zeroes\")\n",
    "  # df_trimmed_copy = df_trimmed_copy[df_trimmed_copy['LEN'] != 0]\n",
    "  y_train = df_trimmed_copy[\"LEN\"]\n",
    "  df_trimmed_copy[\"CALL_TYPE\"] = df_trimmed_copy[\"CALL_TYPE\"].map(LetterToIndex)\n",
    "  # df_trimmed_copy[\"DAY_TYPE\"] = df_trimmed_copy[\"DAY_TYPE\"].map(LetterToIndex)\n",
    "  #took out \"TRIP_ID\"\n",
    "  # x_train = df_trimmed_copy[[\"TRIP_ID\", \"CALL_TYPE\", \"ORIGIN_CALL\", \"ORIGIN_STAND\", \"TAXI_ID\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"MIDMINS\"]]\n",
    "  return df_trimmed_copy, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_unfiltered, y_train = processCSV('archive/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culled_set = x_train_unfiltered\n",
    "testcsvv =pd.read_csv('archive/test_public.csv')\n",
    "\n",
    "test_counts = testcsvv['ORIGIN_CALL'].value_counts()\n",
    "culled_set = x_train_unfiltered[x_train_unfiltered['ORIGIN_CALL'].isin(np.array(test_counts.index))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcsvv[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"DAYOFYEAR\"]] = testcsvv[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "dayofyear = testcsvv['DAYOFYEAR'].value_counts()\n",
    "\n",
    "culled_set = culled_set[culled_set['DAYOFYEAR'].isin(np.array(dayofyear.index))]\n",
    "# culled_set.shape\n",
    "print('mean', np.mean(culled_set['LEN']))\n",
    "print('std', np.std(culled_set['LEN']))\n",
    "print('median', np.median(culled_set['LEN']))\n",
    "print('min', np.min(culled_set['LEN']))\n",
    "print('max', np.max(culled_set['LEN']))\n",
    "plt.hist(culled_set['LEN'])\n",
    "plt.show()\n",
    "print(culled_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 999):\n",
    "    a = x_train_unfiltered['DAYOFYEAR']\n",
    "    vals = a.value_counts()\n",
    "    total = 0\n",
    "    for val in vals:\n",
    "        total += val\n",
    "    print('total ', total)\n",
    "    print(a.shape)\n",
    "    for val, cnt in vals.items():\n",
    "        print(val, cnt, cnt/total)\n",
    "    # print(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 999):\n",
    "    a = x_train_unfiltered['TAXI_ID']\n",
    "    vals = a.value_counts()\n",
    "    total = 0\n",
    "    for val in vals:\n",
    "        total += val\n",
    "    print('total ', total)\n",
    "    print(a.shape)\n",
    "    for val, cnt in vals.items():\n",
    "        print(val, cnt, cnt/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 999):\n",
    "    a = x_train_unfiltered[x_train_unfiltered['LEN'] == 0]['TAXI_ID']\n",
    "    vals = a.value_counts()\n",
    "    total = 0\n",
    "    for val in vals:\n",
    "        total += val\n",
    "    print('total ', total)\n",
    "    print(a.shape)\n",
    "    for val, cnt in vals.items():\n",
    "        print(val, cnt, cnt/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = max(x_train_unfiltered[x_train_unfiltered['DAYOFYEAR'] == 1]['LEN'])\n",
    "print(a)\n",
    "days = range(1, 366)\n",
    "weekdays= range(0, 7)\n",
    "\n",
    "daysInTest = [279, 273, 226, 304, 355, 272]\n",
    "days_filtered = A = [i for i in days if i not in daysInTest]\n",
    "\n",
    "dayvals = []\n",
    "specialvals = []\n",
    "for day in days:\n",
    "    if day not in daysInTest:\n",
    "        dayvals.append(np.mean(x_train_unfiltered[x_train_unfiltered['DAYOFYEAR'] == day]['LEN']))\n",
    "    else:\n",
    "        specialvals.append(np.mean(x_train_unfiltered[x_train_unfiltered['DAYOFYEAR'] == day]['LEN']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiIdValues = x_train_unfiltered['TAXI_ID'].value_counts()\n",
    "taxiMin = min(x_train_unfiltered['TAXI_ID'])\n",
    "\n",
    "x_axis_id_minus = []\n",
    "y_axis_avg_len = []\n",
    "for val, cnt in taxiIdValues.items():\n",
    "    x_axis_id_minus.append(val - taxiMin)\n",
    "    y_axis_avg_len.append(np.mean(x_train_unfiltered[x_train_unfiltered['TAXI_ID'] == val]['LEN']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_axis_id_minus)):\n",
    "    if y_axis_avg_len[i] < 50:\n",
    "        taxId = x_axis_id_minus[i] + taxiMin\n",
    "        print(taxId, \": \")\n",
    "        print(x_train_unfiltered[x_train_unfiltered['TAXI_ID'] == taxId]['LEN'].values)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x_axis_id_minus, y_axis_avg_len)\n",
    "# plt.xlabel('TAXI Id')\n",
    "# plt.ylabel('LEN')\n",
    "# plt.title('Date vs. TAxi ID')\n",
    "\n",
    "# Display the plot\n",
    "# plt.show()\n",
    "# Create a scatter plot\n",
    "plt.scatter(days_filtered, dayvals)\n",
    "plt.scatter(daysInTest, specialvals, color='red')\n",
    "\n",
    "plt.xlabel('Day of year')\n",
    "plt.ylabel('LEN')\n",
    "\n",
    "\n",
    "plt.title('Date vs. Trip length')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(daysInTest)\n",
    "print(specialvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_unfiltered[[\"TRIP_ID\", \"CALL_TYPE\", \"ORIGIN_CALL\", \"ORIGIN_STAND\", \"TAXI_ID\", \"YR\", \"MON\", \"HR\", \"WK\", \"DAYOFYEAR\", \"MIDMINS\", \"ORIGIN_\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(columns=['TRIP_ID'])#this doesnt actually drop, just without tripid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# shuffle_indices = np.random.permutation(len(x_train))\n",
    "# X_shuffled = x_train[shuffle_indices]\n",
    "# y_shuffled = y_train[shuffle_indices]\n",
    "\n",
    "combined_df = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "# Get the number of rows in the DataFrame\n",
    "num_rows = combined_df.shape[0]\n",
    "\n",
    "# Generate a random permutation of indices\n",
    "perm = np.random.permutation(num_rows)\n",
    "\n",
    "# Shuffle the combined DataFrame using the permutation\n",
    "shuffled_df = combined_df.iloc[perm]\n",
    "\n",
    "shuffled_df['CALL_TYPE'] = shuffled_df['CALL_TYPE'].astype('category')\n",
    "shuffled_df['ORIGIN_CALL'] = shuffled_df['ORIGIN_CALL'].astype('category')\n",
    "shuffled_df['ORIGIN_STAND'] = shuffled_df['ORIGIN_STAND'].astype('category')\n",
    "shuffled_df['TAXI_ID'] = shuffled_df['TAXI_ID'].astype('category')\n",
    "shuffled_df['YR'] = shuffled_df['YR'].astype('category')\n",
    "shuffled_df['MON'] = shuffled_df['MON'].astype('category')\n",
    "shuffled_df['HR'] = shuffled_df['HR'].astype('category')\n",
    "shuffled_df['WK'] = shuffled_df['WK'].astype('category')\n",
    "shuffled_df['DAYOFYEAR'] = shuffled_df['DAYOFYEAR'].astype('category')\n",
    "\n",
    "# Split the shuffled DataFrame back into separate DataFrames\n",
    "shuffled_train_df = shuffled_df.iloc[:, :-1]\n",
    "shuffled_label_df = shuffled_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(shuffled_train_df, shuffled_label_df, test_size=0.2, random_state=None)\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and target variables\n",
    "\n",
    "# Convert DataFrame to DMatrix\n",
    "# feature_types = ['c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'q']\n",
    "\n",
    "data_matrix = xgb.DMatrix(data = X_train.drop(columns=['TRIP_ID']), label=Y_train, enable_categorical=True)\n",
    "validation_matrix = xgb.DMatrix(data = X_val.drop(columns=['TRIP_ID']), label=Y_val, enable_categorical=True)\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    # 'colsample_bytree': 0.5586586280723452,\n",
    "    # 'learning_rate': 0.06889888561468978,\n",
    "    'max_depth': 6,\n",
    "    # 'n_estimators': 1164,\n",
    "    # 'reg_alpha': 25.287638310615133,\n",
    "    # 'reg_lambda': 67.51702941180568,\n",
    "    # 'subsample': 0.77179392553171\n",
    "}\n",
    "\n",
    "xg_model = xgb.train(parameters, data_matrix, num_boost_round=2000, early_stopping_rounds=10, evals=[(validation_matrix, 'validation')], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = processCSV('archive/test_public.csv')\n",
    "test_x = test_x.drop(columns=[\"DAY_TYPE\", \"MISSING_DATA\", 'POLYLINE', 'LEN', 'TIMESTAMP', 'DAY'])\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x['CALL_TYPE'] = test_x['CALL_TYPE'].astype('category')\n",
    "test_x['ORIGIN_CALL'] = test_x['ORIGIN_CALL'].astype('category')\n",
    "test_x['ORIGIN_STAND'] = test_x['ORIGIN_STAND'].astype('category')\n",
    "test_x['TAXI_ID'] = test_x['TAXI_ID'].astype('category')\n",
    "test_x['YR'] = test_x['YR'].astype('category')\n",
    "test_x['MON'] = test_x['MON'].astype('category')\n",
    "test_x['HR'] = test_x['HR'].astype('category')\n",
    "test_x['WK'] = test_x['WK'].astype('category')\n",
    "test_x['DAYOFYEAR'] = test_x['DAYOFYEAR'].astype('category')\n",
    "y_pred = xg_model.predict(xgb.DMatrix(test_x.drop(columns=['TRIP_ID']), enable_categorical=True))\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ids = test_x[\"TRIP_ID\"].values\n",
    "# print(ids)\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred[i] = y_pred[i]\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : y_pred}\n",
    "newdf = pd.DataFrame(d)\n",
    "# print(newdf)\n",
    "newdf.to_csv(\"filteredToDayOfYearCategoricalBoostedSubmitTomorrow.csv\", index=None)\n",
    "labels = y_pred\n",
    "# print(labels)\n",
    "print('mean', np.mean(labels))\n",
    "print('std', np.std(labels))\n",
    "print('median', np.median(labels))\n",
    "print('min', np.min(labels))\n",
    "print('max', np.max(labels))\n",
    "print('----------')\n",
    "sns.displot(labels).set(title='Travel Times Generated by Boosted Decision Tree')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcsvv =pd.read_csv('archive/test_public.csv')\n",
    "ids = test_x[\"TRIP_ID\"].values\n",
    "means = []\n",
    "dayOfYearToMeanMap = {\n",
    "    279: 649.5969399406257,\n",
    "    273: 615.9071133347835,\n",
    "    226: 753.1351183063512,\n",
    "    304: 616.6265060240963,\n",
    "    355: 733.6887668918919,\n",
    "    272: 640.0439491445613,\n",
    "#     305: 0\n",
    "}\n",
    "testcsvv[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"DAYOFYEAR\"]] = testcsvv[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "for xid in ids:\n",
    "    row = testcsvv[testcsvv['TRIP_ID'] == xid]['DAYOFYEAR'].values[0]\n",
    "    print(xid,row)\n",
    "    mean = dayOfYearToMeanMap[row]\n",
    "    means.append(mean)\n",
    "\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : y_preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "\n",
    "newdf.to_csv(\"MeanOf6UniqueDays.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_time({'TIMESTAMP':1414814368})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Feature Analysis\n",
    "\n",
    "For our feature analysis, we are looking at which of our engineered features may be useful in making a taxicab time regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n samples to analyze. Set to -1 to use all data\n",
    "end = -1\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "df_tr = x_train_unfiltered\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\n",
    "# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \n",
    "# how many 30s trips, etc. )\n",
    "buckets = (int(mean + outlier_threshold * std) // 15)\n",
    "\n",
    "print(f\"Using: {len(df_trimmed)}/{len(df_tr)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,14))\n",
    "\n",
    "# Now, we visualize some features that we think might be useful\n",
    "for idx, v in enumerate([\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"ORIGIN_STAND\"]):\n",
    "  # idx // 3 = row, idx % 3 = column\n",
    "  ax = axs[idx // 3, idx % 3]\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "  df_subset = df_trimmed.dropna(subset=v)\n",
    "  \n",
    "  # Create a histogram. Look up the documentation for more details\n",
    "  ax.hist2d(df_subset[v][:end], df_subset[\"LEN\"][:end], cmap=\"CMRmap\", bins=(120,buckets))\n",
    "  \n",
    "  # Some stylistic things to make the graphs look nice\n",
    "  ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "  ax.set_facecolor(\"black\")\n",
    "  ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "  ax.set_title(f\"Feature: {v}\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for v in [0, 5, 11, 17, 23]:\n",
    "  # Filter data where the HR matches v\n",
    "  hourly_data = df_trimmed[df_trimmed[\"HR\"] == v][\"LEN\"]\n",
    "  histogram, bin_boundary = np.histogram(hourly_data, bins=buckets)\n",
    "  histogram = histogram / len(hourly_data)\n",
    "  # The center is the left_bound and right_bound of a bucket\n",
    "  bin_centers = [(bin_boundary[i] + bin_boundary[i + 1]) / 2 for i in range(buckets)]\n",
    "  plt.plot(bin_centers, histogram, label=f\"HR={v}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
