{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "#df_tr = pd.read_csv(\"archive/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr.head()\n",
    "# avg = df_tr['LEN'].mean()\n",
    "# print(avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    x=x.values[0]\n",
    "    if x==\"A\":\n",
    "        return 1, 0, 0\n",
    "    elif x==\"B\":\n",
    "        return 0, 1, 0\n",
    "    elif x==\"C\":\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "# df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "# df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"HR\"].value_counts()\n",
    "# print(vals)\n",
    "# def reduce(x):\n",
    "#     return x - 2013\n",
    "# hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "# print(hr_oh)\n",
    "# print(hr_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "\n",
    "# # df_tr[\"TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(reduce)\n",
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "# oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"TAXI_ID\"].values))\n",
    "# print(oh)\n",
    "# print(oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr[[\"CALL_A\", \"LEN\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Trimming\n",
    "# mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "# median = df_tr[\"LEN\"].median()\n",
    "# outlier_threshold = 3\n",
    "# df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "# print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "# print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "# df_tr = df_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, csvpath):\n",
    "    df_tr = pd.read_csv(csvpath)\n",
    "\n",
    "    if \"POLYLINE\" not in df_tr: #test dataset\n",
    "      df_tr[\"POLYLINE\"]=\"trololololo\"\n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "    else: \n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "      # first trim the dataset\n",
    "\n",
    "      mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "      median = df_tr[\"LEN\"].median()\n",
    "      outlier_threshold = 3\n",
    "      df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "      df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "      df_trimmed = df_trimmed[df_trimmed['LEN'] != 0]\n",
    "      print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "      print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "      df_tr = df_trimmed\n",
    "\n",
    "\n",
    "    df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "    df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "    df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")\n",
    "    hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "    \n",
    "         \n",
    "    x=df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\", \"DAY_A\", \"DAY_B\", \"DAY_C\"]].values\n",
    "    \n",
    "    y=df_tr[\"LEN\"].values\n",
    " \n",
    "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "    self.x_train=torch.cat([self.x_train, hr_oh], dim=1)\n",
    "    print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df_tr\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 1710670\n",
      "After Trimming: 1656255\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "myDs=MyDataset(\"archive/train.csv\")\n",
    "train_loader=DataLoader(myDs,batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dro = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(30, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, -1) # flatten all dimensions except batch\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dro(x)\n",
    "        x = self.fc4(x)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "criterion = RMSELoss\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  6470] loss: 373.977\n",
      "[2,  6470] loss: 390.265\n",
      "[3,  6470] loss: 432.296\n",
      "[4,  6470] loss: 373.192\n",
      "[5,  6470] loss: 405.163\n",
      "[6,  6470] loss: 324.475\n",
      "[7,  6470] loss: 454.418\n",
      "[8,  6470] loss: 392.683\n",
      "[9,  6470] loss: 366.894\n",
      "[10,  6470] loss: 369.555\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            # print(outputs[0], labels[0])\n",
    "            # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = MyDataset(\"archive/test_public.csv\")\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(round(output.item()))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "ids = testset.df[\"TRIP_ID\"]        \n",
    "print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample submission file that is given on kaggle\n",
    "# df_sample = pd.read_csv(\"archive/sampleSubmission.csv\")\n",
    "\n",
    "# df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# # mean(716.43) -> 792.73593\n",
    "# # median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
