{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "#df_tr = pd.read_csv(\"archive/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr.head()\n",
    "# avg = df_tr['LEN'].mean()\n",
    "# print(avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    x=x.values[0]\n",
    "    if x==\"A\":\n",
    "        return 1, 0, 0\n",
    "    elif x==\"B\":\n",
    "        return 0, 1, 0\n",
    "    elif x==\"C\":\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "# df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "# df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"HR\"].value_counts()\n",
    "# print(vals)\n",
    "# def reduce(x):\n",
    "#     return x - 2013\n",
    "# hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "# print(hr_oh)\n",
    "# print(hr_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "\n",
    "# # df_tr[\"TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(reduce)\n",
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "# oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"TAXI_ID\"].values))\n",
    "# print(oh)\n",
    "# print(oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr[[\"CALL_A\", \"LEN\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Trimming\n",
    "# mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "# median = df_tr[\"LEN\"].median()\n",
    "# outlier_threshold = 3\n",
    "# df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "# print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "# print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "# df_tr = df_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, csvpath):\n",
    "    df_tr = pd.read_csv(csvpath)\n",
    "\n",
    "    if \"POLYLINE\" not in df_tr: #test dataset\n",
    "      df_tr[\"POLYLINE\"]=\"trololololo\"\n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "    else: \n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "      # first trim the dataset\n",
    "\n",
    "      mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "      median = df_tr[\"LEN\"].median()\n",
    "      outlier_threshold = 3\n",
    "      df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "      df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "      print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "      print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "      df_tr = df_trimmed\n",
    "\n",
    "\n",
    "    df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "    df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "    df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")\n",
    "    hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "    \n",
    "         \n",
    "    x=df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\", \"DAY_A\", \"DAY_B\", \"DAY_C\"]].values\n",
    "    \n",
    "    y=df_tr[\"LEN\"].values\n",
    " \n",
    "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "    self.x_train=torch.cat([self.x_train, hr_oh], dim=1)\n",
    "    print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df_tr\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 1710670\n",
      "After Trimming: 1692763\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "myDs=MyDataset(\"archive/train.csv\")\n",
    "train_loader=DataLoader(myDs,batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dro = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(30, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "        self.fc5 = nn.Linear(1000, 1000)\n",
    "        self.fc6 = nn.Linear(1000, 1000)\n",
    "        self.fc7 = nn.Linear(1000, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, -1) # flatten all dimensions except batch\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.dro(x)\n",
    "        x = self.fc7(x)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "criterion = RMSELoss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0748], grad_fn=<SelectBackward0>) tensor([660.])\n",
      "[1,   100] loss: 774.387\n",
      "tensor([0.2049], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,   200] loss: 786.993\n",
      "tensor([0.3354], grad_fn=<SelectBackward0>) tensor([1020.])\n",
      "[1,   300] loss: 781.526\n",
      "tensor([0.4569], grad_fn=<SelectBackward0>) tensor([795.])\n",
      "[1,   400] loss: 780.316\n",
      "tensor([0.6175], grad_fn=<SelectBackward0>) tensor([960.])\n",
      "[1,   500] loss: 774.131\n",
      "tensor([0.7467], grad_fn=<SelectBackward0>) tensor([780.])\n",
      "[1,   600] loss: 786.163\n",
      "tensor([0.9166], grad_fn=<SelectBackward0>) tensor([465.])\n",
      "[1,   700] loss: 778.604\n",
      "tensor([1.1427], grad_fn=<SelectBackward0>) tensor([60.])\n",
      "[1,   800] loss: 781.901\n",
      "tensor([1.3325], grad_fn=<SelectBackward0>) tensor([1155.])\n",
      "[1,   900] loss: 783.279\n",
      "tensor([1.7277], grad_fn=<SelectBackward0>) tensor([540.])\n",
      "[1,  1000] loss: 771.950\n",
      "tensor([2.0260], grad_fn=<SelectBackward0>) tensor([615.])\n",
      "[1,  1100] loss: 783.889\n",
      "tensor([2.4849], grad_fn=<SelectBackward0>) tensor([1035.])\n",
      "[1,  1200] loss: 777.049\n",
      "tensor([3.3230], grad_fn=<SelectBackward0>) tensor([585.])\n",
      "[1,  1300] loss: 772.777\n",
      "tensor([4.5585], grad_fn=<SelectBackward0>) tensor([240.])\n",
      "[1,  1400] loss: 772.348\n",
      "tensor([18.0926], grad_fn=<SelectBackward0>) tensor([540.])\n",
      "[1,  1500] loss: 772.870\n",
      "tensor([411.7370], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[1,  1600] loss: 530.892\n",
      "tensor([584.2563], grad_fn=<SelectBackward0>) tensor([690.])\n",
      "[1,  1700] loss: 400.819\n",
      "tensor([595.2554], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[1,  1800] loss: 418.304\n",
      "tensor([666.0537], grad_fn=<SelectBackward0>) tensor([300.])\n",
      "[1,  1900] loss: 404.793\n",
      "tensor([616.7363], grad_fn=<SelectBackward0>) tensor([360.])\n",
      "[1,  2000] loss: 393.028\n",
      "tensor([750.5099], grad_fn=<SelectBackward0>) tensor([750.])\n",
      "[1,  2100] loss: 414.874\n",
      "tensor([707.6878], grad_fn=<SelectBackward0>) tensor([210.])\n",
      "[1,  2200] loss: 412.458\n",
      "tensor([533.9363], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1,  2300] loss: 410.040\n",
      "tensor([510.6605], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1,  2400] loss: 403.193\n",
      "tensor([753.5220], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1,  2500] loss: 409.729\n",
      "tensor([806.4138], grad_fn=<SelectBackward0>) tensor([300.])\n",
      "[1,  2600] loss: 413.898\n",
      "tensor([488.7662], grad_fn=<SelectBackward0>) tensor([165.])\n",
      "[1,  2700] loss: 402.903\n",
      "tensor([669.3674], grad_fn=<SelectBackward0>) tensor([1275.])\n",
      "[1,  2800] loss: 404.433\n",
      "tensor([645.3999], grad_fn=<SelectBackward0>) tensor([870.])\n",
      "[1,  2900] loss: 392.831\n",
      "tensor([713.9051], grad_fn=<SelectBackward0>) tensor([420.])\n",
      "[1,  3000] loss: 402.095\n",
      "tensor([611.5622], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1,  3100] loss: 406.215\n",
      "tensor([606.9501], grad_fn=<SelectBackward0>) tensor([885.])\n",
      "[1,  3200] loss: 403.340\n",
      "tensor([525.2789], grad_fn=<SelectBackward0>) tensor([405.])\n",
      "[1,  3300] loss: 399.205\n",
      "tensor([707.5297], grad_fn=<SelectBackward0>) tensor([1080.])\n",
      "[1,  3400] loss: 400.517\n",
      "tensor([711.6157], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,  3500] loss: 391.846\n",
      "tensor([515.8073], grad_fn=<SelectBackward0>) tensor([480.])\n",
      "[1,  3600] loss: 391.186\n",
      "tensor([794.2908], grad_fn=<SelectBackward0>) tensor([855.])\n",
      "[1,  3700] loss: 393.695\n",
      "tensor([784.9819], grad_fn=<SelectBackward0>) tensor([1185.])\n",
      "[1,  3800] loss: 410.228\n",
      "tensor([744.2166], grad_fn=<SelectBackward0>) tensor([885.])\n",
      "[1,  3900] loss: 407.425\n",
      "tensor([573.6935], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1,  4000] loss: 400.048\n",
      "tensor([583.9008], grad_fn=<SelectBackward0>) tensor([2745.])\n",
      "[1,  4100] loss: 404.168\n",
      "tensor([533.2722], grad_fn=<SelectBackward0>) tensor([765.])\n",
      "[1,  4200] loss: 397.634\n",
      "tensor([591.7837], grad_fn=<SelectBackward0>) tensor([540.])\n",
      "[1,  4300] loss: 396.073\n",
      "tensor([513.8542], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1,  4400] loss: 404.078\n",
      "tensor([771.6421], grad_fn=<SelectBackward0>) tensor([1245.])\n",
      "[1,  4500] loss: 401.227\n",
      "tensor([703.5021], grad_fn=<SelectBackward0>) tensor([630.])\n",
      "[1,  4600] loss: 394.593\n",
      "tensor([848.6720], grad_fn=<SelectBackward0>) tensor([480.])\n",
      "[1,  4700] loss: 409.839\n",
      "tensor([624.2725], grad_fn=<SelectBackward0>) tensor([570.])\n",
      "[1,  4800] loss: 404.881\n",
      "tensor([759.2717], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1,  4900] loss: 393.293\n",
      "tensor([732.0986], grad_fn=<SelectBackward0>) tensor([855.])\n",
      "[1,  5000] loss: 400.033\n",
      "tensor([670.8773], grad_fn=<SelectBackward0>) tensor([450.])\n",
      "[1,  5100] loss: 392.915\n",
      "tensor([739.2362], grad_fn=<SelectBackward0>) tensor([810.])\n",
      "[1,  5200] loss: 399.623\n",
      "tensor([762.3776], grad_fn=<SelectBackward0>) tensor([615.])\n",
      "[1,  5300] loss: 403.038\n",
      "tensor([794.6721], grad_fn=<SelectBackward0>) tensor([780.])\n",
      "[1,  5400] loss: 400.240\n",
      "tensor([681.9709], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1,  5500] loss: 399.421\n",
      "tensor([653.6841], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[1,  5600] loss: 410.302\n",
      "tensor([737.8582], grad_fn=<SelectBackward0>) tensor([735.])\n",
      "[1,  5700] loss: 405.521\n",
      "tensor([666.9327], grad_fn=<SelectBackward0>) tensor([1185.])\n",
      "[1,  5800] loss: 408.028\n",
      "tensor([688.8322], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,  5900] loss: 403.350\n",
      "tensor([708.9886], grad_fn=<SelectBackward0>) tensor([630.])\n",
      "[1,  6000] loss: 397.235\n",
      "tensor([745.8660], grad_fn=<SelectBackward0>) tensor([1290.])\n",
      "[1,  6100] loss: 405.648\n",
      "tensor([591.4997], grad_fn=<SelectBackward0>) tensor([585.])\n",
      "[1,  6200] loss: 398.410\n",
      "tensor([675.5991], grad_fn=<SelectBackward0>) tensor([1365.])\n",
      "[1,  6300] loss: 410.902\n",
      "tensor([698.9233], grad_fn=<SelectBackward0>) tensor([585.])\n",
      "[1,  6400] loss: 408.237\n",
      "tensor([790.3861], grad_fn=<SelectBackward0>) tensor([2115.])\n",
      "[1,  6500] loss: 402.341\n",
      "tensor([554.1227], grad_fn=<SelectBackward0>) tensor([1440.])\n",
      "[1,  6600] loss: 404.262\n",
      "tensor([582.4341], grad_fn=<SelectBackward0>) tensor([285.])\n",
      "[1,  6700] loss: 399.319\n",
      "tensor([489.1084], grad_fn=<SelectBackward0>) tensor([750.])\n",
      "[1,  6800] loss: 402.208\n",
      "tensor([643.7318], grad_fn=<SelectBackward0>) tensor([525.])\n",
      "[1,  6900] loss: 407.115\n",
      "tensor([481.5385], grad_fn=<SelectBackward0>) tensor([630.])\n",
      "[1,  7000] loss: 397.303\n",
      "tensor([681.3113], grad_fn=<SelectBackward0>) tensor([930.])\n",
      "[1,  7100] loss: 394.941\n",
      "tensor([661.5593], grad_fn=<SelectBackward0>) tensor([705.])\n",
      "[1,  7200] loss: 396.229\n",
      "tensor([507.3406], grad_fn=<SelectBackward0>) tensor([450.])\n",
      "[1,  7300] loss: 386.299\n",
      "tensor([804.2698], grad_fn=<SelectBackward0>) tensor([450.])\n",
      "[1,  7400] loss: 404.630\n",
      "tensor([592.7278], grad_fn=<SelectBackward0>) tensor([465.])\n",
      "[1,  7500] loss: 401.985\n",
      "tensor([616.3359], grad_fn=<SelectBackward0>) tensor([840.])\n",
      "[1,  7600] loss: 389.502\n",
      "tensor([848.1132], grad_fn=<SelectBackward0>) tensor([660.])\n",
      "[1,  7700] loss: 394.351\n",
      "tensor([754.7329], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1,  7800] loss: 390.499\n",
      "tensor([642.7429], grad_fn=<SelectBackward0>) tensor([270.])\n",
      "[1,  7900] loss: 399.633\n",
      "tensor([589.7432], grad_fn=<SelectBackward0>) tensor([555.])\n",
      "[1,  8000] loss: 400.757\n",
      "tensor([639.3235], grad_fn=<SelectBackward0>) tensor([525.])\n",
      "[1,  8100] loss: 409.424\n",
      "tensor([585.4550], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1,  8200] loss: 401.056\n",
      "tensor([824.3803], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1,  8300] loss: 396.584\n",
      "tensor([674.4104], grad_fn=<SelectBackward0>) tensor([405.])\n",
      "[1,  8400] loss: 402.701\n",
      "tensor([608.2036], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[1,  8500] loss: 394.232\n",
      "tensor([574.3989], grad_fn=<SelectBackward0>) tensor([525.])\n",
      "[1,  8600] loss: 398.070\n",
      "tensor([790.8934], grad_fn=<SelectBackward0>) tensor([600.])\n",
      "[1,  8700] loss: 404.651\n",
      "tensor([625.8176], grad_fn=<SelectBackward0>) tensor([1410.])\n",
      "[1,  8800] loss: 406.574\n",
      "tensor([665.7599], grad_fn=<SelectBackward0>) tensor([600.])\n",
      "[1,  8900] loss: 404.609\n",
      "tensor([548.9777], grad_fn=<SelectBackward0>) tensor([930.])\n",
      "[1,  9000] loss: 401.067\n",
      "tensor([641.6555], grad_fn=<SelectBackward0>) tensor([360.])\n",
      "[1,  9100] loss: 404.730\n",
      "tensor([578.5469], grad_fn=<SelectBackward0>) tensor([615.])\n",
      "[1,  9200] loss: 397.237\n",
      "tensor([551.6334], grad_fn=<SelectBackward0>) tensor([1035.])\n",
      "[1,  9300] loss: 393.864\n",
      "tensor([773.9026], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,  9400] loss: 403.225\n",
      "tensor([512.9692], grad_fn=<SelectBackward0>) tensor([255.])\n",
      "[1,  9500] loss: 399.922\n",
      "tensor([657.0638], grad_fn=<SelectBackward0>) tensor([390.])\n",
      "[1,  9600] loss: 393.485\n",
      "tensor([622.5284], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,  9700] loss: 407.240\n",
      "tensor([509.8594], grad_fn=<SelectBackward0>) tensor([1695.])\n",
      "[1,  9800] loss: 402.208\n",
      "tensor([809.9175], grad_fn=<SelectBackward0>) tensor([585.])\n",
      "[1,  9900] loss: 410.137\n",
      "tensor([632.6453], grad_fn=<SelectBackward0>) tensor([540.])\n",
      "[1, 10000] loss: 395.214\n",
      "tensor([557.0508], grad_fn=<SelectBackward0>) tensor([165.])\n",
      "[1, 10100] loss: 403.436\n",
      "tensor([712.7365], grad_fn=<SelectBackward0>) tensor([465.])\n",
      "[1, 10200] loss: 398.223\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(outputs[0], labels[0])\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_call = df_tr.sort_values(by='ORIGIN_CALL', ascending=True)\n",
    "# print(zero_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "[748, 769, 800, 784, 838, 830, 842, 812, 767, 760, 794, 837, 809, 831, 880, 807, 805, 712, 817, 791, 806, 857, 828, 840, 746, 844, 779, 846, 768, 793, 743, 752, 794, 819, 793, 817, 838, 828, 776, 848, 771, 805, 795, 746, 757, 793, 854, 854, 767, 837, 835, 831, 743, 766, 800, 751, 795, 786, 885, 849, 869, 826, 794, 770, 764, 762, 818, 869, 809, 810, 759, 860, 742, 793, 665, 745, 682, 754, 738, 713, 773, 658, 741, 711, 709, 721, 727, 724, 624, 813, 735, 719, 700, 697, 682, 816, 744, 727, 743, 751, 641, 711, 749, 718, 737, 737, 706, 687, 692, 703, 772, 763, 718, 778, 685, 708, 640, 667, 695, 661, 714, 768, 719, 713, 720, 723, 753, 712, 745, 662, 672, 652, 741, 688, 743, 703, 680, 668, 735, 735, 765, 690, 779, 662, 756, 712, 623, 741, 697, 692, 677, 894, 849, 737, 718, 801, 772, 849, 837, 774, 817, 812, 855, 801, 825, 791, 864, 863, 840, 739, 819, 737, 825, 752, 837, 823, 821, 872, 906, 896, 782, 833, 845, 764, 747, 860, 912, 759, 771, 824, 885, 846, 887, 851, 786, 876, 842, 839, 826, 843, 788, 942, 771, 786, 874, 845, 730, 841, 828, 849, 809, 786, 820, 776, 802, 888, 773, 799, 748, 747, 855, 857, 860, 839, 752, 814, 793, 788, 604, 586, 575, 693, 594, 619, 577, 573, 616, 613, 606, 580, 584, 678, 594, 677, 635, 609, 607, 584, 648, 559, 619, 621, 649, 655, 562, 578, 601, 640, 666, 633, 642, 647, 663, 616, 651, 608, 652, 674, 630, 641, 645, 638, 606, 612, 619, 606, 573, 658, 634, 646, 670, 644, 671, 626, 642, 659, 582, 648, 617, 628, 610, 703, 777, 720, 724, 701, 734, 645, 696, 776, 706, 641, 719, 746, 711, 755, 712, 769, 666, 800, 669, 726, 705, 676, 683, 760, 637, 710, 735, 725]\n",
      "    TRIP_ID  TRAVEL_TIME\n",
      "0        T1          748\n",
      "1        T2          769\n",
      "2        T3          800\n",
      "3        T4          784\n",
      "4        T5          838\n",
      "..      ...          ...\n",
      "315    T323          760\n",
      "316    T324          637\n",
      "317    T325          710\n",
      "318    T326          735\n",
      "319    T327          725\n",
      "\n",
      "[320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = MyDataset(\"archive/test_public.csv\")\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(round(output.item()))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "ids = testset.df[\"TRIP_ID\"]        \n",
    "print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample submission file that is given on kaggle\n",
    "# df_sample = pd.read_csv(\"archive/sampleSubmission.csv\")\n",
    "\n",
    "# df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# # mean(716.43) -> 792.73593\n",
    "# # median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Feature Analysis\n",
    "\n",
    "For our feature analysis, we are looking at which of our engineered features may be useful in making a taxicab time regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m outlier_threshold \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# VERRRRRY long taxi trips in the dataset)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_trimmed \u001b[39m=\u001b[39m df_tr[df_tr[\u001b[39m\"\u001b[39m\u001b[39mLEN\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m mean \u001b[39m+\u001b[39m outlier_threshold \u001b[39m*\u001b[39m std]\n\u001b[0;32m     11\u001b[0m \u001b[39m# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# how many 30s trips, etc. )\u001b[39;00m\n\u001b[0;32m     14\u001b[0m buckets \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(mean \u001b[39m+\u001b[39m outlier_threshold \u001b[39m*\u001b[39m std) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m15\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tr' is not defined"
     ]
    }
   ],
   "source": [
    "# First n samples to analyze. Set to -1 to use all data\n",
    "end = -1\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\n",
    "# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \n",
    "# how many 30s trips, etc. )\n",
    "buckets = (int(mean + outlier_threshold * std) // 15)\n",
    "\n",
    "print(f\"Using: {len(df_trimmed)}/{len(df_tr)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,14))\n",
    "\n",
    "# Now, we visualize some features that we think might be useful\n",
    "for idx, v in enumerate([\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"ORIGIN_STAND\"]):\n",
    "  # idx // 3 = row, idx % 3 = column\n",
    "  ax = axs[idx // 3, idx % 3]\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "  df_subset = df_trimmed.dropna(subset=v)\n",
    "  \n",
    "  # Create a histogram. Look up the documentation for more details\n",
    "  ax.hist2d(df_subset[v][:end], df_subset[\"LEN\"][:end], cmap=\"CMRmap\", bins=(120,buckets))\n",
    "  \n",
    "  # Some stylistic things to make the graphs look nice\n",
    "  ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "  ax.set_facecolor(\"black\")\n",
    "  ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "  ax.set_title(f\"Feature: {v}\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for v in [0, 5, 11, 17, 23]:\n",
    "  # Filter data where the HR matches v\n",
    "  hourly_data = df_trimmed[df_trimmed[\"HR\"] == v][\"LEN\"]\n",
    "  histogram, bin_boundary = np.histogram(hourly_data, bins=buckets)\n",
    "  histogram = histogram / len(hourly_data)\n",
    "  # The center is the left_bound and right_bound of a bucket\n",
    "  bin_centers = [(bin_boundary[i] + bin_boundary[i + 1]) / 2 for i in range(buckets)]\n",
    "  plt.plot(bin_centers, histogram, label=f\"HR={v}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
