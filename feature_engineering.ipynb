{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "#df_tr = pd.read_csv(\"archive/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr.head()\n",
    "# avg = df_tr['LEN'].mean()\n",
    "# print(avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "def parse_midnight_minutes(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.hour * 60 + dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    x=x.values[0]\n",
    "    if x==\"A\":\n",
    "        return 1, 0, 0\n",
    "    elif x==\"B\":\n",
    "        return 0, 1, 0\n",
    "    elif x==\"C\":\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "# df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "# df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"HR\"].value_counts()\n",
    "# print(vals)\n",
    "# def reduce(x):\n",
    "#     return x - 2013\n",
    "# hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "# print(hr_oh)\n",
    "# print(hr_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "\n",
    "# # df_tr[\"TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(reduce)\n",
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "# oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"TAXI_ID\"].values))\n",
    "# print(oh)\n",
    "# print(oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr[[\"CALL_A\", \"LEN\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Trimming\n",
    "# mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "# median = df_tr[\"LEN\"].median()\n",
    "# outlier_threshold = 3\n",
    "# df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "# print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "# print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "# df_tr = df_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 1710670\n",
      "After Trimming: 1656255\n",
      "ORIGIN_CALL\n",
      "2002.0     56688\n",
      "63882.0     6320\n",
      "2001.0      2397\n",
      "13168.0     1290\n",
      "6728.0      1089\n",
      "           ...  \n",
      "18954.0       50\n",
      "15826.0       50\n",
      "8110.0        50\n",
      "5093.0        50\n",
      "2983.0        50\n",
      "Name: count, Length: 734, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"archive/train.csv\")\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "outlier_threshold = 3\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "df_trimmed = df_trimmed[df_trimmed['LEN'] != 0]\n",
    "print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "df_tr = df_trimmed\n",
    "\n",
    "\n",
    "\n",
    "trlen = len(df_tr)\n",
    "\n",
    "df_ts = pd.read_csv(\"archive/test_public.csv\")\n",
    "df_ts[\"POLYLINE\"]=\"trololololo\"\n",
    "\n",
    "\n",
    "\n",
    "df_both = pd.concat([df_tr, df_ts])\n",
    "df_both[\"LEN\"] = df_both[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "ocvc = df_both[\"ORIGIN_CALL\"].value_counts()\n",
    "def filterOC(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    if(ocvc[x] < 50):\n",
    "        return None\n",
    "    return x\n",
    "df_both[\"ORIGIN_CALL\"] = df_both[\"ORIGIN_CALL\"].apply(filterOC)\n",
    "print(df_both[\"ORIGIN_CALL\"].value_counts())\n",
    "# # df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "df_both[\"MIDMINS\"] = df_both[[\"TIMESTAMP\"]].apply(parse_midnight_minutes, axis=1, result_type=\"expand\")\n",
    "#df_tr[\"MIDMINS\"]=(df_tr[\"MIDMINS\"]-df_tr[\"MIDMINS\"].min())/(df_tr[\"MIDMINS\"].max()-df_tr[\"MIDMINS\"].min())\n",
    "df_both = pd.get_dummies(data=df_both, columns=['CALL_TYPE', \"ORIGIN_STAND\", \"ORIGIN_CALL\"])\n",
    "\n",
    "\n",
    "df_tr = df_both.iloc[:trlen]\n",
    "df_ts = df_both.iloc[trlen:]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, df):\n",
    "\n",
    "    boolcols = list(df.columns)\n",
    "    badcols = [\"TRIP_ID\", 'MIDMINS',  'TIMESTAMP', 'MISSING_DATA', 'POLYLINE', 'LEN', \"TAXI_ID\", \"DAY_TYPE\"] \n",
    "    for b in badcols:\n",
    "      boolcols.remove(b)\n",
    "    # print(boolcols)\n",
    "    \n",
    "    \n",
    "         \n",
    "    boolz=df[boolcols].values\n",
    "    intz = df[\"MIDMINS\"].values\n",
    "    # print(boolz)\n",
    "    y=df[\"LEN\"].values\n",
    "\n",
    "    booltens = torch.tensor(boolz,dtype=torch.float32)\n",
    "    inttens = torch.tensor(intz,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # print(booltens.shape)\n",
    "    # print(inttens.shape)\n",
    " \n",
    "    self.x_train=torch.cat([booltens, inttens], dim=1)\n",
    "    # print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490630 165625\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "trds=MyDataset(df_tr)\n",
    "trds, valds = random_split(trds, [0.9, 0.1])\n",
    "print(len(trds), len(valds))\n",
    "train_loader=DataLoader(trds,batch_size=256, shuffle=True)\n",
    "val_loader=DataLoader(valds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dro = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(801, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # x = self.dro(x)\n",
    "        x = self.fc4(x)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "criterion = RMSELoss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[313], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):  \u001b[39m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, \u001b[39m0\u001b[39m):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m         \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m      9\u001b[0m         \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:139\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[0;32m    138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            #print(outputs[0], labels[0])\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 635.196\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        # if i%1000==0:\n",
    "        #     print(round(loss.item()), outputs, labels)\n",
    "\n",
    "print(f'Total Loss: {total_loss / len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.43415832519531, 48.91222381591797, 55.246734619140625, 50.59748077392578, 64.25698852539062, 56.9205207824707, 55.619293212890625, 44.270263671875, 56.29725646972656, 52.418270111083984, 57.08964920043945, 58.451297760009766, 61.36245346069336, 44.7822265625, 57.559452056884766, 52.52726745605469, 60.672210693359375, 48.79900360107422, 56.55269241333008, 54.90105056762695, 57.927001953125, 50.48965835571289, 55.6378059387207, 49.111324310302734, 65.85307312011719, 40.00066375732422, 53.19807434082031, 43.28772735595703, 61.94066619873047, 61.494056701660156, 47.66448211669922, 52.27593994140625, 61.961448669433594, 55.30530548095703, 57.578468322753906, 52.722625732421875, 60.225440979003906, 46.24495315551758, 57.22691345214844, 47.36805725097656, 46.23332977294922, 49.342708587646484, 45.1513557434082, 58.308311462402344, 58.46783447265625, 56.09163284301758, 42.48173141479492, 53.95802688598633, 43.48570251464844, 49.6137580871582, 44.46013641357422, 47.69865036010742, 63.159996032714844, 45.51823425292969, 60.06694030761719, 55.24493408203125, 40.86273193359375, 50.00526428222656, 46.49486541748047, 60.29799270629883, 49.80222702026367, 55.15604782104492, 57.02198791503906, 53.088600158691406, 48.78795623779297, 58.48512649536133, 59.017295837402344, 47.98918914794922, 49.988494873046875, 48.543739318847656, 43.00575256347656, 49.43804168701172, 53.95763397216797, 43.62759017944336, 7.138123035430908, 7.857490539550781, 7.980016231536865, 8.566438674926758, 6.870397090911865, 5.553647518157959, 6.329471111297607, 8.095044136047363, 6.100003719329834, 6.147560119628906, 6.115061283111572, 6.82403564453125, 6.481764316558838, 6.781692981719971, 7.690202236175537, 7.588141918182373, 6.411098957061768, 6.101541996002197, 7.394975185394287, 7.882521152496338, 2.4595725536346436, 4.253060340881348, 5.628911972045898, 5.756243705749512, 5.82886266708374, 6.807745456695557, 5.44155740737915, 6.587198257446289, 7.477681636810303, 6.360744953155518, 8.433594703674316, 6.61637020111084, 5.945168972015381, 5.180623531341553, 5.956531047821045, 6.217745304107666, 6.792454242706299, 6.41870641708374, 6.835276126861572, 5.95456600189209, 7.587531089782715, 7.284327030181885, 5.743216514587402, 4.224789619445801, 6.188930034637451, 7.080009937286377, 7.146316051483154, 7.661068439483643, 6.987215995788574, 8.698033332824707, 5.6899542808532715, 123.2348403930664, 6.236734867095947, 6.559962749481201, 5.188006401062012, 7.686543941497803, 7.781395435333252, 6.981281280517578, 0.4099191129207611, 7.490413188934326, 8.303816795349121, 7.34967565536499, 6.404122352600098, 6.321204662322998, 5.822258949279785, 7.4018707275390625, 6.685196399688721, 7.3652849197387695, 6.3500447273254395, 3.2522897720336914, 7.638265132904053, 7.940636157989502, 6.297039985656738, 5.201096057891846, 6.587025165557861, 6.401617050170898, 8.395197868347168, 51.8143196105957, 47.771446228027344, 49.25374221801758, 53.153480529785156, 46.402931213378906, 55.17219161987305, 43.017730712890625, 43.313175201416016, 59.0145378112793, 60.6817626953125, 58.92421340942383, 39.85829162597656, 44.65584182739258, 49.60676574707031, 42.92791748046875, 43.48602294921875, 48.968849182128906, 55.077903747558594, 49.08121871948242, 42.80049514770508, 31.931026458740234, 52.78804016113281, 45.589725494384766, 46.169921875, 47.14759826660156, 50.864845275878906, 51.614830017089844, 59.92230987548828, 54.194610595703125, 47.611778259277344, 61.94214630126953, 58.119110107421875, 59.87445831298828, 44.59187698364258, 44.519351959228516, 40.90486526489258, 51.07398223876953, 59.46234893798828, 48.2725830078125, 52.0967903137207, 52.95582580566406, 65.64071655273438, 62.230926513671875, 55.46452331542969, 53.04848861694336, 44.53791809082031, 53.493797302246094, 51.345245361328125, 50.92110824584961, 42.28712463378906, 45.09733581542969, 52.07947540283203, 45.76734924316406, 54.100616455078125, 61.00125503540039, 39.51325225830078, 59.65940856933594, 57.190391540527344, 44.77544021606445, 51.417701721191406, 47.39741516113281, 46.23886489868164, 53.04013442993164, 50.292579650878906, 55.54048156738281, 44.17607116699219, 51.0935173034668, 40.322853088378906, 45.489967346191406, 53.692298889160156, 55.41236877441406, 38.661888122558594, 53.63654327392578, 55.56944274902344, 42.866943359375, 36.82536697387695, 51.786373138427734, 99.47904968261719, 95.92552185058594, 89.28486633300781, 113.34263610839844, 96.79737091064453, 124.80462646484375, 90.20323181152344, 94.45535278320312, 73.70751190185547, 108.09483337402344, 95.78828430175781, 122.525390625, 96.29557800292969, 98.55548095703125, 125.3926010131836, 94.55850219726562, 109.87615966796875, 95.26179504394531, 86.3215103149414, 113.59791564941406, 98.48977661132812, 102.02954864501953, 91.26951599121094, 97.8218994140625, 95.79412841796875, 88.97969055175781, 102.37010192871094, 124.37298583984375, 113.66781616210938, 94.33113098144531, 114.11924743652344, 91.97407531738281, 102.9630355834961, 103.83749389648438, 110.17523193359375, 93.32858276367188, 97.20636749267578, 124.03179931640625, 112.34329223632812, 108.25676727294922, 101.54686737060547, 84.69203186035156, 123.868896484375, 110.95458984375, 101.70755767822266, 106.86383056640625, 101.09253692626953, 80.69168090820312, 89.25929260253906, 93.14491271972656, 85.42640686035156, 120.6011734008789, 95.36676788330078, 112.1679916381836, 95.00210571289062, 106.08473205566406, 111.65269470214844, 92.33224487304688, 111.08348846435547, 94.90695190429688, 116.76860809326172, 116.26715087890625, 28.783605575561523, 27.972002029418945, 33.0424919128418, 24.89788818359375, 27.51812744140625, 33.010345458984375, 37.469398498535156, 25.512359619140625, 36.588783264160156, 29.142621994018555, 17.834749221801758, 27.332725524902344, 28.46542739868164, 17.84477996826172, 29.957496643066406, 28.810453414916992, 28.48790740966797, 34.0672607421875, 32.534149169921875, 27.94737434387207, 32.17093276977539, 35.665828704833984, 20.235485076904297, 23.882902145385742, 32.80732345581055, 32.89414596557617, 32.61840057373047, 32.92955780029297, 33.06813049316406, 28.62269401550293]\n",
      "    TRIP_ID  TRAVEL_TIME\n",
      "0        T1    47.434158\n",
      "1        T2    48.912224\n",
      "2        T3    55.246735\n",
      "3        T4    50.597481\n",
      "4        T5    64.256989\n",
      "..      ...          ...\n",
      "315    T323    32.894146\n",
      "316    T324    32.618401\n",
      "317    T325    32.929558\n",
      "318    T326    33.068130\n",
      "319    T327    28.622694\n",
      "\n",
      "[320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = MyDataset(df_ts)\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(output.item())\n",
    "ids = testset.df[\"TRIP_ID\"]        \n",
    "print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample submission file that is given on kaggle\n",
    "# df_sample = pd.read_csv(\"archive/sampleSubmission.csv\")\n",
    "\n",
    "# df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# # mean(716.43) -> 792.73593\n",
    "# # median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosted Decision Trees**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCSV(csvname):\n",
    "  xg_train = pd.read_csv(csvname)\n",
    "  df_tr = xg_train\n",
    "  if \"POLYLINE\" not in df_tr: #test dataset\n",
    "    df_tr[\"POLYLINE\"]=\"trololololo\"\n",
    "    df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "  else: \n",
    "    df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "    mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "    median = df_tr[\"LEN\"].median()\n",
    "    outlier_threshold = 3\n",
    "    print(mean, std)\n",
    "    df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "    df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "    print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "    print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "    df_tr = df_trimmed\n",
    "    # first trim the dataset\n",
    "\n",
    "  df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "  LetterToIndex = {'A': 0, 'B': 1, 'C': 2}\n",
    "  df_trimmed_copy = df_tr\n",
    "  print('there was ' + str(len(df_trimmed_copy['LEN'] == 0)) + \" zeroes\")\n",
    "  df_trimmed_copy = df_trimmed_copy[df_trimmed_copy['LEN'] != 0]\n",
    "\n",
    "  y_train = df_trimmed_copy[\"LEN\"]\n",
    "  df_trimmed_copy[\"CALL_TYPE\"] = df_trimmed_copy[\"CALL_TYPE\"].map(LetterToIndex)\n",
    "  # df_trimmed_copy[\"DAY_TYPE\"] = df_trimmed_copy[\"DAY_TYPE\"].map(LetterToIndex)\n",
    "  #took out \"TRIP_ID\"\n",
    "  x_train = df_trimmed_copy[[\"TRIP_ID\", \"CALL_TYPE\", \"ORIGIN_CALL\", \"ORIGIN_STAND\", \"TAXI_ID\", \"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]]\n",
    "  return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = processCSV('archive/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(columns=['TRIP_ID'])#this doesnt actually drop, just without tripid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# shuffle_indices = np.random.permutation(len(x_train))\n",
    "# X_shuffled = x_train[shuffle_indices]\n",
    "# y_shuffled = y_train[shuffle_indices]\n",
    "\n",
    "combined_df = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "# Get the number of rows in the DataFrame\n",
    "num_rows = combined_df.shape[0]\n",
    "\n",
    "# Generate a random permutation of indices\n",
    "perm = np.random.permutation(num_rows)\n",
    "\n",
    "# Shuffle the combined DataFrame using the permutation\n",
    "shuffled_df = combined_df.iloc[perm]\n",
    "\n",
    "# Split the shuffled DataFrame back into separate DataFrames\n",
    "shuffled_train_df = shuffled_df.iloc[:, :-1]\n",
    "shuffled_label_df = shuffled_df.iloc[:, -1]\n",
    "\n",
    "# print(shuffled_X)\n",
    "# print(shuffled_Y)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(shuffled_train_df, shuffled_label_df, test_size=0.3, random_state=None)\n",
    "\n",
    "# print(X_train, X_val, Y_train, Y_val)\n",
    "# parameters = {\n",
    "#     'objective': 'reg:squarederror',  # Regression objective\n",
    "#     'eta': 0.1,                       # Learning rate\n",
    "#     'max_depth': 3,                   # Maximum depth of each tree\n",
    "#     'subsample': 0.8,                 # Subsample ratio of the training instances\n",
    "#     'colsample_bytree': 0.8,          # Subsample ratio of columns when constructing each tree\n",
    "#     'eval_metric': 'rmse',             # Evaluation metric to use (RMSE)\n",
    "#     'shuffle': True\n",
    "# }\n",
    "\n",
    "# # Create the XGBoost regressor\n",
    "# xgmodel = xgb.XGBRegressor(parameters)\n",
    "\n",
    "# # Train the model\n",
    "# xgmodel.fit(x_train, y_train)\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and target variables\n",
    "\n",
    "# Convert DataFrame to DMatrix\n",
    "data_matrix = xgb.DMatrix(data = X_train.drop(columns=['TRIP_ID']), label=Y_train)\n",
    "validation_matrix = xgb.DMatrix(data = X_val.drop(columns=['TRIP_ID']), label=Y_val)\n",
    "\n",
    "# Define XGBoost parameters\n",
    "# parameters = {'objective': 'reg:squarederror', \n",
    "#               'max_depth': 30, \n",
    "#               # 'booster': 'gbtree', \n",
    "#                'colsample_bytree': .6,\n",
    "#                'eta': .4,\n",
    "#               # 'max_depth': 5,\n",
    "#               'reg_alpha': 608.85614181114505,\n",
    "#              'reg_lambda': 809.260067417192285,\n",
    "#               'subsample': 0.9\n",
    "#               }\n",
    "\n",
    "# parameters = {'objective': 'reg:squarederror', 'max_depth': 30, 'booster': 'gbtree', 'subsample': 0.9}\n",
    "# parameters = {'objective': 'reg:squarederror', \n",
    "#               'max_depth': 4, \n",
    "#               # 'booster': 'gbtree', \n",
    "#               # 'colsample_bytree': .13641457979856397,\n",
    "#               # 'max_depth': 5,\n",
    "#               'reg_alpha': 68.85614181114505,\n",
    "#             #   'reg_lambda': 9.260067417192285,\n",
    "#               # 'subsample': 0.8980218163372579\n",
    "# }\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'colsample_bytree': 0.5586586280723452,\n",
    "    'learning_rate': 0.06889888561468978,\n",
    "    'max_depth': 5,\n",
    "    # 'n_estimators': 1164,\n",
    "    'reg_alpha': 25.287638310615133,\n",
    "    'reg_lambda': 67.51702941180568,\n",
    "    'subsample': 0.77179392553171\n",
    "}\n",
    "\n",
    "xg_model = xgb.train(parameters, data_matrix, num_boost_round=10000, early_stopping_rounds=20, evals=[(validation_matrix, 'validation')], verbose_eval=100)\n",
    "\n",
    "\n",
    "\n",
    "# ('colsample_bytree', 0.13641457979856397),\n",
    "#              ('learning_rate', 0.09604083138419779),\n",
    "#              ('max_depth', 5),\n",
    "#              ('n_estimators', 2206),\n",
    "#              ('reg_alpha', 68.85614181114505),\n",
    "#              ('reg_lambda', 9.260067417192285),\n",
    "#              ('subsample', 0.8980218163372579)\n",
    "\n",
    "# reg = XGBRegressor(random_state=0, booster='gbtree', objective='reg:squarederror', tree_method='gpu_hist', **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = processCSV('archive/test_public.csv')\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xg_model.predict(xgb.DMatrix(test_x.drop(columns=['TRIP_ID'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_x[\"TRIP_ID\"]\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : y_pred}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"tree_pred9_bayesian_opt_max_depth_5_6232_boosts.csv\", index=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Feature Analysis\n",
    "\n",
    "For our feature analysis, we are looking at which of our engineered features may be useful in making a taxicab time regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n samples to analyze. Set to -1 to use all data\n",
    "end = -1\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\n",
    "# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \n",
    "# how many 30s trips, etc. )\n",
    "buckets = (int(mean + outlier_threshold * std) // 15)\n",
    "\n",
    "print(f\"Using: {len(df_trimmed)}/{len(df_tr)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,14))\n",
    "\n",
    "# Now, we visualize some features that we think might be useful\n",
    "for idx, v in enumerate([\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"ORIGIN_STAND\"]):\n",
    "  # idx // 3 = row, idx % 3 = column\n",
    "  ax = axs[idx // 3, idx % 3]\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "  df_subset = df_trimmed.dropna(subset=v)\n",
    "  \n",
    "  # Create a histogram. Look up the documentation for more details\n",
    "  ax.hist2d(df_subset[v][:end], df_subset[\"LEN\"][:end], cmap=\"CMRmap\", bins=(120,buckets))\n",
    "  \n",
    "  # Some stylistic things to make the graphs look nice\n",
    "  ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "  ax.set_facecolor(\"black\")\n",
    "  ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "  ax.set_title(f\"Feature: {v}\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for v in [0, 5, 11, 17, 23]:\n",
    "  # Filter data where the HR matches v\n",
    "  hourly_data = df_trimmed[df_trimmed[\"HR\"] == v][\"LEN\"]\n",
    "  histogram, bin_boundary = np.histogram(hourly_data, bins=buckets)\n",
    "  histogram = histogram / len(hourly_data)\n",
    "  # The center is the left_bound and right_bound of a bucket\n",
    "  bin_centers = [(bin_boundary[i] + bin_boundary[i + 1]) / 2 for i in range(buckets)]\n",
    "  plt.plot(bin_centers, histogram, label=f\"HR={v}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
