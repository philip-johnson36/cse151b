{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# These are all of the files you are given\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_tr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39marchive/train.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# These are all of the files you are given\n",
    "df_tr = pd.read_csv(\"archive/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORIGIN_CALL\n",
       "2002.0     57571\n",
       "63882.0     6406\n",
       "2001.0      2499\n",
       "13168.0     1314\n",
       "6728.0      1115\n",
       "           ...  \n",
       "23600.0        1\n",
       "37142.0        1\n",
       "7028.0         1\n",
       "49288.0        1\n",
       "34164.0        1\n",
       "Name: count, Length: 57105, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr[\"ORIGIN_CALL\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr.head()\n",
    "# avg = df_tr['LEN'].mean()\n",
    "# print(avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "def parse_midnight_minutes(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.hour * 60 + dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    x=x.values[0]\n",
    "    if x==\"A\":\n",
    "        return 1, 0, 0\n",
    "    elif x==\"B\":\n",
    "        return 0, 1, 0\n",
    "    elif x==\"C\":\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "# df_tr[[\"CALL_A\", \"CALL_B\", \"CALL_C\"]] = df_tr[[\"CALL_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")   \n",
    "# df_tr[[\"DAY_A\", \"DAY_B\", \"DAY_C\"]] = df_tr[[\"DAY_TYPE\"]].apply(onehot, axis=1, result_type=\"expand\")     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"HR\"].value_counts()\n",
    "# print(vals)\n",
    "# def reduce(x):\n",
    "#     return x - 2013\n",
    "# hr_oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"HR\"].values))\n",
    "# print(hr_oh)\n",
    "# print(hr_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "\n",
    "# # df_tr[\"TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(reduce)\n",
    "# vals = df_tr[\"TAXI_ID\"].values\n",
    "# print(vals)\n",
    "# print(min(vals))\n",
    "# print(max(vals))\n",
    "# oh = torch.nn.functional.one_hot(torch.tensor(df_tr[\"TAXI_ID\"].values))\n",
    "# print(oh)\n",
    "# print(oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr[[\"CALL_A\", \"LEN\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Trimming\n",
    "# mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "# median = df_tr[\"LEN\"].median()\n",
    "# outlier_threshold = 3\n",
    "# df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "# print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "# print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "# df_tr = df_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, csvpath):\n",
    "    df_tr = pd.read_csv(csvpath)\n",
    "\n",
    "    if \"POLYLINE\" not in df_tr: #test dataset\n",
    "      df_tr[\"POLYLINE\"]=\"trololololo\"\n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "    else: \n",
    "      df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "      # first trim the dataset\n",
    "\n",
    "      mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "      median = df_tr[\"LEN\"].median()\n",
    "      outlier_threshold = 3\n",
    "      df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "      df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "      print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "      print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "\n",
    "      df_tr = df_trimmed\n",
    "\n",
    "\n",
    "    # df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "    df_tr[\"MIDMINS\"] = df_tr[[\"TIMESTAMP\"]].apply(parse_midnight_minutes, axis=1, result_type=\"expand\")\n",
    "    #df_tr[\"MIDMINS\"]=(df_tr[\"MIDMINS\"]-df_tr[\"MIDMINS\"].min())/(df_tr[\"MIDMINS\"].max()-df_tr[\"MIDMINS\"].min())\n",
    "    df_tr = pd.get_dummies(data=df_tr, columns=['CALL_TYPE', \"DAY_TYPE\", \"ORIGIN_STAND\", \"TAXI_ID\"])\n",
    "\n",
    "\n",
    "    cols = list(df_tr.columns)\n",
    "    badcols = [\"TRIP_ID\", 'ORIGIN_CALL',  'TIMESTAMP', 'MISSING_DATA', 'POLYLINE', 'LEN',] \n",
    "    for b in badcols:\n",
    "      cols.remove(b)\n",
    "    print(cols)\n",
    "    \n",
    "    \n",
    "         \n",
    "    x=df_tr[cols].astype(int).values\n",
    "    print(x)\n",
    "    y=df_tr[\"LEN\"].values\n",
    " \n",
    "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "    print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df_tr\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 1710670\n",
      "After Trimming: 1692763\n",
      "['MIDMINS', 'CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'DAY_TYPE_A', 'ORIGIN_STAND_1.0', 'ORIGIN_STAND_2.0', 'ORIGIN_STAND_3.0', 'ORIGIN_STAND_4.0', 'ORIGIN_STAND_5.0', 'ORIGIN_STAND_6.0', 'ORIGIN_STAND_7.0', 'ORIGIN_STAND_8.0', 'ORIGIN_STAND_9.0', 'ORIGIN_STAND_10.0', 'ORIGIN_STAND_11.0', 'ORIGIN_STAND_12.0', 'ORIGIN_STAND_13.0', 'ORIGIN_STAND_14.0', 'ORIGIN_STAND_15.0', 'ORIGIN_STAND_16.0', 'ORIGIN_STAND_17.0', 'ORIGIN_STAND_18.0', 'ORIGIN_STAND_19.0', 'ORIGIN_STAND_20.0', 'ORIGIN_STAND_21.0', 'ORIGIN_STAND_22.0', 'ORIGIN_STAND_23.0', 'ORIGIN_STAND_24.0', 'ORIGIN_STAND_25.0', 'ORIGIN_STAND_26.0', 'ORIGIN_STAND_27.0', 'ORIGIN_STAND_28.0', 'ORIGIN_STAND_29.0', 'ORIGIN_STAND_30.0', 'ORIGIN_STAND_31.0', 'ORIGIN_STAND_32.0', 'ORIGIN_STAND_33.0', 'ORIGIN_STAND_34.0', 'ORIGIN_STAND_35.0', 'ORIGIN_STAND_36.0', 'ORIGIN_STAND_37.0', 'ORIGIN_STAND_38.0', 'ORIGIN_STAND_39.0', 'ORIGIN_STAND_40.0', 'ORIGIN_STAND_41.0', 'ORIGIN_STAND_42.0', 'ORIGIN_STAND_43.0', 'ORIGIN_STAND_44.0', 'ORIGIN_STAND_45.0', 'ORIGIN_STAND_46.0', 'ORIGIN_STAND_47.0', 'ORIGIN_STAND_48.0', 'ORIGIN_STAND_49.0', 'ORIGIN_STAND_50.0', 'ORIGIN_STAND_51.0', 'ORIGIN_STAND_52.0', 'ORIGIN_STAND_53.0', 'ORIGIN_STAND_54.0', 'ORIGIN_STAND_55.0', 'ORIGIN_STAND_56.0', 'ORIGIN_STAND_57.0', 'ORIGIN_STAND_58.0', 'ORIGIN_STAND_59.0', 'ORIGIN_STAND_60.0', 'ORIGIN_STAND_61.0', 'ORIGIN_STAND_62.0', 'ORIGIN_STAND_63.0', 'TAXI_ID_20000001', 'TAXI_ID_20000002', 'TAXI_ID_20000003', 'TAXI_ID_20000004', 'TAXI_ID_20000005', 'TAXI_ID_20000006', 'TAXI_ID_20000007', 'TAXI_ID_20000008', 'TAXI_ID_20000009', 'TAXI_ID_20000010', 'TAXI_ID_20000011', 'TAXI_ID_20000012', 'TAXI_ID_20000013', 'TAXI_ID_20000014', 'TAXI_ID_20000015', 'TAXI_ID_20000017', 'TAXI_ID_20000018', 'TAXI_ID_20000020', 'TAXI_ID_20000021', 'TAXI_ID_20000022', 'TAXI_ID_20000023', 'TAXI_ID_20000024', 'TAXI_ID_20000026', 'TAXI_ID_20000027', 'TAXI_ID_20000030', 'TAXI_ID_20000031', 'TAXI_ID_20000032', 'TAXI_ID_20000036', 'TAXI_ID_20000037', 'TAXI_ID_20000038', 'TAXI_ID_20000039', 'TAXI_ID_20000040', 'TAXI_ID_20000041', 'TAXI_ID_20000042', 'TAXI_ID_20000044', 'TAXI_ID_20000045', 'TAXI_ID_20000046', 'TAXI_ID_20000047', 'TAXI_ID_20000048', 'TAXI_ID_20000049', 'TAXI_ID_20000050', 'TAXI_ID_20000051', 'TAXI_ID_20000053', 'TAXI_ID_20000054', 'TAXI_ID_20000055', 'TAXI_ID_20000057', 'TAXI_ID_20000058', 'TAXI_ID_20000060', 'TAXI_ID_20000065', 'TAXI_ID_20000066', 'TAXI_ID_20000067', 'TAXI_ID_20000068', 'TAXI_ID_20000071', 'TAXI_ID_20000072', 'TAXI_ID_20000073', 'TAXI_ID_20000074', 'TAXI_ID_20000075', 'TAXI_ID_20000076', 'TAXI_ID_20000077', 'TAXI_ID_20000079', 'TAXI_ID_20000080', 'TAXI_ID_20000081', 'TAXI_ID_20000083', 'TAXI_ID_20000084', 'TAXI_ID_20000085', 'TAXI_ID_20000086', 'TAXI_ID_20000089', 'TAXI_ID_20000092', 'TAXI_ID_20000093', 'TAXI_ID_20000094', 'TAXI_ID_20000095', 'TAXI_ID_20000098', 'TAXI_ID_20000099', 'TAXI_ID_20000100', 'TAXI_ID_20000101', 'TAXI_ID_20000104', 'TAXI_ID_20000105', 'TAXI_ID_20000107', 'TAXI_ID_20000108', 'TAXI_ID_20000109', 'TAXI_ID_20000111', 'TAXI_ID_20000112', 'TAXI_ID_20000113', 'TAXI_ID_20000114', 'TAXI_ID_20000116', 'TAXI_ID_20000118', 'TAXI_ID_20000119', 'TAXI_ID_20000120', 'TAXI_ID_20000121', 'TAXI_ID_20000123', 'TAXI_ID_20000126', 'TAXI_ID_20000128', 'TAXI_ID_20000129', 'TAXI_ID_20000131', 'TAXI_ID_20000136', 'TAXI_ID_20000137', 'TAXI_ID_20000138', 'TAXI_ID_20000140', 'TAXI_ID_20000142', 'TAXI_ID_20000144', 'TAXI_ID_20000146', 'TAXI_ID_20000148', 'TAXI_ID_20000151', 'TAXI_ID_20000152', 'TAXI_ID_20000153', 'TAXI_ID_20000154', 'TAXI_ID_20000156', 'TAXI_ID_20000157', 'TAXI_ID_20000158', 'TAXI_ID_20000159', 'TAXI_ID_20000160', 'TAXI_ID_20000161', 'TAXI_ID_20000163', 'TAXI_ID_20000166', 'TAXI_ID_20000167', 'TAXI_ID_20000170', 'TAXI_ID_20000171', 'TAXI_ID_20000172', 'TAXI_ID_20000173', 'TAXI_ID_20000174', 'TAXI_ID_20000177', 'TAXI_ID_20000178', 'TAXI_ID_20000179', 'TAXI_ID_20000180', 'TAXI_ID_20000184', 'TAXI_ID_20000185', 'TAXI_ID_20000186', 'TAXI_ID_20000187', 'TAXI_ID_20000188', 'TAXI_ID_20000189', 'TAXI_ID_20000190', 'TAXI_ID_20000192', 'TAXI_ID_20000195', 'TAXI_ID_20000196', 'TAXI_ID_20000197', 'TAXI_ID_20000198', 'TAXI_ID_20000199', 'TAXI_ID_20000201', 'TAXI_ID_20000205', 'TAXI_ID_20000206', 'TAXI_ID_20000207', 'TAXI_ID_20000213', 'TAXI_ID_20000215', 'TAXI_ID_20000216', 'TAXI_ID_20000217', 'TAXI_ID_20000222', 'TAXI_ID_20000224', 'TAXI_ID_20000225', 'TAXI_ID_20000226', 'TAXI_ID_20000229', 'TAXI_ID_20000230', 'TAXI_ID_20000231', 'TAXI_ID_20000233', 'TAXI_ID_20000235', 'TAXI_ID_20000239', 'TAXI_ID_20000242', 'TAXI_ID_20000243', 'TAXI_ID_20000245', 'TAXI_ID_20000246', 'TAXI_ID_20000247', 'TAXI_ID_20000248', 'TAXI_ID_20000249', 'TAXI_ID_20000250', 'TAXI_ID_20000251', 'TAXI_ID_20000252', 'TAXI_ID_20000254', 'TAXI_ID_20000255', 'TAXI_ID_20000256', 'TAXI_ID_20000257', 'TAXI_ID_20000258', 'TAXI_ID_20000260', 'TAXI_ID_20000261', 'TAXI_ID_20000263', 'TAXI_ID_20000264', 'TAXI_ID_20000267', 'TAXI_ID_20000268', 'TAXI_ID_20000269', 'TAXI_ID_20000271', 'TAXI_ID_20000272', 'TAXI_ID_20000273', 'TAXI_ID_20000276', 'TAXI_ID_20000279', 'TAXI_ID_20000280', 'TAXI_ID_20000281', 'TAXI_ID_20000285', 'TAXI_ID_20000286', 'TAXI_ID_20000288', 'TAXI_ID_20000289', 'TAXI_ID_20000294', 'TAXI_ID_20000295', 'TAXI_ID_20000296', 'TAXI_ID_20000297', 'TAXI_ID_20000303', 'TAXI_ID_20000304', 'TAXI_ID_20000305', 'TAXI_ID_20000307', 'TAXI_ID_20000308', 'TAXI_ID_20000309', 'TAXI_ID_20000310', 'TAXI_ID_20000311', 'TAXI_ID_20000312', 'TAXI_ID_20000314', 'TAXI_ID_20000315', 'TAXI_ID_20000319', 'TAXI_ID_20000320', 'TAXI_ID_20000321', 'TAXI_ID_20000324', 'TAXI_ID_20000325', 'TAXI_ID_20000326', 'TAXI_ID_20000327', 'TAXI_ID_20000328', 'TAXI_ID_20000329', 'TAXI_ID_20000330', 'TAXI_ID_20000331', 'TAXI_ID_20000332', 'TAXI_ID_20000333', 'TAXI_ID_20000334', 'TAXI_ID_20000337', 'TAXI_ID_20000338', 'TAXI_ID_20000340', 'TAXI_ID_20000341', 'TAXI_ID_20000342', 'TAXI_ID_20000343', 'TAXI_ID_20000344', 'TAXI_ID_20000345', 'TAXI_ID_20000346', 'TAXI_ID_20000347', 'TAXI_ID_20000349', 'TAXI_ID_20000351', 'TAXI_ID_20000352', 'TAXI_ID_20000353', 'TAXI_ID_20000356', 'TAXI_ID_20000359', 'TAXI_ID_20000360', 'TAXI_ID_20000361', 'TAXI_ID_20000362', 'TAXI_ID_20000363', 'TAXI_ID_20000364', 'TAXI_ID_20000367', 'TAXI_ID_20000370', 'TAXI_ID_20000372', 'TAXI_ID_20000373', 'TAXI_ID_20000376', 'TAXI_ID_20000377', 'TAXI_ID_20000380', 'TAXI_ID_20000381', 'TAXI_ID_20000383', 'TAXI_ID_20000384', 'TAXI_ID_20000386', 'TAXI_ID_20000387', 'TAXI_ID_20000388', 'TAXI_ID_20000389', 'TAXI_ID_20000391', 'TAXI_ID_20000392', 'TAXI_ID_20000393', 'TAXI_ID_20000395', 'TAXI_ID_20000397', 'TAXI_ID_20000398', 'TAXI_ID_20000400', 'TAXI_ID_20000403', 'TAXI_ID_20000406', 'TAXI_ID_20000407', 'TAXI_ID_20000408', 'TAXI_ID_20000409', 'TAXI_ID_20000410', 'TAXI_ID_20000419', 'TAXI_ID_20000421', 'TAXI_ID_20000423', 'TAXI_ID_20000424', 'TAXI_ID_20000426', 'TAXI_ID_20000429', 'TAXI_ID_20000430', 'TAXI_ID_20000431', 'TAXI_ID_20000432', 'TAXI_ID_20000434', 'TAXI_ID_20000435', 'TAXI_ID_20000436', 'TAXI_ID_20000440', 'TAXI_ID_20000443', 'TAXI_ID_20000446', 'TAXI_ID_20000448', 'TAXI_ID_20000449', 'TAXI_ID_20000450', 'TAXI_ID_20000451', 'TAXI_ID_20000452', 'TAXI_ID_20000453', 'TAXI_ID_20000454', 'TAXI_ID_20000455', 'TAXI_ID_20000456', 'TAXI_ID_20000458', 'TAXI_ID_20000460', 'TAXI_ID_20000463', 'TAXI_ID_20000464', 'TAXI_ID_20000465', 'TAXI_ID_20000467', 'TAXI_ID_20000468', 'TAXI_ID_20000470', 'TAXI_ID_20000472', 'TAXI_ID_20000473', 'TAXI_ID_20000476', 'TAXI_ID_20000477', 'TAXI_ID_20000480', 'TAXI_ID_20000481', 'TAXI_ID_20000482', 'TAXI_ID_20000483', 'TAXI_ID_20000484', 'TAXI_ID_20000486', 'TAXI_ID_20000487', 'TAXI_ID_20000488', 'TAXI_ID_20000492', 'TAXI_ID_20000494', 'TAXI_ID_20000495', 'TAXI_ID_20000496', 'TAXI_ID_20000497', 'TAXI_ID_20000499', 'TAXI_ID_20000500', 'TAXI_ID_20000502', 'TAXI_ID_20000503', 'TAXI_ID_20000504', 'TAXI_ID_20000506', 'TAXI_ID_20000507', 'TAXI_ID_20000508', 'TAXI_ID_20000509', 'TAXI_ID_20000510', 'TAXI_ID_20000513', 'TAXI_ID_20000514', 'TAXI_ID_20000515', 'TAXI_ID_20000516', 'TAXI_ID_20000517', 'TAXI_ID_20000518', 'TAXI_ID_20000520', 'TAXI_ID_20000521', 'TAXI_ID_20000523', 'TAXI_ID_20000524', 'TAXI_ID_20000525', 'TAXI_ID_20000527', 'TAXI_ID_20000529', 'TAXI_ID_20000534', 'TAXI_ID_20000535', 'TAXI_ID_20000539', 'TAXI_ID_20000540', 'TAXI_ID_20000541', 'TAXI_ID_20000542', 'TAXI_ID_20000543', 'TAXI_ID_20000545', 'TAXI_ID_20000546', 'TAXI_ID_20000547', 'TAXI_ID_20000548', 'TAXI_ID_20000549', 'TAXI_ID_20000554', 'TAXI_ID_20000557', 'TAXI_ID_20000560', 'TAXI_ID_20000561', 'TAXI_ID_20000562', 'TAXI_ID_20000565', 'TAXI_ID_20000569', 'TAXI_ID_20000570', 'TAXI_ID_20000571', 'TAXI_ID_20000572', 'TAXI_ID_20000574', 'TAXI_ID_20000576', 'TAXI_ID_20000577', 'TAXI_ID_20000578', 'TAXI_ID_20000579', 'TAXI_ID_20000585', 'TAXI_ID_20000586', 'TAXI_ID_20000588', 'TAXI_ID_20000589', 'TAXI_ID_20000591', 'TAXI_ID_20000594', 'TAXI_ID_20000595', 'TAXI_ID_20000596', 'TAXI_ID_20000597', 'TAXI_ID_20000598', 'TAXI_ID_20000600', 'TAXI_ID_20000601', 'TAXI_ID_20000602', 'TAXI_ID_20000603', 'TAXI_ID_20000604', 'TAXI_ID_20000605', 'TAXI_ID_20000606', 'TAXI_ID_20000607', 'TAXI_ID_20000608', 'TAXI_ID_20000609', 'TAXI_ID_20000610', 'TAXI_ID_20000611', 'TAXI_ID_20000612', 'TAXI_ID_20000615', 'TAXI_ID_20000616', 'TAXI_ID_20000617', 'TAXI_ID_20000618', 'TAXI_ID_20000619', 'TAXI_ID_20000620', 'TAXI_ID_20000621', 'TAXI_ID_20000623', 'TAXI_ID_20000624', 'TAXI_ID_20000625', 'TAXI_ID_20000626', 'TAXI_ID_20000632', 'TAXI_ID_20000633', 'TAXI_ID_20000634', 'TAXI_ID_20000640', 'TAXI_ID_20000648', 'TAXI_ID_20000649', 'TAXI_ID_20000651', 'TAXI_ID_20000653', 'TAXI_ID_20000657', 'TAXI_ID_20000658', 'TAXI_ID_20000662', 'TAXI_ID_20000663', 'TAXI_ID_20000664', 'TAXI_ID_20000665', 'TAXI_ID_20000667', 'TAXI_ID_20000668', 'TAXI_ID_20000669', 'TAXI_ID_20000670', 'TAXI_ID_20000671', 'TAXI_ID_20000672', 'TAXI_ID_20000674', 'TAXI_ID_20000675', 'TAXI_ID_20000676', 'TAXI_ID_20000678', 'TAXI_ID_20000680', 'TAXI_ID_20000681', 'TAXI_ID_20000682', 'TAXI_ID_20000684', 'TAXI_ID_20000685', 'TAXI_ID_20000686', 'TAXI_ID_20000687', 'TAXI_ID_20000688', 'TAXI_ID_20000692', 'TAXI_ID_20000693', 'TAXI_ID_20000695', 'TAXI_ID_20000697', 'TAXI_ID_20000698', 'TAXI_ID_20000900', 'TAXI_ID_20000901', 'TAXI_ID_20000902', 'TAXI_ID_20000903', 'TAXI_ID_20000904', 'TAXI_ID_20000911', 'TAXI_ID_20000931', 'TAXI_ID_20000940', 'TAXI_ID_20000941', 'TAXI_ID_20000969', 'TAXI_ID_20000970', 'TAXI_ID_20000980', 'TAXI_ID_20000981']\n",
      "[[1020    0    0 ...    0    0    0]\n",
      " [1028    0    1 ...    0    0    0]\n",
      " [1022    0    0 ...    0    0    0]\n",
      " ...\n",
      " [ 161    0    0 ...    0    0    0]\n",
      " [ 503    0    1 ...    0    0    0]\n",
      " [ 759    0    1 ...    0    0    0]]\n",
      "tensor([[1.0200e+03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0280e+03, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0220e+03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [1.6100e+02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [5.0300e+02, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [7.5900e+02, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "myDs=MyDataset(\"archive/train.csv\")\n",
    "train_loader=DataLoader(myDs,batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dro = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(516, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "        self.fc5 = nn.Linear(1000, 1000)\n",
    "        self.fc6 = nn.Linear(1000, 1000)\n",
    "        self.fc7 = nn.Linear(1000, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = torch.flatten(x, -1) # flatten all dimensions except batch\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = self.fc7(x)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "criterion = RMSELoss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8501], grad_fn=<SelectBackward0>) tensor([405.])\n",
      "[1,   100] loss: 620.018\n",
      "tensor([649.1924], grad_fn=<SelectBackward0>) tensor([225.])\n",
      "[1,   200] loss: 556.580\n",
      "tensor([283.8115], grad_fn=<SelectBackward0>) tensor([315.])\n",
      "[1,   300] loss: 563.184\n",
      "tensor([452.8177], grad_fn=<SelectBackward0>) tensor([210.])\n",
      "[1,   400] loss: 559.261\n",
      "tensor([749.9622], grad_fn=<SelectBackward0>) tensor([45.])\n",
      "[1,   500] loss: 563.220\n",
      "tensor([1021.0473], grad_fn=<SelectBackward0>) tensor([735.])\n",
      "[1,   600] loss: 548.171\n",
      "tensor([109.9346], grad_fn=<SelectBackward0>) tensor([450.])\n",
      "[1,   700] loss: 559.863\n",
      "tensor([164.9598], grad_fn=<SelectBackward0>) tensor([1455.])\n",
      "[1,   800] loss: 551.600\n",
      "tensor([409.7453], grad_fn=<SelectBackward0>) tensor([2280.])\n",
      "[1,   900] loss: 537.782\n",
      "tensor([235.1538], grad_fn=<SelectBackward0>) tensor([240.])\n",
      "[1,  1000] loss: 522.379\n",
      "tensor([1071.6766], grad_fn=<SelectBackward0>) tensor([540.])\n",
      "[1,  1100] loss: 504.436\n",
      "tensor([670.1725], grad_fn=<SelectBackward0>) tensor([630.])\n",
      "[1,  1200] loss: 464.994\n",
      "tensor([721.3524], grad_fn=<SelectBackward0>) tensor([885.])\n",
      "[1,  1300] loss: 448.886\n",
      "tensor([536.8511], grad_fn=<SelectBackward0>) tensor([120.])\n",
      "[1,  1400] loss: 439.022\n",
      "tensor([573.7902], grad_fn=<SelectBackward0>) tensor([1080.])\n",
      "[1,  1500] loss: 406.468\n",
      "tensor([792.1383], grad_fn=<SelectBackward0>) tensor([435.])\n",
      "[1,  1600] loss: 412.871\n",
      "tensor([628.1099], grad_fn=<SelectBackward0>) tensor([1050.])\n",
      "[1,  1700] loss: 402.463\n",
      "tensor([516.7815], grad_fn=<SelectBackward0>) tensor([30.])\n",
      "[1,  1800] loss: 399.965\n",
      "tensor([539.8996], grad_fn=<SelectBackward0>) tensor([495.])\n",
      "[1,  1900] loss: 412.706\n",
      "tensor([519.5194], grad_fn=<SelectBackward0>) tensor([90.])\n",
      "[1,  2000] loss: 401.876\n",
      "tensor([528.7623], grad_fn=<SelectBackward0>) tensor([600.])\n",
      "[1,  2100] loss: 392.962\n",
      "tensor([666.8292], grad_fn=<SelectBackward0>) tensor([405.])\n",
      "[1,  2200] loss: 403.102\n",
      "tensor([733.1481], grad_fn=<SelectBackward0>) tensor([1020.])\n",
      "[1,  2300] loss: 401.423\n",
      "tensor([692.6870], grad_fn=<SelectBackward0>) tensor([240.])\n",
      "[1,  2400] loss: 403.112\n",
      "tensor([786.2274], grad_fn=<SelectBackward0>) tensor([840.])\n",
      "[1,  2500] loss: 398.938\n",
      "tensor([697.9823], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1,  2600] loss: 404.732\n",
      "tensor([707.1173], grad_fn=<SelectBackward0>) tensor([705.])\n",
      "[1,  2700] loss: 395.107\n",
      "tensor([728.8549], grad_fn=<SelectBackward0>) tensor([1020.])\n",
      "[1,  2800] loss: 399.344\n",
      "tensor([700.2622], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[1,  2900] loss: 388.786\n",
      "tensor([795.9365], grad_fn=<SelectBackward0>) tensor([570.])\n",
      "[1,  3000] loss: 389.389\n",
      "tensor([602.9846], grad_fn=<SelectBackward0>) tensor([495.])\n",
      "[1,  3100] loss: 392.448\n",
      "tensor([707.3706], grad_fn=<SelectBackward0>) tensor([570.])\n",
      "[1,  3200] loss: 403.226\n",
      "tensor([577.4088], grad_fn=<SelectBackward0>) tensor([780.])\n",
      "[1,  3300] loss: 397.249\n",
      "tensor([726.3584], grad_fn=<SelectBackward0>) tensor([420.])\n",
      "[1,  3400] loss: 394.509\n",
      "tensor([603.8902], grad_fn=<SelectBackward0>) tensor([210.])\n",
      "[1,  3500] loss: 395.447\n",
      "tensor([631.5052], grad_fn=<SelectBackward0>) tensor([300.])\n",
      "[1,  3600] loss: 398.320\n",
      "tensor([773.4956], grad_fn=<SelectBackward0>) tensor([1290.])\n",
      "[1,  3700] loss: 399.985\n",
      "tensor([561.1307], grad_fn=<SelectBackward0>) tensor([1095.])\n",
      "[1,  3800] loss: 397.177\n",
      "tensor([694.1602], grad_fn=<SelectBackward0>) tensor([915.])\n",
      "[1,  3900] loss: 395.340\n",
      "tensor([734.8284], grad_fn=<SelectBackward0>) tensor([555.])\n",
      "[1,  4000] loss: 383.763\n",
      "tensor([621.4534], grad_fn=<SelectBackward0>) tensor([240.])\n",
      "[1,  4100] loss: 399.315\n",
      "tensor([709.4456], grad_fn=<SelectBackward0>) tensor([270.])\n",
      "[1,  4200] loss: 393.390\n",
      "tensor([778.4122], grad_fn=<SelectBackward0>) tensor([840.])\n",
      "[1,  4300] loss: 397.121\n",
      "tensor([530.7211], grad_fn=<SelectBackward0>) tensor([765.])\n",
      "[1,  4400] loss: 395.830\n",
      "tensor([653.7660], grad_fn=<SelectBackward0>) tensor([1695.])\n",
      "[1,  4500] loss: 396.890\n",
      "tensor([773.2988], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1,  4600] loss: 392.140\n",
      "tensor([557.0385], grad_fn=<SelectBackward0>) tensor([405.])\n",
      "[1,  4700] loss: 394.212\n",
      "tensor([700.5563], grad_fn=<SelectBackward0>) tensor([480.])\n",
      "[1,  4800] loss: 404.604\n",
      "tensor([579.8572], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1,  4900] loss: 398.962\n",
      "tensor([800.4532], grad_fn=<SelectBackward0>) tensor([930.])\n",
      "[1,  5000] loss: 394.349\n",
      "tensor([653.3445], grad_fn=<SelectBackward0>) tensor([570.])\n",
      "[1,  5100] loss: 398.893\n",
      "tensor([754.3463], grad_fn=<SelectBackward0>) tensor([555.])\n",
      "[1,  5200] loss: 396.680\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(outputs[0], labels[0])\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_call = df_tr.sort_values(by='ORIGIN_CALL', ascending=True)\n",
    "# print(zero_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "[752, 730, 707, 699, 743, 799, 780, 803, 843, 760, 553, 655, 651, 693, 743, 625, 669, 718, 803, 667, 859, 783, 828, 778, 664, 730, 803, 805, 688, 695, 810, 712, 753, 637, 667, 663, 829, 602, 822, 703, 624, 696, 740, 880, 665, 659, 743, 719, 573, 607, 604, 653, 662, 783, 769, 621, 883, 696, 631, 843, 813, 656, 752, 928, 902, 803, 734, 770, 713, 566, 733, 642, 703, 746, 605, 865, 637, 744, 793, 770, 727, 881, 691, 745, 683, 678, 736, 771, 618, 654, 684, 766, 846, 671, 643, 733, 715, 734, 731, 733, 614, 781, 756, 532, 545, 720, 693, 554, 785, 697, 679, 727, 612, 643, 689, 808, 637, 735, 759, 649, 744, 783, 694, 680, 589, 587, 792, 718, 811, 523, 574, 674, 757, 712, 687, 755, 765, 561, 723, 695, 865, 572, 733, 943, 670, 693, 637, 864, 790, 650, 736, 737, 744, 822, 805, 727, 977, 876, 713, 777, 887, 777, 884, 665, 874, 660, 746, 883, 758, 979, 834, 834, 782, 650, 736, 712, 816, 834, 833, 842, 736, 716, 867, 700, 747, 761, 802, 718, 706, 762, 855, 601, 891, 723, 954, 764, 815, 568, 791, 763, 772, 857, 704, 683, 836, 570, 817, 888, 888, 695, 755, 707, 763, 636, 609, 834, 609, 627, 672, 821, 807, 764, 801, 701, 739, 639, 763, 866, 567, 612, 575, 650, 602, 640, 681, 638, 677, 611, 624, 546, 696, 654, 514, 762, 666, 647, 587, 620, 602, 558, 529, 675, 857, 592, 560, 679, 595, 638, 681, 625, 621, 642, 620, 626, 614, 544, 612, 651, 588, 607, 615, 664, 617, 532, 643, 531, 530, 589, 621, 647, 641, 670, 593, 518, 642, 839, 629, 756, 642, 661, 619, 727, 716, 660, 758, 668, 773, 704, 667, 621, 740, 654, 747, 715, 790, 748, 780, 762, 643, 786, 731, 767, 734, 650, 654, 741, 677, 600, 730, 674]\n",
      "    TRIP_ID  TRAVEL_TIME\n",
      "0        T1          752\n",
      "1        T2          730\n",
      "2        T3          707\n",
      "3        T4          699\n",
      "4        T5          743\n",
      "..      ...          ...\n",
      "315    T323          741\n",
      "316    T324          677\n",
      "317    T325          600\n",
      "318    T326          730\n",
      "319    T327          674\n",
      "\n",
      "[320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = MyDataset(\"archive/test_public.csv\")\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(round(output.item()))\n",
    "ids = testset.df[\"TRIP_ID\"]        \n",
    "print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "print(newdf)\n",
    "newdf.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample submission file that is given on kaggle\n",
    "# df_sample = pd.read_csv(\"archive/sampleSubmission.csv\")\n",
    "\n",
    "# df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# # mean(716.43) -> 792.73593\n",
    "# # median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Feature Analysis\n",
    "\n",
    "For our feature analysis, we are looking at which of our engineered features may be useful in making a taxicab time regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m outlier_threshold \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# VERRRRRY long taxi trips in the dataset)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_trimmed \u001b[39m=\u001b[39m df_tr[df_tr[\u001b[39m\"\u001b[39m\u001b[39mLEN\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m mean \u001b[39m+\u001b[39m outlier_threshold \u001b[39m*\u001b[39m std]\n\u001b[0;32m     11\u001b[0m \u001b[39m# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# how many 30s trips, etc. )\u001b[39;00m\n\u001b[0;32m     14\u001b[0m buckets \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(mean \u001b[39m+\u001b[39m outlier_threshold \u001b[39m*\u001b[39m std) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m15\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "# First n samples to analyze. Set to -1 to use all data\n",
    "end = -1\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\n",
    "# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \n",
    "# how many 30s trips, etc. )\n",
    "buckets = (int(mean + outlier_threshold * std) // 15)\n",
    "\n",
    "print(f\"Using: {len(df_trimmed)}/{len(df_tr)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18,14))\n",
    "\n",
    "# Now, we visualize some features that we think might be useful\n",
    "for idx, v in enumerate([\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"ORIGIN_STAND\"]):\n",
    "  # idx // 3 = row, idx % 3 = column\n",
    "  ax = axs[idx // 3, idx % 3]\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "  df_subset = df_trimmed.dropna(subset=v)\n",
    "  \n",
    "  # Create a histogram. Look up the documentation for more details\n",
    "  ax.hist2d(df_subset[v][:end], df_subset[\"LEN\"][:end], cmap=\"CMRmap\", bins=(120,buckets))\n",
    "  \n",
    "  # Some stylistic things to make the graphs look nice\n",
    "  ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "  ax.set_facecolor(\"black\")\n",
    "  ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "  ax.set_title(f\"Feature: {v}\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for v in [0, 5, 11, 17, 23]:\n",
    "  # Filter data where the HR matches v\n",
    "  hourly_data = df_trimmed[df_trimmed[\"HR\"] == v][\"LEN\"]\n",
    "  histogram, bin_boundary = np.histogram(hourly_data, bins=buckets)\n",
    "  histogram = histogram / len(hourly_data)\n",
    "  # The center is the left_bound and right_bound of a bucket\n",
    "  bin_centers = [(bin_boundary[i] + bin_boundary[i + 1]) / 2 for i in range(buckets)]\n",
    "  plt.plot(bin_centers, histogram, label=f\"HR={v}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
