{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are all of the files you are given\n",
    "# df_tr = pd.read_csv(\"archive/train.csv\")\n",
    "# df_ts = pd.read_csv(\"archive/test_public.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "    # df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "\n",
    "# print(df_tr[\"TAXI_ID\"].value_counts)\n",
    "# mergd = pd.merge(df_tr, df_ts, on=[\"TAXI_ID\", \"ORIGIN_CALL\", \"ORIGIN_STAND\"], suffixes=(None, \"_x\"))\n",
    "# # print((mergd))\n",
    "# mergd = mergd[mergd[\"POLYLINE\"].notnull()].drop([\"TRIP_ID_x\", \"CALL_TYPE_x\", \"TIMESTAMP_x\", \"DAY_TYPE_x\", \"MISSING_DATA_x\"], axis=1)\n",
    "\n",
    "# # print(df_tr[\"ORIGIN_STAND\"].value_counts())\n",
    "# # print(mergd[\"ORIGIN_STAND\"].value_counts())\n",
    "# print((mergd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "# df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  # print(datetime.now())\n",
    "  dt = datetime.fromtimestamp(x['TIMESTAMP'])\n",
    "  # print(dt.timetuple())\n",
    "    # print(dt.year, dt.month, dt.day, dt.hour, dt.weekday(), dt.minute)\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday(), dt.timetuple().tm_yday\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "# df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "def parse_midnight_minutes(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return (dt.hour * 60 + dt.minute) / 1440"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trimming: 1710670\n",
      "After Trimming: 1656255\n",
      "Mean: 693.1521422025355 STD: 399.09803689601046 Median: 615.0 Min: 15 Max: 2760\n"
     ]
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"archive/train.csv\")\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "outlier_threshold = 3\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "df_trimmed = df_trimmed[df_trimmed['MISSING_DATA'] == False]\n",
    "df_trimmed = df_trimmed[df_trimmed['LEN'] != 0]\n",
    "print(\"Before Trimming: \" + str(len(df_tr)))\n",
    "print(\"After Trimming: \" + str(len(df_trimmed)))\n",
    "df_tr = df_trimmed\n",
    "df_tr[\"ISTEST\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_ts = pd.read_csv(\"archive/test_public.csv\")\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WKDAY\", \"YRDAY\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "df_tr = df_tr.drop([\"YR\", \"MON\", \"DAY\", \"WKDAY\"], axis=1)\n",
    "df_ts[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WKDAY\", \"YRDAY\"]] = df_ts[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "df_ts = df_ts.drop([\"YR\", \"MON\", \"DAY\", \"WKDAY\"], axis=1)\n",
    "df_ts[\"ISTEST\"] = 1\n",
    "\n",
    "#this filters df_tr such that all values in the columns in on list must match df_ts. \n",
    "#if u wanna add more columns (more filtering) add ur column here and take it out of the drop line 3 down from here\n",
    "df_tr = pd.merge(df_tr, df_ts, how=\"inner\", on=[\"TAXI_ID\"], suffixes=(None, \"_x\"))\n",
    "# print((df_tr[\"YRDAY\"].value_counts()))\n",
    "df_tr = df_tr[df_tr[\"POLYLINE\"].notnull()].drop([\"ISTEST_x\", \"HR_x\", \"YRDAY_x\", \"ORIGIN_CALL_x\", \"ORIGIN_STAND_x\", \"TRIP_ID_x\", \"CALL_TYPE_x\", \"TIMESTAMP_x\", \"DAY_TYPE_x\", \"MISSING_DATA_x\"], axis=1)\n",
    "df_ts[\"POLYLINE\"]=\"trololololo\"\n",
    "trlen = len(df_tr)\n",
    "\n",
    "\n",
    "df_both = pd.concat([df_tr, df_ts])\n",
    "df_both[\"LEN\"] = df_both[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "ocvc = df_both[\"ORIGIN_CALL\"].value_counts()\n",
    "def filterOC(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    if(ocvc[x] < 100):\n",
    "        return None\n",
    "    return x\n",
    "df_both[\"ORIGIN_CALL\"] = df_both[\"ORIGIN_CALL\"].apply(filterOC)\n",
    "# print(df_both[\"ORIGIN_CALL\"].value_counts())\n",
    "\n",
    "df_both[\"MIDMINS\"] = df_both[[\"TIMESTAMP\"]].apply(parse_midnight_minutes, axis=1, result_type=\"expand\")\n",
    "#df_tr[\"MIDMINS\"]=(df_tr[\"MIDMINS\"]-df_tr[\"MIDMINS\"].min())/(df_tr[\"MIDMINS\"].max()-df_tr[\"MIDMINS\"].min())\n",
    "df_both = pd.get_dummies(data=df_both, columns=['CALL_TYPE', \"ORIGIN_STAND\", \"ORIGIN_CALL\", \"YRDAY\", \"HR\", \"TAXI_ID\"])\n",
    "\n",
    "\n",
    "df_tr = df_both.iloc[:trlen]\n",
    "mean, std, median, min, max = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std(), df_tr[\"LEN\"].median(), df_tr[\"LEN\"].min(), df_tr[\"LEN\"].max()\n",
    "print(f\"Mean: {mean} STD: {std} Median: {median} Min: {min} Max: {max}\")\n",
    "\n",
    "df_ts = df_both.iloc[trlen:]\n",
    "\n",
    "# print(df_ts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TRIP_ID   TIMESTAMP DAY_TYPE  MISSING_DATA   \n",
      "849424  1384635089620000129  1384635089        A         False  \\\n",
      "616973  1395243728620000562  1395243728        A         False   \n",
      "329312  1381326868620000665  1381326868        A         False   \n",
      "759072  1377121528620000249  1377121528        A         False   \n",
      "955492  1385090450620000136  1385090450        A         False   \n",
      "...                     ...         ...      ...           ...   \n",
      "288                    T296  1414814057        A         False   \n",
      "142                    T148  1412064430        A         False   \n",
      "187                    T195  1412617425        A         False   \n",
      "219                    T227  1412617117        A         False   \n",
      "16                      T17  1408038804        A         False   \n",
      "\n",
      "                                                 POLYLINE   LEN  ISTEST   \n",
      "849424  [[-8.61183,41.146146],[-8.611848,41.146182],[-...   960       0  \\\n",
      "616973  [[-8.601993,41.157162],[-8.602011,41.157135],[...  1230       0   \n",
      "329312  [[-8.620821,41.163372],[-8.620614,41.164245],[...   810       0   \n",
      "759072  [[-8.609616,41.140602],[-8.609202,41.139225],[...  1275       0   \n",
      "955492  [[-8.608086,41.147298],[-8.608581,41.147469],[...   780       0   \n",
      "...                                                   ...   ...     ...   \n",
      "288                                           trololololo     0       1   \n",
      "142                                           trololololo     0       1   \n",
      "187                                           trololololo     0       1   \n",
      "219                                           trololololo     0       1   \n",
      "16                                            trololololo     0       1   \n",
      "\n",
      "         MIDMINS  CALL_TYPE_A  CALL_TYPE_B  ...  TAXI_ID_20000685   \n",
      "849424  0.535417        False        False  ...             False  \\\n",
      "616973  0.362500        False        False  ...             False   \n",
      "329312  0.287500        False        False  ...             False   \n",
      "759072  0.614583         True        False  ...             False   \n",
      "955492  0.805556        False        False  ...             False   \n",
      "...          ...          ...          ...  ...               ...   \n",
      "288     0.870833        False        False  ...             False   \n",
      "142     0.046528         True        False  ...             False   \n",
      "187     0.446528        False         True  ...             False   \n",
      "219     0.443056        False         True  ...             False   \n",
      "16      0.453472        False        False  ...             False   \n",
      "\n",
      "        TAXI_ID_20000686  TAXI_ID_20000687  TAXI_ID_20000688   \n",
      "849424             False             False             False  \\\n",
      "616973             False             False             False   \n",
      "329312             False             False             False   \n",
      "759072             False             False             False   \n",
      "955492             False             False             False   \n",
      "...                  ...               ...               ...   \n",
      "288                False             False             False   \n",
      "142                False             False             False   \n",
      "187                False             False             False   \n",
      "219                False             False             False   \n",
      "16                 False             False             False   \n",
      "\n",
      "        TAXI_ID_20000693  TAXI_ID_20000698  TAXI_ID_20000900   \n",
      "849424             False             False             False  \\\n",
      "616973             False             False             False   \n",
      "329312             False             False             False   \n",
      "759072             False             False             False   \n",
      "955492             False             False             False   \n",
      "...                  ...               ...               ...   \n",
      "288                False             False             False   \n",
      "142                False             False             False   \n",
      "187                False             False             False   \n",
      "219                False             False             False   \n",
      "16                 False             False             False   \n",
      "\n",
      "        TAXI_ID_20000901  TAXI_ID_20000903  TAXI_ID_20000904  \n",
      "849424             False             False             False  \n",
      "616973             False             False             False  \n",
      "329312             False             False             False  \n",
      "759072             False             False             False  \n",
      "955492             False             False             False  \n",
      "...                  ...               ...               ...  \n",
      "288                False             False             False  \n",
      "142                False             False             False  \n",
      "187                False             False             False  \n",
      "219                False             False             False  \n",
      "16                 False             False             False  \n",
      "\n",
      "[1309400 rows x 877 columns]\n"
     ]
    }
   ],
   "source": [
    "df_bin = pd.concat([df_tr.sample(n=round(len(df_tr)/2)), df_ts.sample(n=round(len(df_tr)/2), replace=True)])\n",
    "print(df_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last training recorded on:  2014-06-30 16:59:14\n",
      "first testing recorded on:  2014-08-14 09:02:23\n"
     ]
    }
   ],
   "source": [
    "maxtrain = df_tr[\"TIMESTAMP\"].max()\n",
    "mintest = df_ts[\"TIMESTAMP\"].min()\n",
    "print(\"last training recorded on: \", datetime.fromtimestamp(maxtrain))\n",
    "print(\"first testing recorded on: \", datetime.fromtimestamp(mintest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_tr[df_tr[\"ORIGIN_CALL_2002.0\"]][\"LEN\"].mean())\n",
    "# print(df_tr[\"LEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, df, bin=False):\n",
    "\n",
    "    boolcols = list(df.columns)\n",
    "    badcols = [\"TRIP_ID\", 'MIDMINS',  'TIMESTAMP', 'MISSING_DATA', 'POLYLINE', 'LEN', \"DAY_TYPE\", \"ISTEST\"] \n",
    "    for b in badcols:\n",
    "      boolcols.remove(b)\n",
    "    print(boolcols)\n",
    "    \n",
    "    \n",
    "         \n",
    "    boolz=df[boolcols].values\n",
    "    intz = df[\"MIDMINS\"].values\n",
    "    # print(boolz)\n",
    "    y=df[\"LEN\"].values\n",
    "    if(bin):\n",
    "      y = df[\"ISTEST\"].values\n",
    "\n",
    "    booltens = torch.tensor(boolz,dtype=torch.float32)\n",
    "    inttens = torch.tensor(intz,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # print(booltens.shape)\n",
    "    # print(inttens.shape)\n",
    " \n",
    "    self.x_train=torch.cat([booltens, inttens], dim=1)\n",
    "    # print(self.x_train)\n",
    "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "    self.df = df\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'ORIGIN_STAND_1.0', 'ORIGIN_STAND_2.0', 'ORIGIN_STAND_3.0', 'ORIGIN_STAND_4.0', 'ORIGIN_STAND_5.0', 'ORIGIN_STAND_6.0', 'ORIGIN_STAND_7.0', 'ORIGIN_STAND_8.0', 'ORIGIN_STAND_9.0', 'ORIGIN_STAND_10.0', 'ORIGIN_STAND_11.0', 'ORIGIN_STAND_12.0', 'ORIGIN_STAND_13.0', 'ORIGIN_STAND_14.0', 'ORIGIN_STAND_15.0', 'ORIGIN_STAND_16.0', 'ORIGIN_STAND_17.0', 'ORIGIN_STAND_18.0', 'ORIGIN_STAND_19.0', 'ORIGIN_STAND_20.0', 'ORIGIN_STAND_21.0', 'ORIGIN_STAND_22.0', 'ORIGIN_STAND_23.0', 'ORIGIN_STAND_24.0', 'ORIGIN_STAND_25.0', 'ORIGIN_STAND_26.0', 'ORIGIN_STAND_27.0', 'ORIGIN_STAND_28.0', 'ORIGIN_STAND_29.0', 'ORIGIN_STAND_30.0', 'ORIGIN_STAND_31.0', 'ORIGIN_STAND_32.0', 'ORIGIN_STAND_33.0', 'ORIGIN_STAND_34.0', 'ORIGIN_STAND_35.0', 'ORIGIN_STAND_36.0', 'ORIGIN_STAND_37.0', 'ORIGIN_STAND_38.0', 'ORIGIN_STAND_39.0', 'ORIGIN_STAND_40.0', 'ORIGIN_STAND_41.0', 'ORIGIN_STAND_42.0', 'ORIGIN_STAND_43.0', 'ORIGIN_STAND_44.0', 'ORIGIN_STAND_45.0', 'ORIGIN_STAND_46.0', 'ORIGIN_STAND_47.0', 'ORIGIN_STAND_48.0', 'ORIGIN_STAND_49.0', 'ORIGIN_STAND_50.0', 'ORIGIN_STAND_51.0', 'ORIGIN_STAND_52.0', 'ORIGIN_STAND_53.0', 'ORIGIN_STAND_54.0', 'ORIGIN_STAND_55.0', 'ORIGIN_STAND_56.0', 'ORIGIN_STAND_57.0', 'ORIGIN_STAND_58.0', 'ORIGIN_STAND_59.0', 'ORIGIN_STAND_60.0', 'ORIGIN_STAND_61.0', 'ORIGIN_STAND_62.0', 'ORIGIN_STAND_63.0', 'ORIGIN_CALL_2001.0', 'ORIGIN_CALL_2002.0', 'ORIGIN_CALL_2024.0', 'ORIGIN_CALL_2747.0', 'ORIGIN_CALL_2801.0', 'ORIGIN_CALL_2974.0', 'ORIGIN_CALL_3710.0', 'ORIGIN_CALL_3711.0', 'ORIGIN_CALL_3712.0', 'ORIGIN_CALL_3731.0', 'ORIGIN_CALL_3734.0', 'ORIGIN_CALL_4196.0', 'ORIGIN_CALL_4206.0', 'ORIGIN_CALL_4293.0', 'ORIGIN_CALL_4609.0', 'ORIGIN_CALL_4618.0', 'ORIGIN_CALL_4619.0', 'ORIGIN_CALL_4646.0', 'ORIGIN_CALL_4691.0', 'ORIGIN_CALL_4784.0', 'ORIGIN_CALL_4792.0', 'ORIGIN_CALL_4819.0', 'ORIGIN_CALL_5081.0', 'ORIGIN_CALL_5182.0', 'ORIGIN_CALL_5239.0', 'ORIGIN_CALL_5347.0', 'ORIGIN_CALL_5591.0', 'ORIGIN_CALL_5602.0', 'ORIGIN_CALL_5779.0', 'ORIGIN_CALL_5926.0', 'ORIGIN_CALL_6084.0', 'ORIGIN_CALL_6089.0', 'ORIGIN_CALL_6584.0', 'ORIGIN_CALL_6654.0', 'ORIGIN_CALL_6659.0', 'ORIGIN_CALL_6675.0', 'ORIGIN_CALL_6714.0', 'ORIGIN_CALL_6715.0', 'ORIGIN_CALL_6728.0', 'ORIGIN_CALL_6896.0', 'ORIGIN_CALL_7817.0', 'ORIGIN_CALL_8246.0', 'ORIGIN_CALL_8847.0', 'ORIGIN_CALL_8939.0', 'ORIGIN_CALL_8950.0', 'ORIGIN_CALL_9139.0', 'ORIGIN_CALL_9151.0', 'ORIGIN_CALL_9327.0', 'ORIGIN_CALL_9393.0', 'ORIGIN_CALL_9676.0', 'ORIGIN_CALL_9682.0', 'ORIGIN_CALL_9705.0', 'ORIGIN_CALL_10456.0', 'ORIGIN_CALL_10591.0', 'ORIGIN_CALL_11066.0', 'ORIGIN_CALL_11330.0', 'ORIGIN_CALL_11499.0', 'ORIGIN_CALL_11763.0', 'ORIGIN_CALL_11868.0', 'ORIGIN_CALL_12161.0', 'ORIGIN_CALL_12190.0', 'ORIGIN_CALL_12394.0', 'ORIGIN_CALL_12482.0', 'ORIGIN_CALL_12536.0', 'ORIGIN_CALL_12564.0', 'ORIGIN_CALL_12616.0', 'ORIGIN_CALL_12692.0', 'ORIGIN_CALL_12728.0', 'ORIGIN_CALL_12773.0', 'ORIGIN_CALL_12897.0', 'ORIGIN_CALL_13009.0', 'ORIGIN_CALL_13168.0', 'ORIGIN_CALL_13226.0', 'ORIGIN_CALL_13287.0', 'ORIGIN_CALL_13360.0', 'ORIGIN_CALL_14045.0', 'ORIGIN_CALL_14083.0', 'ORIGIN_CALL_14084.0', 'ORIGIN_CALL_14085.0', 'ORIGIN_CALL_14104.0', 'ORIGIN_CALL_14118.0', 'ORIGIN_CALL_14123.0', 'ORIGIN_CALL_14133.0', 'ORIGIN_CALL_14134.0', 'ORIGIN_CALL_14144.0', 'ORIGIN_CALL_14182.0', 'ORIGIN_CALL_14853.0', 'ORIGIN_CALL_14918.0', 'ORIGIN_CALL_15086.0', 'ORIGIN_CALL_15097.0', 'ORIGIN_CALL_15242.0', 'ORIGIN_CALL_15339.0', 'ORIGIN_CALL_15356.0', 'ORIGIN_CALL_15420.0', 'ORIGIN_CALL_15427.0', 'ORIGIN_CALL_15608.0', 'ORIGIN_CALL_15617.0', 'ORIGIN_CALL_15689.0', 'ORIGIN_CALL_15756.0', 'ORIGIN_CALL_15769.0', 'ORIGIN_CALL_15792.0', 'ORIGIN_CALL_16090.0', 'ORIGIN_CALL_16096.0', 'ORIGIN_CALL_16414.0', 'ORIGIN_CALL_16644.0', 'ORIGIN_CALL_16683.0', 'ORIGIN_CALL_16865.0', 'ORIGIN_CALL_17343.0', 'ORIGIN_CALL_18020.0', 'ORIGIN_CALL_18439.0', 'ORIGIN_CALL_18561.0', 'ORIGIN_CALL_18599.0', 'ORIGIN_CALL_18820.0', 'ORIGIN_CALL_19141.0', 'ORIGIN_CALL_19150.0', 'ORIGIN_CALL_19344.0', 'ORIGIN_CALL_22438.0', 'ORIGIN_CALL_23692.0', 'ORIGIN_CALL_23874.0', 'ORIGIN_CALL_23998.0', 'ORIGIN_CALL_24199.0', 'ORIGIN_CALL_24323.0', 'ORIGIN_CALL_25664.0', 'ORIGIN_CALL_26617.0', 'ORIGIN_CALL_26752.0', 'ORIGIN_CALL_27171.0', 'ORIGIN_CALL_28065.0', 'ORIGIN_CALL_28951.0', 'ORIGIN_CALL_29534.0', 'ORIGIN_CALL_29682.0', 'ORIGIN_CALL_30604.0', 'ORIGIN_CALL_30608.0', 'ORIGIN_CALL_30643.0', 'ORIGIN_CALL_30726.0', 'ORIGIN_CALL_33535.0', 'ORIGIN_CALL_34525.0', 'ORIGIN_CALL_34861.0', 'ORIGIN_CALL_35601.0', 'ORIGIN_CALL_36195.0', 'ORIGIN_CALL_36601.0', 'ORIGIN_CALL_36773.0', 'ORIGIN_CALL_37007.0', 'ORIGIN_CALL_37083.0', 'ORIGIN_CALL_38347.0', 'ORIGIN_CALL_39530.0', 'ORIGIN_CALL_40352.0', 'ORIGIN_CALL_40433.0', 'ORIGIN_CALL_40886.0', 'ORIGIN_CALL_41277.0', 'ORIGIN_CALL_43024.0', 'ORIGIN_CALL_43288.0', 'ORIGIN_CALL_43421.0', 'ORIGIN_CALL_44210.0', 'ORIGIN_CALL_47219.0', 'ORIGIN_CALL_47504.0', 'ORIGIN_CALL_52543.0', 'ORIGIN_CALL_55014.0', 'ORIGIN_CALL_55325.0', 'ORIGIN_CALL_55387.0', 'ORIGIN_CALL_56289.0', 'ORIGIN_CALL_56610.0', 'ORIGIN_CALL_56888.0', 'ORIGIN_CALL_56903.0', 'ORIGIN_CALL_57075.0', 'ORIGIN_CALL_58637.0', 'ORIGIN_CALL_59108.0', 'ORIGIN_CALL_60441.0', 'ORIGIN_CALL_60911.0', 'ORIGIN_CALL_61064.0', 'ORIGIN_CALL_63882.0', 'YRDAY_1', 'YRDAY_2', 'YRDAY_3', 'YRDAY_4', 'YRDAY_5', 'YRDAY_6', 'YRDAY_7', 'YRDAY_8', 'YRDAY_9', 'YRDAY_10', 'YRDAY_11', 'YRDAY_12', 'YRDAY_13', 'YRDAY_14', 'YRDAY_15', 'YRDAY_16', 'YRDAY_17', 'YRDAY_18', 'YRDAY_19', 'YRDAY_20', 'YRDAY_21', 'YRDAY_22', 'YRDAY_23', 'YRDAY_24', 'YRDAY_25', 'YRDAY_26', 'YRDAY_27', 'YRDAY_28', 'YRDAY_29', 'YRDAY_30', 'YRDAY_31', 'YRDAY_32', 'YRDAY_33', 'YRDAY_34', 'YRDAY_35', 'YRDAY_36', 'YRDAY_37', 'YRDAY_38', 'YRDAY_39', 'YRDAY_40', 'YRDAY_41', 'YRDAY_42', 'YRDAY_43', 'YRDAY_44', 'YRDAY_45', 'YRDAY_46', 'YRDAY_47', 'YRDAY_48', 'YRDAY_49', 'YRDAY_50', 'YRDAY_51', 'YRDAY_52', 'YRDAY_53', 'YRDAY_54', 'YRDAY_55', 'YRDAY_56', 'YRDAY_57', 'YRDAY_58', 'YRDAY_59', 'YRDAY_60', 'YRDAY_61', 'YRDAY_62', 'YRDAY_63', 'YRDAY_64', 'YRDAY_65', 'YRDAY_66', 'YRDAY_67', 'YRDAY_68', 'YRDAY_69', 'YRDAY_70', 'YRDAY_71', 'YRDAY_72', 'YRDAY_73', 'YRDAY_74', 'YRDAY_75', 'YRDAY_76', 'YRDAY_77', 'YRDAY_78', 'YRDAY_79', 'YRDAY_80', 'YRDAY_81', 'YRDAY_82', 'YRDAY_83', 'YRDAY_84', 'YRDAY_85', 'YRDAY_86', 'YRDAY_87', 'YRDAY_88', 'YRDAY_89', 'YRDAY_90', 'YRDAY_91', 'YRDAY_92', 'YRDAY_93', 'YRDAY_94', 'YRDAY_95', 'YRDAY_96', 'YRDAY_97', 'YRDAY_98', 'YRDAY_99', 'YRDAY_100', 'YRDAY_101', 'YRDAY_102', 'YRDAY_103', 'YRDAY_104', 'YRDAY_105', 'YRDAY_106', 'YRDAY_107', 'YRDAY_108', 'YRDAY_109', 'YRDAY_110', 'YRDAY_111', 'YRDAY_112', 'YRDAY_113', 'YRDAY_114', 'YRDAY_115', 'YRDAY_116', 'YRDAY_117', 'YRDAY_118', 'YRDAY_119', 'YRDAY_120', 'YRDAY_121', 'YRDAY_122', 'YRDAY_123', 'YRDAY_124', 'YRDAY_125', 'YRDAY_126', 'YRDAY_127', 'YRDAY_128', 'YRDAY_129', 'YRDAY_130', 'YRDAY_131', 'YRDAY_132', 'YRDAY_133', 'YRDAY_134', 'YRDAY_135', 'YRDAY_136', 'YRDAY_137', 'YRDAY_138', 'YRDAY_139', 'YRDAY_140', 'YRDAY_141', 'YRDAY_142', 'YRDAY_143', 'YRDAY_144', 'YRDAY_145', 'YRDAY_146', 'YRDAY_147', 'YRDAY_148', 'YRDAY_149', 'YRDAY_150', 'YRDAY_151', 'YRDAY_152', 'YRDAY_153', 'YRDAY_154', 'YRDAY_155', 'YRDAY_156', 'YRDAY_157', 'YRDAY_158', 'YRDAY_159', 'YRDAY_160', 'YRDAY_161', 'YRDAY_162', 'YRDAY_163', 'YRDAY_164', 'YRDAY_165', 'YRDAY_166', 'YRDAY_167', 'YRDAY_168', 'YRDAY_169', 'YRDAY_170', 'YRDAY_171', 'YRDAY_172', 'YRDAY_173', 'YRDAY_174', 'YRDAY_175', 'YRDAY_176', 'YRDAY_177', 'YRDAY_178', 'YRDAY_179', 'YRDAY_180', 'YRDAY_181', 'YRDAY_182', 'YRDAY_183', 'YRDAY_184', 'YRDAY_185', 'YRDAY_186', 'YRDAY_187', 'YRDAY_188', 'YRDAY_189', 'YRDAY_190', 'YRDAY_191', 'YRDAY_192', 'YRDAY_193', 'YRDAY_194', 'YRDAY_195', 'YRDAY_196', 'YRDAY_197', 'YRDAY_198', 'YRDAY_199', 'YRDAY_200', 'YRDAY_201', 'YRDAY_202', 'YRDAY_203', 'YRDAY_204', 'YRDAY_205', 'YRDAY_206', 'YRDAY_207', 'YRDAY_208', 'YRDAY_209', 'YRDAY_210', 'YRDAY_211', 'YRDAY_212', 'YRDAY_213', 'YRDAY_214', 'YRDAY_215', 'YRDAY_216', 'YRDAY_217', 'YRDAY_218', 'YRDAY_219', 'YRDAY_220', 'YRDAY_221', 'YRDAY_222', 'YRDAY_223', 'YRDAY_224', 'YRDAY_225', 'YRDAY_226', 'YRDAY_227', 'YRDAY_228', 'YRDAY_229', 'YRDAY_230', 'YRDAY_231', 'YRDAY_232', 'YRDAY_233', 'YRDAY_234', 'YRDAY_235', 'YRDAY_236', 'YRDAY_237', 'YRDAY_238', 'YRDAY_239', 'YRDAY_240', 'YRDAY_241', 'YRDAY_242', 'YRDAY_243', 'YRDAY_244', 'YRDAY_245', 'YRDAY_246', 'YRDAY_247', 'YRDAY_248', 'YRDAY_249', 'YRDAY_250', 'YRDAY_251', 'YRDAY_252', 'YRDAY_253', 'YRDAY_254', 'YRDAY_255', 'YRDAY_256', 'YRDAY_257', 'YRDAY_258', 'YRDAY_259', 'YRDAY_260', 'YRDAY_261', 'YRDAY_262', 'YRDAY_263', 'YRDAY_264', 'YRDAY_265', 'YRDAY_266', 'YRDAY_267', 'YRDAY_268', 'YRDAY_269', 'YRDAY_270', 'YRDAY_271', 'YRDAY_272', 'YRDAY_273', 'YRDAY_274', 'YRDAY_275', 'YRDAY_276', 'YRDAY_277', 'YRDAY_278', 'YRDAY_279', 'YRDAY_280', 'YRDAY_281', 'YRDAY_282', 'YRDAY_283', 'YRDAY_284', 'YRDAY_285', 'YRDAY_286', 'YRDAY_287', 'YRDAY_288', 'YRDAY_289', 'YRDAY_290', 'YRDAY_291', 'YRDAY_292', 'YRDAY_293', 'YRDAY_294', 'YRDAY_295', 'YRDAY_296', 'YRDAY_297', 'YRDAY_298', 'YRDAY_299', 'YRDAY_300', 'YRDAY_301', 'YRDAY_302', 'YRDAY_303', 'YRDAY_304', 'YRDAY_305', 'YRDAY_306', 'YRDAY_307', 'YRDAY_308', 'YRDAY_309', 'YRDAY_310', 'YRDAY_311', 'YRDAY_312', 'YRDAY_313', 'YRDAY_314', 'YRDAY_315', 'YRDAY_316', 'YRDAY_317', 'YRDAY_318', 'YRDAY_319', 'YRDAY_320', 'YRDAY_321', 'YRDAY_322', 'YRDAY_323', 'YRDAY_324', 'YRDAY_325', 'YRDAY_326', 'YRDAY_327', 'YRDAY_328', 'YRDAY_329', 'YRDAY_330', 'YRDAY_331', 'YRDAY_332', 'YRDAY_333', 'YRDAY_334', 'YRDAY_335', 'YRDAY_336', 'YRDAY_337', 'YRDAY_338', 'YRDAY_339', 'YRDAY_340', 'YRDAY_341', 'YRDAY_342', 'YRDAY_343', 'YRDAY_344', 'YRDAY_345', 'YRDAY_346', 'YRDAY_347', 'YRDAY_348', 'YRDAY_349', 'YRDAY_350', 'YRDAY_351', 'YRDAY_352', 'YRDAY_353', 'YRDAY_354', 'YRDAY_355', 'YRDAY_356', 'YRDAY_357', 'YRDAY_358', 'YRDAY_359', 'YRDAY_360', 'YRDAY_361', 'YRDAY_362', 'YRDAY_363', 'YRDAY_364', 'YRDAY_365', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'TAXI_ID_20000004', 'TAXI_ID_20000005', 'TAXI_ID_20000008', 'TAXI_ID_20000009', 'TAXI_ID_20000010', 'TAXI_ID_20000012', 'TAXI_ID_20000015', 'TAXI_ID_20000017', 'TAXI_ID_20000020', 'TAXI_ID_20000021', 'TAXI_ID_20000022', 'TAXI_ID_20000026', 'TAXI_ID_20000036', 'TAXI_ID_20000039', 'TAXI_ID_20000040', 'TAXI_ID_20000041', 'TAXI_ID_20000044', 'TAXI_ID_20000047', 'TAXI_ID_20000048', 'TAXI_ID_20000049', 'TAXI_ID_20000051', 'TAXI_ID_20000053', 'TAXI_ID_20000054', 'TAXI_ID_20000055', 'TAXI_ID_20000057', 'TAXI_ID_20000060', 'TAXI_ID_20000067', 'TAXI_ID_20000071', 'TAXI_ID_20000079', 'TAXI_ID_20000081', 'TAXI_ID_20000085', 'TAXI_ID_20000086', 'TAXI_ID_20000092', 'TAXI_ID_20000099', 'TAXI_ID_20000100', 'TAXI_ID_20000101', 'TAXI_ID_20000105', 'TAXI_ID_20000108', 'TAXI_ID_20000109', 'TAXI_ID_20000112', 'TAXI_ID_20000116', 'TAXI_ID_20000118', 'TAXI_ID_20000121', 'TAXI_ID_20000123', 'TAXI_ID_20000126', 'TAXI_ID_20000128', 'TAXI_ID_20000129', 'TAXI_ID_20000136', 'TAXI_ID_20000140', 'TAXI_ID_20000144', 'TAXI_ID_20000146', 'TAXI_ID_20000148', 'TAXI_ID_20000154', 'TAXI_ID_20000156', 'TAXI_ID_20000158', 'TAXI_ID_20000159', 'TAXI_ID_20000160', 'TAXI_ID_20000163', 'TAXI_ID_20000166', 'TAXI_ID_20000167', 'TAXI_ID_20000171', 'TAXI_ID_20000177', 'TAXI_ID_20000178', 'TAXI_ID_20000180', 'TAXI_ID_20000185', 'TAXI_ID_20000188', 'TAXI_ID_20000190', 'TAXI_ID_20000192', 'TAXI_ID_20000197', 'TAXI_ID_20000198', 'TAXI_ID_20000199', 'TAXI_ID_20000206', 'TAXI_ID_20000207', 'TAXI_ID_20000213', 'TAXI_ID_20000222', 'TAXI_ID_20000224', 'TAXI_ID_20000230', 'TAXI_ID_20000235', 'TAXI_ID_20000239', 'TAXI_ID_20000243', 'TAXI_ID_20000245', 'TAXI_ID_20000247', 'TAXI_ID_20000248', 'TAXI_ID_20000249', 'TAXI_ID_20000250', 'TAXI_ID_20000252', 'TAXI_ID_20000255', 'TAXI_ID_20000256', 'TAXI_ID_20000260', 'TAXI_ID_20000261', 'TAXI_ID_20000263', 'TAXI_ID_20000268', 'TAXI_ID_20000272', 'TAXI_ID_20000276', 'TAXI_ID_20000280', 'TAXI_ID_20000281', 'TAXI_ID_20000285', 'TAXI_ID_20000286', 'TAXI_ID_20000288', 'TAXI_ID_20000294', 'TAXI_ID_20000295', 'TAXI_ID_20000296', 'TAXI_ID_20000303', 'TAXI_ID_20000304', 'TAXI_ID_20000307', 'TAXI_ID_20000310', 'TAXI_ID_20000311', 'TAXI_ID_20000312', 'TAXI_ID_20000314', 'TAXI_ID_20000320', 'TAXI_ID_20000325', 'TAXI_ID_20000327', 'TAXI_ID_20000328', 'TAXI_ID_20000331', 'TAXI_ID_20000333', 'TAXI_ID_20000334', 'TAXI_ID_20000338', 'TAXI_ID_20000340', 'TAXI_ID_20000342', 'TAXI_ID_20000345', 'TAXI_ID_20000347', 'TAXI_ID_20000349', 'TAXI_ID_20000351', 'TAXI_ID_20000352', 'TAXI_ID_20000353', 'TAXI_ID_20000356', 'TAXI_ID_20000361', 'TAXI_ID_20000362', 'TAXI_ID_20000363', 'TAXI_ID_20000370', 'TAXI_ID_20000372', 'TAXI_ID_20000377', 'TAXI_ID_20000381', 'TAXI_ID_20000383', 'TAXI_ID_20000384', 'TAXI_ID_20000387', 'TAXI_ID_20000391', 'TAXI_ID_20000393', 'TAXI_ID_20000395', 'TAXI_ID_20000400', 'TAXI_ID_20000403', 'TAXI_ID_20000406', 'TAXI_ID_20000407', 'TAXI_ID_20000410', 'TAXI_ID_20000421', 'TAXI_ID_20000424', 'TAXI_ID_20000426', 'TAXI_ID_20000429', 'TAXI_ID_20000430', 'TAXI_ID_20000431', 'TAXI_ID_20000434', 'TAXI_ID_20000436', 'TAXI_ID_20000440', 'TAXI_ID_20000446', 'TAXI_ID_20000450', 'TAXI_ID_20000452', 'TAXI_ID_20000453', 'TAXI_ID_20000454', 'TAXI_ID_20000455', 'TAXI_ID_20000456', 'TAXI_ID_20000460', 'TAXI_ID_20000463', 'TAXI_ID_20000464', 'TAXI_ID_20000467', 'TAXI_ID_20000473', 'TAXI_ID_20000476', 'TAXI_ID_20000477', 'TAXI_ID_20000480', 'TAXI_ID_20000483', 'TAXI_ID_20000486', 'TAXI_ID_20000488', 'TAXI_ID_20000492', 'TAXI_ID_20000494', 'TAXI_ID_20000495', 'TAXI_ID_20000496', 'TAXI_ID_20000497', 'TAXI_ID_20000499', 'TAXI_ID_20000500', 'TAXI_ID_20000502', 'TAXI_ID_20000503', 'TAXI_ID_20000510', 'TAXI_ID_20000513', 'TAXI_ID_20000517', 'TAXI_ID_20000518', 'TAXI_ID_20000523', 'TAXI_ID_20000525', 'TAXI_ID_20000529', 'TAXI_ID_20000539', 'TAXI_ID_20000540', 'TAXI_ID_20000541', 'TAXI_ID_20000542', 'TAXI_ID_20000543', 'TAXI_ID_20000546', 'TAXI_ID_20000547', 'TAXI_ID_20000548', 'TAXI_ID_20000549', 'TAXI_ID_20000554', 'TAXI_ID_20000560', 'TAXI_ID_20000561', 'TAXI_ID_20000562', 'TAXI_ID_20000565', 'TAXI_ID_20000569', 'TAXI_ID_20000572', 'TAXI_ID_20000574', 'TAXI_ID_20000576', 'TAXI_ID_20000577', 'TAXI_ID_20000578', 'TAXI_ID_20000589', 'TAXI_ID_20000591', 'TAXI_ID_20000595', 'TAXI_ID_20000596', 'TAXI_ID_20000597', 'TAXI_ID_20000603', 'TAXI_ID_20000607', 'TAXI_ID_20000612', 'TAXI_ID_20000617', 'TAXI_ID_20000619', 'TAXI_ID_20000621', 'TAXI_ID_20000624', 'TAXI_ID_20000625', 'TAXI_ID_20000626', 'TAXI_ID_20000632', 'TAXI_ID_20000633', 'TAXI_ID_20000649', 'TAXI_ID_20000653', 'TAXI_ID_20000657', 'TAXI_ID_20000662', 'TAXI_ID_20000664', 'TAXI_ID_20000665', 'TAXI_ID_20000667', 'TAXI_ID_20000668', 'TAXI_ID_20000675', 'TAXI_ID_20000678', 'TAXI_ID_20000682', 'TAXI_ID_20000685', 'TAXI_ID_20000686', 'TAXI_ID_20000687', 'TAXI_ID_20000688', 'TAXI_ID_20000693', 'TAXI_ID_20000698', 'TAXI_ID_20000900', 'TAXI_ID_20000901', 'TAXI_ID_20000903', 'TAXI_ID_20000904']\n",
      "1243930 65470\n",
      "['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'ORIGIN_STAND_1.0', 'ORIGIN_STAND_2.0', 'ORIGIN_STAND_3.0', 'ORIGIN_STAND_4.0', 'ORIGIN_STAND_5.0', 'ORIGIN_STAND_6.0', 'ORIGIN_STAND_7.0', 'ORIGIN_STAND_8.0', 'ORIGIN_STAND_9.0', 'ORIGIN_STAND_10.0', 'ORIGIN_STAND_11.0', 'ORIGIN_STAND_12.0', 'ORIGIN_STAND_13.0', 'ORIGIN_STAND_14.0', 'ORIGIN_STAND_15.0', 'ORIGIN_STAND_16.0', 'ORIGIN_STAND_17.0', 'ORIGIN_STAND_18.0', 'ORIGIN_STAND_19.0', 'ORIGIN_STAND_20.0', 'ORIGIN_STAND_21.0', 'ORIGIN_STAND_22.0', 'ORIGIN_STAND_23.0', 'ORIGIN_STAND_24.0', 'ORIGIN_STAND_25.0', 'ORIGIN_STAND_26.0', 'ORIGIN_STAND_27.0', 'ORIGIN_STAND_28.0', 'ORIGIN_STAND_29.0', 'ORIGIN_STAND_30.0', 'ORIGIN_STAND_31.0', 'ORIGIN_STAND_32.0', 'ORIGIN_STAND_33.0', 'ORIGIN_STAND_34.0', 'ORIGIN_STAND_35.0', 'ORIGIN_STAND_36.0', 'ORIGIN_STAND_37.0', 'ORIGIN_STAND_38.0', 'ORIGIN_STAND_39.0', 'ORIGIN_STAND_40.0', 'ORIGIN_STAND_41.0', 'ORIGIN_STAND_42.0', 'ORIGIN_STAND_43.0', 'ORIGIN_STAND_44.0', 'ORIGIN_STAND_45.0', 'ORIGIN_STAND_46.0', 'ORIGIN_STAND_47.0', 'ORIGIN_STAND_48.0', 'ORIGIN_STAND_49.0', 'ORIGIN_STAND_50.0', 'ORIGIN_STAND_51.0', 'ORIGIN_STAND_52.0', 'ORIGIN_STAND_53.0', 'ORIGIN_STAND_54.0', 'ORIGIN_STAND_55.0', 'ORIGIN_STAND_56.0', 'ORIGIN_STAND_57.0', 'ORIGIN_STAND_58.0', 'ORIGIN_STAND_59.0', 'ORIGIN_STAND_60.0', 'ORIGIN_STAND_61.0', 'ORIGIN_STAND_62.0', 'ORIGIN_STAND_63.0', 'ORIGIN_CALL_2001.0', 'ORIGIN_CALL_2002.0', 'ORIGIN_CALL_2024.0', 'ORIGIN_CALL_2747.0', 'ORIGIN_CALL_2801.0', 'ORIGIN_CALL_2974.0', 'ORIGIN_CALL_3710.0', 'ORIGIN_CALL_3711.0', 'ORIGIN_CALL_3712.0', 'ORIGIN_CALL_3731.0', 'ORIGIN_CALL_3734.0', 'ORIGIN_CALL_4196.0', 'ORIGIN_CALL_4206.0', 'ORIGIN_CALL_4293.0', 'ORIGIN_CALL_4609.0', 'ORIGIN_CALL_4618.0', 'ORIGIN_CALL_4619.0', 'ORIGIN_CALL_4646.0', 'ORIGIN_CALL_4691.0', 'ORIGIN_CALL_4784.0', 'ORIGIN_CALL_4792.0', 'ORIGIN_CALL_4819.0', 'ORIGIN_CALL_5081.0', 'ORIGIN_CALL_5182.0', 'ORIGIN_CALL_5239.0', 'ORIGIN_CALL_5347.0', 'ORIGIN_CALL_5591.0', 'ORIGIN_CALL_5602.0', 'ORIGIN_CALL_5779.0', 'ORIGIN_CALL_5926.0', 'ORIGIN_CALL_6084.0', 'ORIGIN_CALL_6089.0', 'ORIGIN_CALL_6584.0', 'ORIGIN_CALL_6654.0', 'ORIGIN_CALL_6659.0', 'ORIGIN_CALL_6675.0', 'ORIGIN_CALL_6714.0', 'ORIGIN_CALL_6715.0', 'ORIGIN_CALL_6728.0', 'ORIGIN_CALL_6896.0', 'ORIGIN_CALL_7817.0', 'ORIGIN_CALL_8246.0', 'ORIGIN_CALL_8847.0', 'ORIGIN_CALL_8939.0', 'ORIGIN_CALL_8950.0', 'ORIGIN_CALL_9139.0', 'ORIGIN_CALL_9151.0', 'ORIGIN_CALL_9327.0', 'ORIGIN_CALL_9393.0', 'ORIGIN_CALL_9676.0', 'ORIGIN_CALL_9682.0', 'ORIGIN_CALL_9705.0', 'ORIGIN_CALL_10456.0', 'ORIGIN_CALL_10591.0', 'ORIGIN_CALL_11066.0', 'ORIGIN_CALL_11330.0', 'ORIGIN_CALL_11499.0', 'ORIGIN_CALL_11763.0', 'ORIGIN_CALL_11868.0', 'ORIGIN_CALL_12161.0', 'ORIGIN_CALL_12190.0', 'ORIGIN_CALL_12394.0', 'ORIGIN_CALL_12482.0', 'ORIGIN_CALL_12536.0', 'ORIGIN_CALL_12564.0', 'ORIGIN_CALL_12616.0', 'ORIGIN_CALL_12692.0', 'ORIGIN_CALL_12728.0', 'ORIGIN_CALL_12773.0', 'ORIGIN_CALL_12897.0', 'ORIGIN_CALL_13009.0', 'ORIGIN_CALL_13168.0', 'ORIGIN_CALL_13226.0', 'ORIGIN_CALL_13287.0', 'ORIGIN_CALL_13360.0', 'ORIGIN_CALL_14045.0', 'ORIGIN_CALL_14083.0', 'ORIGIN_CALL_14084.0', 'ORIGIN_CALL_14085.0', 'ORIGIN_CALL_14104.0', 'ORIGIN_CALL_14118.0', 'ORIGIN_CALL_14123.0', 'ORIGIN_CALL_14133.0', 'ORIGIN_CALL_14134.0', 'ORIGIN_CALL_14144.0', 'ORIGIN_CALL_14182.0', 'ORIGIN_CALL_14853.0', 'ORIGIN_CALL_14918.0', 'ORIGIN_CALL_15086.0', 'ORIGIN_CALL_15097.0', 'ORIGIN_CALL_15242.0', 'ORIGIN_CALL_15339.0', 'ORIGIN_CALL_15356.0', 'ORIGIN_CALL_15420.0', 'ORIGIN_CALL_15427.0', 'ORIGIN_CALL_15608.0', 'ORIGIN_CALL_15617.0', 'ORIGIN_CALL_15689.0', 'ORIGIN_CALL_15756.0', 'ORIGIN_CALL_15769.0', 'ORIGIN_CALL_15792.0', 'ORIGIN_CALL_16090.0', 'ORIGIN_CALL_16096.0', 'ORIGIN_CALL_16414.0', 'ORIGIN_CALL_16644.0', 'ORIGIN_CALL_16683.0', 'ORIGIN_CALL_16865.0', 'ORIGIN_CALL_17343.0', 'ORIGIN_CALL_18020.0', 'ORIGIN_CALL_18439.0', 'ORIGIN_CALL_18561.0', 'ORIGIN_CALL_18599.0', 'ORIGIN_CALL_18820.0', 'ORIGIN_CALL_19141.0', 'ORIGIN_CALL_19150.0', 'ORIGIN_CALL_19344.0', 'ORIGIN_CALL_22438.0', 'ORIGIN_CALL_23692.0', 'ORIGIN_CALL_23874.0', 'ORIGIN_CALL_23998.0', 'ORIGIN_CALL_24199.0', 'ORIGIN_CALL_24323.0', 'ORIGIN_CALL_25664.0', 'ORIGIN_CALL_26617.0', 'ORIGIN_CALL_26752.0', 'ORIGIN_CALL_27171.0', 'ORIGIN_CALL_28065.0', 'ORIGIN_CALL_28951.0', 'ORIGIN_CALL_29534.0', 'ORIGIN_CALL_29682.0', 'ORIGIN_CALL_30604.0', 'ORIGIN_CALL_30608.0', 'ORIGIN_CALL_30643.0', 'ORIGIN_CALL_30726.0', 'ORIGIN_CALL_33535.0', 'ORIGIN_CALL_34525.0', 'ORIGIN_CALL_34861.0', 'ORIGIN_CALL_35601.0', 'ORIGIN_CALL_36195.0', 'ORIGIN_CALL_36601.0', 'ORIGIN_CALL_36773.0', 'ORIGIN_CALL_37007.0', 'ORIGIN_CALL_37083.0', 'ORIGIN_CALL_38347.0', 'ORIGIN_CALL_39530.0', 'ORIGIN_CALL_40352.0', 'ORIGIN_CALL_40433.0', 'ORIGIN_CALL_40886.0', 'ORIGIN_CALL_41277.0', 'ORIGIN_CALL_43024.0', 'ORIGIN_CALL_43288.0', 'ORIGIN_CALL_43421.0', 'ORIGIN_CALL_44210.0', 'ORIGIN_CALL_47219.0', 'ORIGIN_CALL_47504.0', 'ORIGIN_CALL_52543.0', 'ORIGIN_CALL_55014.0', 'ORIGIN_CALL_55325.0', 'ORIGIN_CALL_55387.0', 'ORIGIN_CALL_56289.0', 'ORIGIN_CALL_56610.0', 'ORIGIN_CALL_56888.0', 'ORIGIN_CALL_56903.0', 'ORIGIN_CALL_57075.0', 'ORIGIN_CALL_58637.0', 'ORIGIN_CALL_59108.0', 'ORIGIN_CALL_60441.0', 'ORIGIN_CALL_60911.0', 'ORIGIN_CALL_61064.0', 'ORIGIN_CALL_63882.0', 'YRDAY_1', 'YRDAY_2', 'YRDAY_3', 'YRDAY_4', 'YRDAY_5', 'YRDAY_6', 'YRDAY_7', 'YRDAY_8', 'YRDAY_9', 'YRDAY_10', 'YRDAY_11', 'YRDAY_12', 'YRDAY_13', 'YRDAY_14', 'YRDAY_15', 'YRDAY_16', 'YRDAY_17', 'YRDAY_18', 'YRDAY_19', 'YRDAY_20', 'YRDAY_21', 'YRDAY_22', 'YRDAY_23', 'YRDAY_24', 'YRDAY_25', 'YRDAY_26', 'YRDAY_27', 'YRDAY_28', 'YRDAY_29', 'YRDAY_30', 'YRDAY_31', 'YRDAY_32', 'YRDAY_33', 'YRDAY_34', 'YRDAY_35', 'YRDAY_36', 'YRDAY_37', 'YRDAY_38', 'YRDAY_39', 'YRDAY_40', 'YRDAY_41', 'YRDAY_42', 'YRDAY_43', 'YRDAY_44', 'YRDAY_45', 'YRDAY_46', 'YRDAY_47', 'YRDAY_48', 'YRDAY_49', 'YRDAY_50', 'YRDAY_51', 'YRDAY_52', 'YRDAY_53', 'YRDAY_54', 'YRDAY_55', 'YRDAY_56', 'YRDAY_57', 'YRDAY_58', 'YRDAY_59', 'YRDAY_60', 'YRDAY_61', 'YRDAY_62', 'YRDAY_63', 'YRDAY_64', 'YRDAY_65', 'YRDAY_66', 'YRDAY_67', 'YRDAY_68', 'YRDAY_69', 'YRDAY_70', 'YRDAY_71', 'YRDAY_72', 'YRDAY_73', 'YRDAY_74', 'YRDAY_75', 'YRDAY_76', 'YRDAY_77', 'YRDAY_78', 'YRDAY_79', 'YRDAY_80', 'YRDAY_81', 'YRDAY_82', 'YRDAY_83', 'YRDAY_84', 'YRDAY_85', 'YRDAY_86', 'YRDAY_87', 'YRDAY_88', 'YRDAY_89', 'YRDAY_90', 'YRDAY_91', 'YRDAY_92', 'YRDAY_93', 'YRDAY_94', 'YRDAY_95', 'YRDAY_96', 'YRDAY_97', 'YRDAY_98', 'YRDAY_99', 'YRDAY_100', 'YRDAY_101', 'YRDAY_102', 'YRDAY_103', 'YRDAY_104', 'YRDAY_105', 'YRDAY_106', 'YRDAY_107', 'YRDAY_108', 'YRDAY_109', 'YRDAY_110', 'YRDAY_111', 'YRDAY_112', 'YRDAY_113', 'YRDAY_114', 'YRDAY_115', 'YRDAY_116', 'YRDAY_117', 'YRDAY_118', 'YRDAY_119', 'YRDAY_120', 'YRDAY_121', 'YRDAY_122', 'YRDAY_123', 'YRDAY_124', 'YRDAY_125', 'YRDAY_126', 'YRDAY_127', 'YRDAY_128', 'YRDAY_129', 'YRDAY_130', 'YRDAY_131', 'YRDAY_132', 'YRDAY_133', 'YRDAY_134', 'YRDAY_135', 'YRDAY_136', 'YRDAY_137', 'YRDAY_138', 'YRDAY_139', 'YRDAY_140', 'YRDAY_141', 'YRDAY_142', 'YRDAY_143', 'YRDAY_144', 'YRDAY_145', 'YRDAY_146', 'YRDAY_147', 'YRDAY_148', 'YRDAY_149', 'YRDAY_150', 'YRDAY_151', 'YRDAY_152', 'YRDAY_153', 'YRDAY_154', 'YRDAY_155', 'YRDAY_156', 'YRDAY_157', 'YRDAY_158', 'YRDAY_159', 'YRDAY_160', 'YRDAY_161', 'YRDAY_162', 'YRDAY_163', 'YRDAY_164', 'YRDAY_165', 'YRDAY_166', 'YRDAY_167', 'YRDAY_168', 'YRDAY_169', 'YRDAY_170', 'YRDAY_171', 'YRDAY_172', 'YRDAY_173', 'YRDAY_174', 'YRDAY_175', 'YRDAY_176', 'YRDAY_177', 'YRDAY_178', 'YRDAY_179', 'YRDAY_180', 'YRDAY_181', 'YRDAY_182', 'YRDAY_183', 'YRDAY_184', 'YRDAY_185', 'YRDAY_186', 'YRDAY_187', 'YRDAY_188', 'YRDAY_189', 'YRDAY_190', 'YRDAY_191', 'YRDAY_192', 'YRDAY_193', 'YRDAY_194', 'YRDAY_195', 'YRDAY_196', 'YRDAY_197', 'YRDAY_198', 'YRDAY_199', 'YRDAY_200', 'YRDAY_201', 'YRDAY_202', 'YRDAY_203', 'YRDAY_204', 'YRDAY_205', 'YRDAY_206', 'YRDAY_207', 'YRDAY_208', 'YRDAY_209', 'YRDAY_210', 'YRDAY_211', 'YRDAY_212', 'YRDAY_213', 'YRDAY_214', 'YRDAY_215', 'YRDAY_216', 'YRDAY_217', 'YRDAY_218', 'YRDAY_219', 'YRDAY_220', 'YRDAY_221', 'YRDAY_222', 'YRDAY_223', 'YRDAY_224', 'YRDAY_225', 'YRDAY_226', 'YRDAY_227', 'YRDAY_228', 'YRDAY_229', 'YRDAY_230', 'YRDAY_231', 'YRDAY_232', 'YRDAY_233', 'YRDAY_234', 'YRDAY_235', 'YRDAY_236', 'YRDAY_237', 'YRDAY_238', 'YRDAY_239', 'YRDAY_240', 'YRDAY_241', 'YRDAY_242', 'YRDAY_243', 'YRDAY_244', 'YRDAY_245', 'YRDAY_246', 'YRDAY_247', 'YRDAY_248', 'YRDAY_249', 'YRDAY_250', 'YRDAY_251', 'YRDAY_252', 'YRDAY_253', 'YRDAY_254', 'YRDAY_255', 'YRDAY_256', 'YRDAY_257', 'YRDAY_258', 'YRDAY_259', 'YRDAY_260', 'YRDAY_261', 'YRDAY_262', 'YRDAY_263', 'YRDAY_264', 'YRDAY_265', 'YRDAY_266', 'YRDAY_267', 'YRDAY_268', 'YRDAY_269', 'YRDAY_270', 'YRDAY_271', 'YRDAY_272', 'YRDAY_273', 'YRDAY_274', 'YRDAY_275', 'YRDAY_276', 'YRDAY_277', 'YRDAY_278', 'YRDAY_279', 'YRDAY_280', 'YRDAY_281', 'YRDAY_282', 'YRDAY_283', 'YRDAY_284', 'YRDAY_285', 'YRDAY_286', 'YRDAY_287', 'YRDAY_288', 'YRDAY_289', 'YRDAY_290', 'YRDAY_291', 'YRDAY_292', 'YRDAY_293', 'YRDAY_294', 'YRDAY_295', 'YRDAY_296', 'YRDAY_297', 'YRDAY_298', 'YRDAY_299', 'YRDAY_300', 'YRDAY_301', 'YRDAY_302', 'YRDAY_303', 'YRDAY_304', 'YRDAY_305', 'YRDAY_306', 'YRDAY_307', 'YRDAY_308', 'YRDAY_309', 'YRDAY_310', 'YRDAY_311', 'YRDAY_312', 'YRDAY_313', 'YRDAY_314', 'YRDAY_315', 'YRDAY_316', 'YRDAY_317', 'YRDAY_318', 'YRDAY_319', 'YRDAY_320', 'YRDAY_321', 'YRDAY_322', 'YRDAY_323', 'YRDAY_324', 'YRDAY_325', 'YRDAY_326', 'YRDAY_327', 'YRDAY_328', 'YRDAY_329', 'YRDAY_330', 'YRDAY_331', 'YRDAY_332', 'YRDAY_333', 'YRDAY_334', 'YRDAY_335', 'YRDAY_336', 'YRDAY_337', 'YRDAY_338', 'YRDAY_339', 'YRDAY_340', 'YRDAY_341', 'YRDAY_342', 'YRDAY_343', 'YRDAY_344', 'YRDAY_345', 'YRDAY_346', 'YRDAY_347', 'YRDAY_348', 'YRDAY_349', 'YRDAY_350', 'YRDAY_351', 'YRDAY_352', 'YRDAY_353', 'YRDAY_354', 'YRDAY_355', 'YRDAY_356', 'YRDAY_357', 'YRDAY_358', 'YRDAY_359', 'YRDAY_360', 'YRDAY_361', 'YRDAY_362', 'YRDAY_363', 'YRDAY_364', 'YRDAY_365', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'TAXI_ID_20000004', 'TAXI_ID_20000005', 'TAXI_ID_20000008', 'TAXI_ID_20000009', 'TAXI_ID_20000010', 'TAXI_ID_20000012', 'TAXI_ID_20000015', 'TAXI_ID_20000017', 'TAXI_ID_20000020', 'TAXI_ID_20000021', 'TAXI_ID_20000022', 'TAXI_ID_20000026', 'TAXI_ID_20000036', 'TAXI_ID_20000039', 'TAXI_ID_20000040', 'TAXI_ID_20000041', 'TAXI_ID_20000044', 'TAXI_ID_20000047', 'TAXI_ID_20000048', 'TAXI_ID_20000049', 'TAXI_ID_20000051', 'TAXI_ID_20000053', 'TAXI_ID_20000054', 'TAXI_ID_20000055', 'TAXI_ID_20000057', 'TAXI_ID_20000060', 'TAXI_ID_20000067', 'TAXI_ID_20000071', 'TAXI_ID_20000079', 'TAXI_ID_20000081', 'TAXI_ID_20000085', 'TAXI_ID_20000086', 'TAXI_ID_20000092', 'TAXI_ID_20000099', 'TAXI_ID_20000100', 'TAXI_ID_20000101', 'TAXI_ID_20000105', 'TAXI_ID_20000108', 'TAXI_ID_20000109', 'TAXI_ID_20000112', 'TAXI_ID_20000116', 'TAXI_ID_20000118', 'TAXI_ID_20000121', 'TAXI_ID_20000123', 'TAXI_ID_20000126', 'TAXI_ID_20000128', 'TAXI_ID_20000129', 'TAXI_ID_20000136', 'TAXI_ID_20000140', 'TAXI_ID_20000144', 'TAXI_ID_20000146', 'TAXI_ID_20000148', 'TAXI_ID_20000154', 'TAXI_ID_20000156', 'TAXI_ID_20000158', 'TAXI_ID_20000159', 'TAXI_ID_20000160', 'TAXI_ID_20000163', 'TAXI_ID_20000166', 'TAXI_ID_20000167', 'TAXI_ID_20000171', 'TAXI_ID_20000177', 'TAXI_ID_20000178', 'TAXI_ID_20000180', 'TAXI_ID_20000185', 'TAXI_ID_20000188', 'TAXI_ID_20000190', 'TAXI_ID_20000192', 'TAXI_ID_20000197', 'TAXI_ID_20000198', 'TAXI_ID_20000199', 'TAXI_ID_20000206', 'TAXI_ID_20000207', 'TAXI_ID_20000213', 'TAXI_ID_20000222', 'TAXI_ID_20000224', 'TAXI_ID_20000230', 'TAXI_ID_20000235', 'TAXI_ID_20000239', 'TAXI_ID_20000243', 'TAXI_ID_20000245', 'TAXI_ID_20000247', 'TAXI_ID_20000248', 'TAXI_ID_20000249', 'TAXI_ID_20000250', 'TAXI_ID_20000252', 'TAXI_ID_20000255', 'TAXI_ID_20000256', 'TAXI_ID_20000260', 'TAXI_ID_20000261', 'TAXI_ID_20000263', 'TAXI_ID_20000268', 'TAXI_ID_20000272', 'TAXI_ID_20000276', 'TAXI_ID_20000280', 'TAXI_ID_20000281', 'TAXI_ID_20000285', 'TAXI_ID_20000286', 'TAXI_ID_20000288', 'TAXI_ID_20000294', 'TAXI_ID_20000295', 'TAXI_ID_20000296', 'TAXI_ID_20000303', 'TAXI_ID_20000304', 'TAXI_ID_20000307', 'TAXI_ID_20000310', 'TAXI_ID_20000311', 'TAXI_ID_20000312', 'TAXI_ID_20000314', 'TAXI_ID_20000320', 'TAXI_ID_20000325', 'TAXI_ID_20000327', 'TAXI_ID_20000328', 'TAXI_ID_20000331', 'TAXI_ID_20000333', 'TAXI_ID_20000334', 'TAXI_ID_20000338', 'TAXI_ID_20000340', 'TAXI_ID_20000342', 'TAXI_ID_20000345', 'TAXI_ID_20000347', 'TAXI_ID_20000349', 'TAXI_ID_20000351', 'TAXI_ID_20000352', 'TAXI_ID_20000353', 'TAXI_ID_20000356', 'TAXI_ID_20000361', 'TAXI_ID_20000362', 'TAXI_ID_20000363', 'TAXI_ID_20000370', 'TAXI_ID_20000372', 'TAXI_ID_20000377', 'TAXI_ID_20000381', 'TAXI_ID_20000383', 'TAXI_ID_20000384', 'TAXI_ID_20000387', 'TAXI_ID_20000391', 'TAXI_ID_20000393', 'TAXI_ID_20000395', 'TAXI_ID_20000400', 'TAXI_ID_20000403', 'TAXI_ID_20000406', 'TAXI_ID_20000407', 'TAXI_ID_20000410', 'TAXI_ID_20000421', 'TAXI_ID_20000424', 'TAXI_ID_20000426', 'TAXI_ID_20000429', 'TAXI_ID_20000430', 'TAXI_ID_20000431', 'TAXI_ID_20000434', 'TAXI_ID_20000436', 'TAXI_ID_20000440', 'TAXI_ID_20000446', 'TAXI_ID_20000450', 'TAXI_ID_20000452', 'TAXI_ID_20000453', 'TAXI_ID_20000454', 'TAXI_ID_20000455', 'TAXI_ID_20000456', 'TAXI_ID_20000460', 'TAXI_ID_20000463', 'TAXI_ID_20000464', 'TAXI_ID_20000467', 'TAXI_ID_20000473', 'TAXI_ID_20000476', 'TAXI_ID_20000477', 'TAXI_ID_20000480', 'TAXI_ID_20000483', 'TAXI_ID_20000486', 'TAXI_ID_20000488', 'TAXI_ID_20000492', 'TAXI_ID_20000494', 'TAXI_ID_20000495', 'TAXI_ID_20000496', 'TAXI_ID_20000497', 'TAXI_ID_20000499', 'TAXI_ID_20000500', 'TAXI_ID_20000502', 'TAXI_ID_20000503', 'TAXI_ID_20000510', 'TAXI_ID_20000513', 'TAXI_ID_20000517', 'TAXI_ID_20000518', 'TAXI_ID_20000523', 'TAXI_ID_20000525', 'TAXI_ID_20000529', 'TAXI_ID_20000539', 'TAXI_ID_20000540', 'TAXI_ID_20000541', 'TAXI_ID_20000542', 'TAXI_ID_20000543', 'TAXI_ID_20000546', 'TAXI_ID_20000547', 'TAXI_ID_20000548', 'TAXI_ID_20000549', 'TAXI_ID_20000554', 'TAXI_ID_20000560', 'TAXI_ID_20000561', 'TAXI_ID_20000562', 'TAXI_ID_20000565', 'TAXI_ID_20000569', 'TAXI_ID_20000572', 'TAXI_ID_20000574', 'TAXI_ID_20000576', 'TAXI_ID_20000577', 'TAXI_ID_20000578', 'TAXI_ID_20000589', 'TAXI_ID_20000591', 'TAXI_ID_20000595', 'TAXI_ID_20000596', 'TAXI_ID_20000597', 'TAXI_ID_20000603', 'TAXI_ID_20000607', 'TAXI_ID_20000612', 'TAXI_ID_20000617', 'TAXI_ID_20000619', 'TAXI_ID_20000621', 'TAXI_ID_20000624', 'TAXI_ID_20000625', 'TAXI_ID_20000626', 'TAXI_ID_20000632', 'TAXI_ID_20000633', 'TAXI_ID_20000649', 'TAXI_ID_20000653', 'TAXI_ID_20000657', 'TAXI_ID_20000662', 'TAXI_ID_20000664', 'TAXI_ID_20000665', 'TAXI_ID_20000667', 'TAXI_ID_20000668', 'TAXI_ID_20000675', 'TAXI_ID_20000678', 'TAXI_ID_20000682', 'TAXI_ID_20000685', 'TAXI_ID_20000686', 'TAXI_ID_20000687', 'TAXI_ID_20000688', 'TAXI_ID_20000693', 'TAXI_ID_20000698', 'TAXI_ID_20000900', 'TAXI_ID_20000901', 'TAXI_ID_20000903', 'TAXI_ID_20000904']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "trds=MyDataset(df_tr)\n",
    "trds, valds = random_split(trds, [0.95, 0.05])\n",
    "print(len(trds), len(valds))\n",
    "train_loader=DataLoader(trds,batch_size=64, shuffle=True)\n",
    "val_loader=DataLoader(valds)\n",
    "binds = MyDataset(df_bin, bin=True)\n",
    "bin_loader = DataLoader(binds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dro = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(870, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dro(x)\n",
    "        x = self.fc4(x)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.dro = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(870, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dro(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "binarynet = BinaryNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "bincriterion = nn.BCELoss()\n",
    "binoptimizer = optim.Adam(binarynet.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.003\n",
      "tensor([0.5324], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5269], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5281], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5246], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5187], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5254], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5217], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5246], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5185], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5205], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5249], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5257], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5291], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5305], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5248], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5297], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5387], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5316], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5235], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5272], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4993], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5029], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4971], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5237], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5314], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5443], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5793], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.5415], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4873], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4524], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.6240], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4102], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4392], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.4194], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.003\n",
      "tensor([0.6341], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.7874], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.3827], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.7619], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.6281], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.3980], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.8858], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.2705], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.7307], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.4930], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.7662], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.8306], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.002\n",
      "tensor([0.7647], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9041], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.1656], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.6684], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.2697], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.8814], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0799], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.1256], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.2168], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0828], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9263], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.8543], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.1798], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0949], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9093], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.2328], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9703], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9348], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0559], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9929], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0338], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9537], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.9688], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.001\n",
      "tensor([0.0456], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.1726], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0518], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.8794], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9917], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0092], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0237], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0248], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9493], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0386], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9961], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9530], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9736], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.3709], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9926], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.2606], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9973], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9969], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9746], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0010], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9807], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9948], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.8974], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9981], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9974], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0278], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9994], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0080], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0053], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9917], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0049], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9980], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.1034], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0059], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.8771], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9955], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9922], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0010], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0226], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0041], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([8.3384e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9934], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9835], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0441], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9863], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9563], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9995], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9869], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0064], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9803], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9942], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0045], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0027], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9976], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9991], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([6.9999e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9862], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9997], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9960], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9697], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0001], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9894], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([3.7505e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9841], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9911], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([2.0901e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9992], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9902], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9902], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9998], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9999], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9998], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9987], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9996], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0015], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([2.8522e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9996], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([6.4429e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0001], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9994], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([3.2098e-06], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0057], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([1.8358e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([1.0144e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9992], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9995], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9962], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([1.0737e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9993], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9963], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9999], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([9.1718e-06], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([3.4561e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9979], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([1.1049e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9929], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([7.5335e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9822], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9871], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([1.6219e-05], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9996], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9998], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([3.7132e-06], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9997], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9998], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9779], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9989], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.0057], grad_fn=<SelectBackward0>) tensor([0.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9807], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "[1] loss: 0.000\n",
      "tensor([0.9913], grad_fn=<SelectBackward0>) tensor([1.])\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "binarynet.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(bin_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        binoptimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = binarynet(inputs)\n",
    "        \n",
    "        loss = bincriterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        binoptimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            #print(outputs[0], labels[0])\n",
    "        if i%100==99:\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(bin_loader):.3f}')\n",
    "            print(outputs[0], labels[0])\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "def badidea(yhat,y,bvalue):\n",
    "    return torch.mean(bvalue * torch.abs((yhat-y)))\n",
    "criterion = RMSELoss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 797.555\n",
      "tensor([0.0874], grad_fn=<SelectBackward0>) tensor([1080.])\n",
      "[1] loss: 799.477\n",
      "tensor([0.9920], grad_fn=<SelectBackward0>) tensor([195.])\n",
      "[1] loss: 791.839\n",
      "tensor([6.1234], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1] loss: 792.045\n",
      "tensor([10.6732], grad_fn=<SelectBackward0>) tensor([720.])\n",
      "[1] loss: 777.898\n",
      "tensor([31.3993], grad_fn=<SelectBackward0>) tensor([420.])\n",
      "[1] loss: 763.346\n",
      "tensor([46.9115], grad_fn=<SelectBackward0>) tensor([495.])\n",
      "[1] loss: 733.646\n",
      "tensor([97.7334], grad_fn=<SelectBackward0>) tensor([615.])\n",
      "[1] loss: 704.891\n",
      "tensor([162.2191], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1] loss: 658.409\n",
      "tensor([287.0467], grad_fn=<SelectBackward0>) tensor([345.])\n",
      "[1] loss: 608.945\n",
      "tensor([301.2354], grad_fn=<SelectBackward0>) tensor([825.])\n",
      "[1] loss: 546.277\n",
      "tensor([423.2489], grad_fn=<SelectBackward0>) tensor([825.])\n",
      "[1] loss: 494.085\n",
      "tensor([449.7378], grad_fn=<SelectBackward0>) tensor([240.])\n",
      "[1] loss: 455.664\n",
      "tensor([419.0476], grad_fn=<SelectBackward0>) tensor([525.])\n",
      "[1] loss: 439.462\n",
      "tensor([610.6821], grad_fn=<SelectBackward0>) tensor([495.])\n",
      "[1] loss: 432.137\n",
      "tensor([784.9243], grad_fn=<SelectBackward0>) tensor([810.])\n",
      "[1] loss: 430.304\n",
      "tensor([757.0829], grad_fn=<SelectBackward0>) tensor([1095.])\n",
      "[1] loss: 429.091\n",
      "tensor([697.5330], grad_fn=<SelectBackward0>) tensor([765.])\n",
      "[1] loss: 426.249\n",
      "tensor([596.1854], grad_fn=<SelectBackward0>) tensor([645.])\n",
      "[1] loss: 429.087\n",
      "tensor([532.6520], grad_fn=<SelectBackward0>) tensor([300.])\n",
      "[2] loss: 423.269\n",
      "tensor([760.6393], grad_fn=<SelectBackward0>) tensor([675.])\n",
      "[2] loss: 427.745\n",
      "tensor([749.1095], grad_fn=<SelectBackward0>) tensor([1230.])\n",
      "[2] loss: 423.983\n",
      "tensor([651.1635], grad_fn=<SelectBackward0>) tensor([630.])\n",
      "[2] loss: 423.919\n",
      "tensor([568.0037], grad_fn=<SelectBackward0>) tensor([1005.])\n",
      "[2] loss: 424.650\n",
      "tensor([798.0636], grad_fn=<SelectBackward0>) tensor([705.])\n",
      "[2] loss: 420.381\n",
      "tensor([1108.1558], grad_fn=<SelectBackward0>) tensor([510.])\n",
      "[2] loss: 424.524\n",
      "tensor([1011.2255], grad_fn=<SelectBackward0>) tensor([510.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 26\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     28\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n\u001b[0;32m     29\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\moond\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "net.train()\n",
    "losses = []\n",
    "times = []\n",
    "start = time.time()\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels.unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # binoutputs = binarynet(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            #print(outputs[0], labels[0])\n",
    "        if i%1000==999:\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / 1000:.3f}')\n",
    "            print(outputs[0], labels[0])\n",
    "            losses.append(running_loss / 1000)\n",
    "            times.append(time.time()-start)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401.2102930545807 424.5237334594727\n"
     ]
    }
   ],
   "source": [
    "print(times[-1], losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(times, losses)\n",
    "plt.xlabel(\"Time in seconds\")\n",
    "plt.ylabel(\"RMSE loss\")\n",
    "plt.title(\"MLP loss vs time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dro): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=870, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 290.280\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        labels=labels\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "        # if i%1000==0:\n",
    "        #     print(round(loss.item()), outputs, labels)\n",
    "\n",
    "print(f'Total Loss: {total_loss / len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'ORIGIN_STAND_1.0', 'ORIGIN_STAND_2.0', 'ORIGIN_STAND_3.0', 'ORIGIN_STAND_4.0', 'ORIGIN_STAND_5.0', 'ORIGIN_STAND_6.0', 'ORIGIN_STAND_7.0', 'ORIGIN_STAND_8.0', 'ORIGIN_STAND_9.0', 'ORIGIN_STAND_10.0', 'ORIGIN_STAND_11.0', 'ORIGIN_STAND_12.0', 'ORIGIN_STAND_13.0', 'ORIGIN_STAND_14.0', 'ORIGIN_STAND_15.0', 'ORIGIN_STAND_16.0', 'ORIGIN_STAND_17.0', 'ORIGIN_STAND_18.0', 'ORIGIN_STAND_19.0', 'ORIGIN_STAND_20.0', 'ORIGIN_STAND_21.0', 'ORIGIN_STAND_22.0', 'ORIGIN_STAND_23.0', 'ORIGIN_STAND_24.0', 'ORIGIN_STAND_25.0', 'ORIGIN_STAND_26.0', 'ORIGIN_STAND_27.0', 'ORIGIN_STAND_28.0', 'ORIGIN_STAND_29.0', 'ORIGIN_STAND_30.0', 'ORIGIN_STAND_31.0', 'ORIGIN_STAND_32.0', 'ORIGIN_STAND_33.0', 'ORIGIN_STAND_34.0', 'ORIGIN_STAND_35.0', 'ORIGIN_STAND_36.0', 'ORIGIN_STAND_37.0', 'ORIGIN_STAND_38.0', 'ORIGIN_STAND_39.0', 'ORIGIN_STAND_40.0', 'ORIGIN_STAND_41.0', 'ORIGIN_STAND_42.0', 'ORIGIN_STAND_43.0', 'ORIGIN_STAND_44.0', 'ORIGIN_STAND_45.0', 'ORIGIN_STAND_46.0', 'ORIGIN_STAND_47.0', 'ORIGIN_STAND_48.0', 'ORIGIN_STAND_49.0', 'ORIGIN_STAND_50.0', 'ORIGIN_STAND_51.0', 'ORIGIN_STAND_52.0', 'ORIGIN_STAND_53.0', 'ORIGIN_STAND_54.0', 'ORIGIN_STAND_55.0', 'ORIGIN_STAND_56.0', 'ORIGIN_STAND_57.0', 'ORIGIN_STAND_58.0', 'ORIGIN_STAND_59.0', 'ORIGIN_STAND_60.0', 'ORIGIN_STAND_61.0', 'ORIGIN_STAND_62.0', 'ORIGIN_STAND_63.0', 'ORIGIN_CALL_2001.0', 'ORIGIN_CALL_2002.0', 'ORIGIN_CALL_2024.0', 'ORIGIN_CALL_2747.0', 'ORIGIN_CALL_2801.0', 'ORIGIN_CALL_2974.0', 'ORIGIN_CALL_3710.0', 'ORIGIN_CALL_3711.0', 'ORIGIN_CALL_3712.0', 'ORIGIN_CALL_3731.0', 'ORIGIN_CALL_3734.0', 'ORIGIN_CALL_4196.0', 'ORIGIN_CALL_4206.0', 'ORIGIN_CALL_4293.0', 'ORIGIN_CALL_4609.0', 'ORIGIN_CALL_4618.0', 'ORIGIN_CALL_4619.0', 'ORIGIN_CALL_4646.0', 'ORIGIN_CALL_4691.0', 'ORIGIN_CALL_4784.0', 'ORIGIN_CALL_4792.0', 'ORIGIN_CALL_4819.0', 'ORIGIN_CALL_5081.0', 'ORIGIN_CALL_5182.0', 'ORIGIN_CALL_5239.0', 'ORIGIN_CALL_5347.0', 'ORIGIN_CALL_5591.0', 'ORIGIN_CALL_5602.0', 'ORIGIN_CALL_5779.0', 'ORIGIN_CALL_5926.0', 'ORIGIN_CALL_6084.0', 'ORIGIN_CALL_6089.0', 'ORIGIN_CALL_6584.0', 'ORIGIN_CALL_6654.0', 'ORIGIN_CALL_6659.0', 'ORIGIN_CALL_6675.0', 'ORIGIN_CALL_6714.0', 'ORIGIN_CALL_6715.0', 'ORIGIN_CALL_6728.0', 'ORIGIN_CALL_6896.0', 'ORIGIN_CALL_7817.0', 'ORIGIN_CALL_8246.0', 'ORIGIN_CALL_8847.0', 'ORIGIN_CALL_8939.0', 'ORIGIN_CALL_8950.0', 'ORIGIN_CALL_9139.0', 'ORIGIN_CALL_9151.0', 'ORIGIN_CALL_9327.0', 'ORIGIN_CALL_9393.0', 'ORIGIN_CALL_9676.0', 'ORIGIN_CALL_9682.0', 'ORIGIN_CALL_9705.0', 'ORIGIN_CALL_10456.0', 'ORIGIN_CALL_10591.0', 'ORIGIN_CALL_11066.0', 'ORIGIN_CALL_11330.0', 'ORIGIN_CALL_11499.0', 'ORIGIN_CALL_11763.0', 'ORIGIN_CALL_11868.0', 'ORIGIN_CALL_12161.0', 'ORIGIN_CALL_12190.0', 'ORIGIN_CALL_12394.0', 'ORIGIN_CALL_12482.0', 'ORIGIN_CALL_12536.0', 'ORIGIN_CALL_12564.0', 'ORIGIN_CALL_12616.0', 'ORIGIN_CALL_12692.0', 'ORIGIN_CALL_12728.0', 'ORIGIN_CALL_12773.0', 'ORIGIN_CALL_12897.0', 'ORIGIN_CALL_13009.0', 'ORIGIN_CALL_13168.0', 'ORIGIN_CALL_13226.0', 'ORIGIN_CALL_13287.0', 'ORIGIN_CALL_13360.0', 'ORIGIN_CALL_14045.0', 'ORIGIN_CALL_14083.0', 'ORIGIN_CALL_14084.0', 'ORIGIN_CALL_14085.0', 'ORIGIN_CALL_14104.0', 'ORIGIN_CALL_14118.0', 'ORIGIN_CALL_14123.0', 'ORIGIN_CALL_14133.0', 'ORIGIN_CALL_14134.0', 'ORIGIN_CALL_14144.0', 'ORIGIN_CALL_14182.0', 'ORIGIN_CALL_14853.0', 'ORIGIN_CALL_14918.0', 'ORIGIN_CALL_15086.0', 'ORIGIN_CALL_15097.0', 'ORIGIN_CALL_15242.0', 'ORIGIN_CALL_15339.0', 'ORIGIN_CALL_15356.0', 'ORIGIN_CALL_15420.0', 'ORIGIN_CALL_15427.0', 'ORIGIN_CALL_15608.0', 'ORIGIN_CALL_15617.0', 'ORIGIN_CALL_15689.0', 'ORIGIN_CALL_15756.0', 'ORIGIN_CALL_15769.0', 'ORIGIN_CALL_15792.0', 'ORIGIN_CALL_16090.0', 'ORIGIN_CALL_16096.0', 'ORIGIN_CALL_16414.0', 'ORIGIN_CALL_16644.0', 'ORIGIN_CALL_16683.0', 'ORIGIN_CALL_16865.0', 'ORIGIN_CALL_17343.0', 'ORIGIN_CALL_18020.0', 'ORIGIN_CALL_18439.0', 'ORIGIN_CALL_18561.0', 'ORIGIN_CALL_18599.0', 'ORIGIN_CALL_18820.0', 'ORIGIN_CALL_19141.0', 'ORIGIN_CALL_19150.0', 'ORIGIN_CALL_19344.0', 'ORIGIN_CALL_22438.0', 'ORIGIN_CALL_23692.0', 'ORIGIN_CALL_23874.0', 'ORIGIN_CALL_23998.0', 'ORIGIN_CALL_24199.0', 'ORIGIN_CALL_24323.0', 'ORIGIN_CALL_25664.0', 'ORIGIN_CALL_26617.0', 'ORIGIN_CALL_26752.0', 'ORIGIN_CALL_27171.0', 'ORIGIN_CALL_28065.0', 'ORIGIN_CALL_28951.0', 'ORIGIN_CALL_29534.0', 'ORIGIN_CALL_29682.0', 'ORIGIN_CALL_30604.0', 'ORIGIN_CALL_30608.0', 'ORIGIN_CALL_30643.0', 'ORIGIN_CALL_30726.0', 'ORIGIN_CALL_33535.0', 'ORIGIN_CALL_34525.0', 'ORIGIN_CALL_34861.0', 'ORIGIN_CALL_35601.0', 'ORIGIN_CALL_36195.0', 'ORIGIN_CALL_36601.0', 'ORIGIN_CALL_36773.0', 'ORIGIN_CALL_37007.0', 'ORIGIN_CALL_37083.0', 'ORIGIN_CALL_38347.0', 'ORIGIN_CALL_39530.0', 'ORIGIN_CALL_40352.0', 'ORIGIN_CALL_40433.0', 'ORIGIN_CALL_40886.0', 'ORIGIN_CALL_41277.0', 'ORIGIN_CALL_43024.0', 'ORIGIN_CALL_43288.0', 'ORIGIN_CALL_43421.0', 'ORIGIN_CALL_44210.0', 'ORIGIN_CALL_47219.0', 'ORIGIN_CALL_47504.0', 'ORIGIN_CALL_52543.0', 'ORIGIN_CALL_55014.0', 'ORIGIN_CALL_55325.0', 'ORIGIN_CALL_55387.0', 'ORIGIN_CALL_56289.0', 'ORIGIN_CALL_56610.0', 'ORIGIN_CALL_56888.0', 'ORIGIN_CALL_56903.0', 'ORIGIN_CALL_57075.0', 'ORIGIN_CALL_58637.0', 'ORIGIN_CALL_59108.0', 'ORIGIN_CALL_60441.0', 'ORIGIN_CALL_60911.0', 'ORIGIN_CALL_61064.0', 'ORIGIN_CALL_63882.0', 'YRDAY_1', 'YRDAY_2', 'YRDAY_3', 'YRDAY_4', 'YRDAY_5', 'YRDAY_6', 'YRDAY_7', 'YRDAY_8', 'YRDAY_9', 'YRDAY_10', 'YRDAY_11', 'YRDAY_12', 'YRDAY_13', 'YRDAY_14', 'YRDAY_15', 'YRDAY_16', 'YRDAY_17', 'YRDAY_18', 'YRDAY_19', 'YRDAY_20', 'YRDAY_21', 'YRDAY_22', 'YRDAY_23', 'YRDAY_24', 'YRDAY_25', 'YRDAY_26', 'YRDAY_27', 'YRDAY_28', 'YRDAY_29', 'YRDAY_30', 'YRDAY_31', 'YRDAY_32', 'YRDAY_33', 'YRDAY_34', 'YRDAY_35', 'YRDAY_36', 'YRDAY_37', 'YRDAY_38', 'YRDAY_39', 'YRDAY_40', 'YRDAY_41', 'YRDAY_42', 'YRDAY_43', 'YRDAY_44', 'YRDAY_45', 'YRDAY_46', 'YRDAY_47', 'YRDAY_48', 'YRDAY_49', 'YRDAY_50', 'YRDAY_51', 'YRDAY_52', 'YRDAY_53', 'YRDAY_54', 'YRDAY_55', 'YRDAY_56', 'YRDAY_57', 'YRDAY_58', 'YRDAY_59', 'YRDAY_60', 'YRDAY_61', 'YRDAY_62', 'YRDAY_63', 'YRDAY_64', 'YRDAY_65', 'YRDAY_66', 'YRDAY_67', 'YRDAY_68', 'YRDAY_69', 'YRDAY_70', 'YRDAY_71', 'YRDAY_72', 'YRDAY_73', 'YRDAY_74', 'YRDAY_75', 'YRDAY_76', 'YRDAY_77', 'YRDAY_78', 'YRDAY_79', 'YRDAY_80', 'YRDAY_81', 'YRDAY_82', 'YRDAY_83', 'YRDAY_84', 'YRDAY_85', 'YRDAY_86', 'YRDAY_87', 'YRDAY_88', 'YRDAY_89', 'YRDAY_90', 'YRDAY_91', 'YRDAY_92', 'YRDAY_93', 'YRDAY_94', 'YRDAY_95', 'YRDAY_96', 'YRDAY_97', 'YRDAY_98', 'YRDAY_99', 'YRDAY_100', 'YRDAY_101', 'YRDAY_102', 'YRDAY_103', 'YRDAY_104', 'YRDAY_105', 'YRDAY_106', 'YRDAY_107', 'YRDAY_108', 'YRDAY_109', 'YRDAY_110', 'YRDAY_111', 'YRDAY_112', 'YRDAY_113', 'YRDAY_114', 'YRDAY_115', 'YRDAY_116', 'YRDAY_117', 'YRDAY_118', 'YRDAY_119', 'YRDAY_120', 'YRDAY_121', 'YRDAY_122', 'YRDAY_123', 'YRDAY_124', 'YRDAY_125', 'YRDAY_126', 'YRDAY_127', 'YRDAY_128', 'YRDAY_129', 'YRDAY_130', 'YRDAY_131', 'YRDAY_132', 'YRDAY_133', 'YRDAY_134', 'YRDAY_135', 'YRDAY_136', 'YRDAY_137', 'YRDAY_138', 'YRDAY_139', 'YRDAY_140', 'YRDAY_141', 'YRDAY_142', 'YRDAY_143', 'YRDAY_144', 'YRDAY_145', 'YRDAY_146', 'YRDAY_147', 'YRDAY_148', 'YRDAY_149', 'YRDAY_150', 'YRDAY_151', 'YRDAY_152', 'YRDAY_153', 'YRDAY_154', 'YRDAY_155', 'YRDAY_156', 'YRDAY_157', 'YRDAY_158', 'YRDAY_159', 'YRDAY_160', 'YRDAY_161', 'YRDAY_162', 'YRDAY_163', 'YRDAY_164', 'YRDAY_165', 'YRDAY_166', 'YRDAY_167', 'YRDAY_168', 'YRDAY_169', 'YRDAY_170', 'YRDAY_171', 'YRDAY_172', 'YRDAY_173', 'YRDAY_174', 'YRDAY_175', 'YRDAY_176', 'YRDAY_177', 'YRDAY_178', 'YRDAY_179', 'YRDAY_180', 'YRDAY_181', 'YRDAY_182', 'YRDAY_183', 'YRDAY_184', 'YRDAY_185', 'YRDAY_186', 'YRDAY_187', 'YRDAY_188', 'YRDAY_189', 'YRDAY_190', 'YRDAY_191', 'YRDAY_192', 'YRDAY_193', 'YRDAY_194', 'YRDAY_195', 'YRDAY_196', 'YRDAY_197', 'YRDAY_198', 'YRDAY_199', 'YRDAY_200', 'YRDAY_201', 'YRDAY_202', 'YRDAY_203', 'YRDAY_204', 'YRDAY_205', 'YRDAY_206', 'YRDAY_207', 'YRDAY_208', 'YRDAY_209', 'YRDAY_210', 'YRDAY_211', 'YRDAY_212', 'YRDAY_213', 'YRDAY_214', 'YRDAY_215', 'YRDAY_216', 'YRDAY_217', 'YRDAY_218', 'YRDAY_219', 'YRDAY_220', 'YRDAY_221', 'YRDAY_222', 'YRDAY_223', 'YRDAY_224', 'YRDAY_225', 'YRDAY_226', 'YRDAY_227', 'YRDAY_228', 'YRDAY_229', 'YRDAY_230', 'YRDAY_231', 'YRDAY_232', 'YRDAY_233', 'YRDAY_234', 'YRDAY_235', 'YRDAY_236', 'YRDAY_237', 'YRDAY_238', 'YRDAY_239', 'YRDAY_240', 'YRDAY_241', 'YRDAY_242', 'YRDAY_243', 'YRDAY_244', 'YRDAY_245', 'YRDAY_246', 'YRDAY_247', 'YRDAY_248', 'YRDAY_249', 'YRDAY_250', 'YRDAY_251', 'YRDAY_252', 'YRDAY_253', 'YRDAY_254', 'YRDAY_255', 'YRDAY_256', 'YRDAY_257', 'YRDAY_258', 'YRDAY_259', 'YRDAY_260', 'YRDAY_261', 'YRDAY_262', 'YRDAY_263', 'YRDAY_264', 'YRDAY_265', 'YRDAY_266', 'YRDAY_267', 'YRDAY_268', 'YRDAY_269', 'YRDAY_270', 'YRDAY_271', 'YRDAY_272', 'YRDAY_273', 'YRDAY_274', 'YRDAY_275', 'YRDAY_276', 'YRDAY_277', 'YRDAY_278', 'YRDAY_279', 'YRDAY_280', 'YRDAY_281', 'YRDAY_282', 'YRDAY_283', 'YRDAY_284', 'YRDAY_285', 'YRDAY_286', 'YRDAY_287', 'YRDAY_288', 'YRDAY_289', 'YRDAY_290', 'YRDAY_291', 'YRDAY_292', 'YRDAY_293', 'YRDAY_294', 'YRDAY_295', 'YRDAY_296', 'YRDAY_297', 'YRDAY_298', 'YRDAY_299', 'YRDAY_300', 'YRDAY_301', 'YRDAY_302', 'YRDAY_303', 'YRDAY_304', 'YRDAY_305', 'YRDAY_306', 'YRDAY_307', 'YRDAY_308', 'YRDAY_309', 'YRDAY_310', 'YRDAY_311', 'YRDAY_312', 'YRDAY_313', 'YRDAY_314', 'YRDAY_315', 'YRDAY_316', 'YRDAY_317', 'YRDAY_318', 'YRDAY_319', 'YRDAY_320', 'YRDAY_321', 'YRDAY_322', 'YRDAY_323', 'YRDAY_324', 'YRDAY_325', 'YRDAY_326', 'YRDAY_327', 'YRDAY_328', 'YRDAY_329', 'YRDAY_330', 'YRDAY_331', 'YRDAY_332', 'YRDAY_333', 'YRDAY_334', 'YRDAY_335', 'YRDAY_336', 'YRDAY_337', 'YRDAY_338', 'YRDAY_339', 'YRDAY_340', 'YRDAY_341', 'YRDAY_342', 'YRDAY_343', 'YRDAY_344', 'YRDAY_345', 'YRDAY_346', 'YRDAY_347', 'YRDAY_348', 'YRDAY_349', 'YRDAY_350', 'YRDAY_351', 'YRDAY_352', 'YRDAY_353', 'YRDAY_354', 'YRDAY_355', 'YRDAY_356', 'YRDAY_357', 'YRDAY_358', 'YRDAY_359', 'YRDAY_360', 'YRDAY_361', 'YRDAY_362', 'YRDAY_363', 'YRDAY_364', 'YRDAY_365', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'TAXI_ID_20000004', 'TAXI_ID_20000005', 'TAXI_ID_20000008', 'TAXI_ID_20000009', 'TAXI_ID_20000010', 'TAXI_ID_20000012', 'TAXI_ID_20000015', 'TAXI_ID_20000017', 'TAXI_ID_20000020', 'TAXI_ID_20000021', 'TAXI_ID_20000022', 'TAXI_ID_20000026', 'TAXI_ID_20000036', 'TAXI_ID_20000039', 'TAXI_ID_20000040', 'TAXI_ID_20000041', 'TAXI_ID_20000044', 'TAXI_ID_20000047', 'TAXI_ID_20000048', 'TAXI_ID_20000049', 'TAXI_ID_20000051', 'TAXI_ID_20000053', 'TAXI_ID_20000054', 'TAXI_ID_20000055', 'TAXI_ID_20000057', 'TAXI_ID_20000060', 'TAXI_ID_20000067', 'TAXI_ID_20000071', 'TAXI_ID_20000079', 'TAXI_ID_20000081', 'TAXI_ID_20000085', 'TAXI_ID_20000086', 'TAXI_ID_20000092', 'TAXI_ID_20000099', 'TAXI_ID_20000100', 'TAXI_ID_20000101', 'TAXI_ID_20000105', 'TAXI_ID_20000108', 'TAXI_ID_20000109', 'TAXI_ID_20000112', 'TAXI_ID_20000116', 'TAXI_ID_20000118', 'TAXI_ID_20000121', 'TAXI_ID_20000123', 'TAXI_ID_20000126', 'TAXI_ID_20000128', 'TAXI_ID_20000129', 'TAXI_ID_20000136', 'TAXI_ID_20000140', 'TAXI_ID_20000144', 'TAXI_ID_20000146', 'TAXI_ID_20000148', 'TAXI_ID_20000154', 'TAXI_ID_20000156', 'TAXI_ID_20000158', 'TAXI_ID_20000159', 'TAXI_ID_20000160', 'TAXI_ID_20000163', 'TAXI_ID_20000166', 'TAXI_ID_20000167', 'TAXI_ID_20000171', 'TAXI_ID_20000177', 'TAXI_ID_20000178', 'TAXI_ID_20000180', 'TAXI_ID_20000185', 'TAXI_ID_20000188', 'TAXI_ID_20000190', 'TAXI_ID_20000192', 'TAXI_ID_20000197', 'TAXI_ID_20000198', 'TAXI_ID_20000199', 'TAXI_ID_20000206', 'TAXI_ID_20000207', 'TAXI_ID_20000213', 'TAXI_ID_20000222', 'TAXI_ID_20000224', 'TAXI_ID_20000230', 'TAXI_ID_20000235', 'TAXI_ID_20000239', 'TAXI_ID_20000243', 'TAXI_ID_20000245', 'TAXI_ID_20000247', 'TAXI_ID_20000248', 'TAXI_ID_20000249', 'TAXI_ID_20000250', 'TAXI_ID_20000252', 'TAXI_ID_20000255', 'TAXI_ID_20000256', 'TAXI_ID_20000260', 'TAXI_ID_20000261', 'TAXI_ID_20000263', 'TAXI_ID_20000268', 'TAXI_ID_20000272', 'TAXI_ID_20000276', 'TAXI_ID_20000280', 'TAXI_ID_20000281', 'TAXI_ID_20000285', 'TAXI_ID_20000286', 'TAXI_ID_20000288', 'TAXI_ID_20000294', 'TAXI_ID_20000295', 'TAXI_ID_20000296', 'TAXI_ID_20000303', 'TAXI_ID_20000304', 'TAXI_ID_20000307', 'TAXI_ID_20000310', 'TAXI_ID_20000311', 'TAXI_ID_20000312', 'TAXI_ID_20000314', 'TAXI_ID_20000320', 'TAXI_ID_20000325', 'TAXI_ID_20000327', 'TAXI_ID_20000328', 'TAXI_ID_20000331', 'TAXI_ID_20000333', 'TAXI_ID_20000334', 'TAXI_ID_20000338', 'TAXI_ID_20000340', 'TAXI_ID_20000342', 'TAXI_ID_20000345', 'TAXI_ID_20000347', 'TAXI_ID_20000349', 'TAXI_ID_20000351', 'TAXI_ID_20000352', 'TAXI_ID_20000353', 'TAXI_ID_20000356', 'TAXI_ID_20000361', 'TAXI_ID_20000362', 'TAXI_ID_20000363', 'TAXI_ID_20000370', 'TAXI_ID_20000372', 'TAXI_ID_20000377', 'TAXI_ID_20000381', 'TAXI_ID_20000383', 'TAXI_ID_20000384', 'TAXI_ID_20000387', 'TAXI_ID_20000391', 'TAXI_ID_20000393', 'TAXI_ID_20000395', 'TAXI_ID_20000400', 'TAXI_ID_20000403', 'TAXI_ID_20000406', 'TAXI_ID_20000407', 'TAXI_ID_20000410', 'TAXI_ID_20000421', 'TAXI_ID_20000424', 'TAXI_ID_20000426', 'TAXI_ID_20000429', 'TAXI_ID_20000430', 'TAXI_ID_20000431', 'TAXI_ID_20000434', 'TAXI_ID_20000436', 'TAXI_ID_20000440', 'TAXI_ID_20000446', 'TAXI_ID_20000450', 'TAXI_ID_20000452', 'TAXI_ID_20000453', 'TAXI_ID_20000454', 'TAXI_ID_20000455', 'TAXI_ID_20000456', 'TAXI_ID_20000460', 'TAXI_ID_20000463', 'TAXI_ID_20000464', 'TAXI_ID_20000467', 'TAXI_ID_20000473', 'TAXI_ID_20000476', 'TAXI_ID_20000477', 'TAXI_ID_20000480', 'TAXI_ID_20000483', 'TAXI_ID_20000486', 'TAXI_ID_20000488', 'TAXI_ID_20000492', 'TAXI_ID_20000494', 'TAXI_ID_20000495', 'TAXI_ID_20000496', 'TAXI_ID_20000497', 'TAXI_ID_20000499', 'TAXI_ID_20000500', 'TAXI_ID_20000502', 'TAXI_ID_20000503', 'TAXI_ID_20000510', 'TAXI_ID_20000513', 'TAXI_ID_20000517', 'TAXI_ID_20000518', 'TAXI_ID_20000523', 'TAXI_ID_20000525', 'TAXI_ID_20000529', 'TAXI_ID_20000539', 'TAXI_ID_20000540', 'TAXI_ID_20000541', 'TAXI_ID_20000542', 'TAXI_ID_20000543', 'TAXI_ID_20000546', 'TAXI_ID_20000547', 'TAXI_ID_20000548', 'TAXI_ID_20000549', 'TAXI_ID_20000554', 'TAXI_ID_20000560', 'TAXI_ID_20000561', 'TAXI_ID_20000562', 'TAXI_ID_20000565', 'TAXI_ID_20000569', 'TAXI_ID_20000572', 'TAXI_ID_20000574', 'TAXI_ID_20000576', 'TAXI_ID_20000577', 'TAXI_ID_20000578', 'TAXI_ID_20000589', 'TAXI_ID_20000591', 'TAXI_ID_20000595', 'TAXI_ID_20000596', 'TAXI_ID_20000597', 'TAXI_ID_20000603', 'TAXI_ID_20000607', 'TAXI_ID_20000612', 'TAXI_ID_20000617', 'TAXI_ID_20000619', 'TAXI_ID_20000621', 'TAXI_ID_20000624', 'TAXI_ID_20000625', 'TAXI_ID_20000626', 'TAXI_ID_20000632', 'TAXI_ID_20000633', 'TAXI_ID_20000649', 'TAXI_ID_20000653', 'TAXI_ID_20000657', 'TAXI_ID_20000662', 'TAXI_ID_20000664', 'TAXI_ID_20000665', 'TAXI_ID_20000667', 'TAXI_ID_20000668', 'TAXI_ID_20000675', 'TAXI_ID_20000678', 'TAXI_ID_20000682', 'TAXI_ID_20000685', 'TAXI_ID_20000686', 'TAXI_ID_20000687', 'TAXI_ID_20000688', 'TAXI_ID_20000693', 'TAXI_ID_20000698', 'TAXI_ID_20000900', 'TAXI_ID_20000901', 'TAXI_ID_20000903', 'TAXI_ID_20000904']\n",
      "[742.9472045898438, 711.1881713867188, 724.5670776367188, 696.5973510742188, 694.0821533203125, 638.9030151367188, 721.9225463867188, 638.2600708007812, 712.2496337890625, 718.2908325195312, 669.1052856445312, 641.9501342773438, 646.3390502929688, 625.9376220703125, 638.7947387695312, 654.249267578125, 647.1967163085938, 656.7985229492188, 726.0663452148438, 646.4981689453125, 735.8611450195312, 652.3788452148438, 649.9652709960938, 743.2503051757812, 664.9788208007812, 666.9266967773438, 742.2982788085938, 719.3466796875, 674.7561645507812, 727.7604370117188, 725.1596069335938, 683.4891967773438, 649.591552734375, 695.9841918945312, 678.6715698242188, 652.6393432617188, 761.483642578125, 649.93017578125, 728.8010864257812, 687.1842651367188, 647.5883178710938, 638.9993286132812, 642.109130859375, 700.9855346679688, 664.6144409179688, 673.5188598632812, 641.4710693359375, 639.0165405273438, 666.0513305664062, 653.4278564453125, 688.7986450195312, 653.8566284179688, 684.2921752929688, 727.5549926757812, 721.4945678710938, 667.5193481445312, 683.4917602539062, 656.4935302734375, 651.3452758789062, 638.660888671875, 730.13916015625, 657.0463256835938, 652.2284545898438, 761.8190307617188, 684.487060546875, 698.5875244140625, 729.4480590820312, 641.8908081054688, 642.1190795898438, 663.2848510742188, 649.3427124023438, 647.71630859375, 742.2830200195312, 657.6276245117188, 622.1627197265625, 609.8469848632812, 639.5680541992188, 600.891845703125, 603.9109497070312, 621.186767578125, 609.3937377929688, 613.0016479492188, 605.5471801757812, 638.7186279296875, 624.0647583007812, 600.318115234375, 598.371826171875, 620.9740600585938, 595.2472534179688, 626.323974609375, 734.9668579101562, 617.1023559570312, 608.69775390625, 694.3048706054688, 603.3168334960938, 601.81005859375, 605.6985473632812, 636.73681640625, 624.9420776367188, 589.0764770507812, 631.1665649414062, 643.5829467773438, 620.9548950195312, 647.66796875, 656.6076049804688, 604.1447143554688, 637.5711059570312, 584.6338500976562, 641.6449584960938, 715.199462890625, 600.2709350585938, 705.9633178710938, 628.87060546875, 613.706787109375, 595.5084838867188, 597.6204223632812, 619.8558959960938, 606.3042602539062, 597.2644653320312, 649.7254028320312, 572.8587036132812, 615.3431396484375, 604.320068359375, 613.7520751953125, 666.9280395507812, 675.1784057617188, 593.6098022460938, 597.728271484375, 601.4707641601562, 656.493896484375, 658.9832153320312, 613.7254028320312, 604.2306518554688, 607.5765991210938, 597.1565551757812, 620.2265625, 694.3528442382812, 620.8370971679688, 599.4157104492188, 596.881103515625, 620.3978881835938, 656.8091430664062, 587.7919921875, 654.7347412109375, 618.6970825195312, 620.9119262695312, 596.4353637695312, 614.9574584960938, 651.705322265625, 595.30859375, 604.7080688476562, 737.2802124023438, 656.6670532226562, 648.3517456054688, 637.289306640625, 662.9854736328125, 691.6643676757812, 650.8436889648438, 673.3088989257812, 727.7127075195312, 741.57666015625, 649.0126953125, 634.824951171875, 632.4907836914062, 639.68994140625, 655.264892578125, 651.6753540039062, 635.1904907226562, 661.109619140625, 696.9765014648438, 654.6751708984375, 645.3569946289062, 636.9718627929688, 670.030029296875, 650.6135864257812, 652.5548706054688, 709.899658203125, 649.9349975585938, 638.7154541015625, 637.3905639648438, 710.065185546875, 689.22509765625, 658.3916625976562, 635.4397583007812, 663.3369140625, 630.5223999023438, 766.0181274414062, 643.5906372070312, 730.4336547851562, 646.03173828125, 656.2149658203125, 647.1611938476562, 657.1716918945312, 655.5022583007812, 692.5952758789062, 635.9603271484375, 655.9110717773438, 658.6898803710938, 624.1968383789062, 672.7223510742188, 625.6129150390625, 626.9627685546875, 658.7589721679688, 653.2738037109375, 642.68017578125, 633.5299682617188, 682.0191650390625, 657.6525268554688, 642.0194702148438, 640.2317504882812, 689.397705078125, 676.8839721679688, 632.9007568359375, 647.9829711914062, 668.7979125976562, 654.5048217773438, 658.1930541992188, 662.5503540039062, 652.4307250976562, 739.3709106445312, 654.8602905273438, 627.8236083984375, 779.4190673828125, 643.0084838867188, 684.5936889648438, 621.1053466796875, 653.6973266601562, 748.2816162109375, 718.8410034179688, 669.2622680664062, 714.4580078125, 689.9722290039062, 645.1343383789062, 664.7201538085938, 698.0319213867188, 667.0830688476562, 669.0435791015625, 669.7255249023438, 672.0914306640625, 694.0104370117188, 662.9567260742188, 685.1155395507812, 686.261962890625, 681.4307250976562, 683.4747314453125, 681.4317016601562, 733.2879638671875, 679.2306518554688, 703.1842041015625, 691.88427734375, 697.26953125, 664.4989624023438, 768.4171752929688, 666.5960083007812, 701.6015014648438, 719.68701171875, 676.9064331054688, 683.8947143554688, 670.2010498046875, 660.1533203125, 675.4386596679688, 682.867431640625, 668.601318359375, 670.9851684570312, 682.4254150390625, 691.0648803710938, 681.8682250976562, 632.6812744140625, 678.8834838867188, 676.5422973632812, 677.010009765625, 670.9877319335938, 673.7031860351562, 685.2986450195312, 677.3715209960938, 686.7177124023438, 690.358154296875, 671.5153198242188, 669.3739624023438, 659.418701171875, 667.2949829101562, 660.70263671875, 672.5241088867188, 685.6066284179688, 677.1538696289062, 655.965087890625, 657.497802734375, 684.0368041992188, 665.4382934570312, 662.5508422851562, 620.4617919921875, 601.4618530273438, 616.826904296875, 614.4393920898438, 632.2274780273438, 650.4257202148438, 638.5972900390625, 657.7142333984375, 614.5689086914062, 645.3844604492188, 620.6246948242188, 607.8203735351562, 624.1470336914062, 613.6463012695312, 622.6791381835938, 625.2471923828125, 623.0548706054688, 624.1837768554688, 612.3270874023438, 653.4188842773438, 605.2423706054688, 602.7062377929688, 602.8522338867188, 600.2711791992188, 630.73583984375, 616.553466796875, 649.5953979492188, 635.4205932617188, 612.9845581054688, 624.1614379882812]\n",
      "Mean: 657.1843450546264 STD: 39.4068517867567 Median: 654.0529479980469 Min: 572.8587036132812 Max: 779.4190673828125\n",
      "    TRIP_ID  TRAVEL_TIME\n",
      "0        T1   742.947205\n",
      "1        T2   711.188171\n",
      "2        T3   724.567078\n",
      "3        T4   696.597351\n",
      "4        T5   694.082153\n",
      "..      ...          ...\n",
      "315    T323   616.553467\n",
      "316    T324   649.595398\n",
      "317    T325   635.420593\n",
      "318    T326   612.984558\n",
      "319    T327   624.161438\n",
      "\n",
      "[320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testset = MyDataset(df_ts)\n",
    "testloader = DataLoader(testset)\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        features, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(features)\n",
    "        preds.append(output.item())\n",
    "ids = testset.df[\"TRIP_ID\"]        \n",
    "print(preds)\n",
    "d = {\"TRIP_ID\" : ids, \"TRAVEL_TIME\" : preds}\n",
    "newdf = pd.DataFrame(d)\n",
    "mean, std, median, min, max = newdf[\"TRAVEL_TIME\"].mean(), newdf[\"TRAVEL_TIME\"].std(), newdf[\"TRAVEL_TIME\"].median(), newdf[\"TRAVEL_TIME\"].min(), newdf[\"TRAVEL_TIME\"].max()\n",
    "print(f\"Mean: {mean} STD: {std} Median: {median} Min: {min} Max: {max}\")\n",
    "print(newdf)\n",
    "newdf.to_csv(\"my_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
